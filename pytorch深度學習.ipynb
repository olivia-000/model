{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c5de2fc-8cb0-476d-8da5-94db69e0c983",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openvino\n",
      "  Downloading openvino-2025.3.0-19807-cp310-cp310-manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Requirement already satisfied: numpy<2.3.0,>=1.16.6 in /usr/local/lib/python3.10/dist-packages (from openvino) (1.24.1)\n",
      "Collecting openvino-telemetry>=2023.2.1 (from openvino)\n",
      "  Downloading openvino_telemetry-2025.2.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from openvino) (23.2)\n",
      "Downloading openvino-2025.3.0-19807-cp310-cp310-manylinux2014_x86_64.whl (49.0 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m49.0/49.0 MB\u001b[0m \u001b[31m74.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading openvino_telemetry-2025.2.0-py3-none-any.whl (25 kB)\n",
      "Installing collected packages: openvino-telemetry, openvino\n",
      "Successfully installed openvino-2025.3.0 openvino-telemetry-2025.2.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install openvino --break-system-packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ed16f60-06d7-41ec-bc7d-8cd79481951f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nvidia-tensorrt\n",
      "  Downloading nvidia_tensorrt-99.0.0-py3-none-manylinux_2_17_x86_64.whl.metadata (596 bytes)\n",
      "Collecting tensorrt (from nvidia-tensorrt)\n",
      "  Downloading tensorrt-10.13.3.9.tar.gz (40 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m40.5/40.5 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting tensorrt_cu13==10.13.3.9 (from tensorrt->nvidia-tensorrt)\n",
      "  Downloading tensorrt_cu13-10.13.3.9.tar.gz (18 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting tensorrt_cu13_libs==10.13.3.9 (from tensorrt_cu13==10.13.3.9->tensorrt->nvidia-tensorrt)\n",
      "  Downloading tensorrt_cu13_libs-10.13.3.9.tar.gz (706 bytes)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting tensorrt_cu13_bindings==10.13.3.9 (from tensorrt_cu13==10.13.3.9->tensorrt->nvidia-tensorrt)\n",
      "  Downloading tensorrt_cu13_bindings-10.13.3.9-cp310-none-manylinux_2_28_x86_64.whl.metadata (606 bytes)\n",
      "Collecting nvidia-cuda-runtime-cu13 (from tensorrt_cu13_libs==10.13.3.9->tensorrt_cu13==10.13.3.9->tensorrt->nvidia-tensorrt)\n",
      "  Downloading nvidia_cuda_runtime_cu13-0.0.0a0-py2.py3-none-any.whl.metadata (225 bytes)\n",
      "Downloading nvidia_tensorrt-99.0.0-py3-none-manylinux_2_17_x86_64.whl (17 kB)\n",
      "Downloading tensorrt_cu13_bindings-10.13.3.9-cp310-none-manylinux_2_28_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m36.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu13-0.0.0a0-py2.py3-none-any.whl (1.2 kB)\n",
      "Building wheels for collected packages: tensorrt, tensorrt_cu13, tensorrt_cu13_libs\n",
      "  Building wheel for tensorrt (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for tensorrt: filename=tensorrt-10.13.3.9-py2.py3-none-any.whl size=46398 sha256=d3940b2a9a155e80a7dd791ddf274da0cf7138bc9a244a739253f240ac8401bf\n",
      "  Stored in directory: /root/.cache/pip/wheels/95/bf/be/5afa83ab190f98622801ca4321ed121e2def5367e3bb891d72\n",
      "  Building wheel for tensorrt_cu13 (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for tensorrt_cu13: filename=tensorrt_cu13-10.13.3.9-py2.py3-none-any.whl size=17436 sha256=a53836856c33dcc8c02b3ffaa19c35dc60861e5d13595f25d233c6feccf3ca64\n",
      "  Stored in directory: /root/.cache/pip/wheels/34/ce/c8/ae41f1c638712fc9ba0d48f1ab718e4f6271c2f7747e3790f6\n",
      "  Building wheel for tensorrt_cu13_libs (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for tensorrt_cu13_libs: filename=tensorrt_cu13_libs-10.13.3.9-py2.py3-none-manylinux_2_28_x86_64.whl size=2740897716 sha256=19a54f2106f7d9496433a01a596fa4e363f58901110306abe1ce7b6e9617c6a7\n",
      "  Stored in directory: /root/.cache/pip/wheels/35/07/43/8bdc7ba375434899dfb507d269d92f16db14a310b5129e413d\n",
      "Successfully built tensorrt tensorrt_cu13 tensorrt_cu13_libs\n",
      "Installing collected packages: tensorrt_cu13_bindings, nvidia-cuda-runtime-cu13, tensorrt_cu13_libs, tensorrt_cu13, tensorrt, nvidia-tensorrt\n",
      "Successfully installed nvidia-cuda-runtime-cu13-0.0.0a0 nvidia-tensorrt-99.0.0 tensorrt-10.13.3.9 tensorrt_cu13-10.13.3.9 tensorrt_cu13_bindings-10.13.3.9 tensorrt_cu13_libs-10.13.3.9\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Collecting torch-tensorrt\n",
      "  Downloading torch_tensorrt-2.8.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_34_x86_64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: packaging>=23 in /usr/local/lib/python3.10/dist-packages (from torch-tensorrt) (23.2)\n",
      "Collecting typing-extensions>=4.7.0 (from torch-tensorrt)\n",
      "  Downloading typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting dllist (from torch-tensorrt)\n",
      "  Downloading dllist-2.0.0-py3-none-any.whl.metadata (2.0 kB)\n",
      "Collecting torch<2.9.0,>=2.8.0 (from torch-tensorrt)\n",
      "  Downloading torch-2.8.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (30 kB)\n",
      "Collecting tensorrt<10.13.0,>=10.12.0 (from torch-tensorrt)\n",
      "  Downloading tensorrt-10.12.0.36.tar.gz (40 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m40.7/40.7 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting tensorrt-cu12-bindings<10.13.0,>=10.12.0 (from torch-tensorrt)\n",
      "  Downloading tensorrt_cu12_bindings-10.12.0.36-cp310-none-manylinux_2_28_x86_64.whl.metadata (607 bytes)\n",
      "Collecting tensorrt-cu12-libs<10.13.0,>=10.12.0 (from torch-tensorrt)\n",
      "  Downloading tensorrt_cu12_libs-10.12.0.36.tar.gz (709 bytes)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch-tensorrt) (1.24.1)\n",
      "Collecting tensorrt_cu12==10.12.0.36 (from tensorrt<10.13.0,>=10.12.0->torch-tensorrt)\n",
      "  Downloading tensorrt_cu12-10.12.0.36.tar.gz (18 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting nvidia-cuda-runtime-cu12 (from tensorrt-cu12-libs<10.13.0,>=10.12.0->torch-tensorrt)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.9.79-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch<2.9.0,>=2.8.0->torch-tensorrt) (3.9.0)\n",
      "Collecting sympy>=1.13.3 (from torch<2.9.0,>=2.8.0->torch-tensorrt)\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<2.9.0,>=2.8.0->torch-tensorrt) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<2.9.0,>=2.8.0->torch-tensorrt) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch<2.9.0,>=2.8.0->torch-tensorrt) (2023.4.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.8.93 (from torch<2.9.0,>=2.8.0->torch-tensorrt)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12 (from tensorrt-cu12-libs<10.13.0,>=10.12.0->torch-tensorrt)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.8.90 (from torch<2.9.0,>=2.8.0->torch-tensorrt)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.10.2.21 (from torch<2.9.0,>=2.8.0->torch-tensorrt)\n",
      "  Downloading nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cublas-cu12==12.8.4.1 (from torch<2.9.0,>=2.8.0->torch-tensorrt)\n",
      "  Downloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cufft-cu12==11.3.3.83 (from torch<2.9.0,>=2.8.0->torch-tensorrt)\n",
      "  Downloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.9.90 (from torch<2.9.0,>=2.8.0->torch-tensorrt)\n",
      "  Downloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.7.3.90 (from torch<2.9.0,>=2.8.0->torch-tensorrt)\n",
      "  Downloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.5.8.93 (from torch<2.9.0,>=2.8.0->torch-tensorrt)\n",
      "  Downloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cusparselt-cu12==0.7.1 (from torch<2.9.0,>=2.8.0->torch-tensorrt)\n",
      "  Downloading nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl.metadata (7.0 kB)\n",
      "Collecting nvidia-nccl-cu12==2.27.3 (from torch<2.9.0,>=2.8.0->torch-tensorrt)\n",
      "  Downloading nvidia_nccl_cu12-2.27.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.8.90 (from torch<2.9.0,>=2.8.0->torch-tensorrt)\n",
      "  Downloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.8.93 (from torch<2.9.0,>=2.8.0->torch-tensorrt)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cufile-cu12==1.13.1.3 (from torch<2.9.0,>=2.8.0->torch-tensorrt)\n",
      "  Downloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting triton==3.4.0 (from torch<2.9.0,>=2.8.0->torch-tensorrt)\n",
      "  Downloading triton-3.4.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.10/dist-packages (from triton==3.4.0->torch<2.9.0,>=2.8.0->torch-tensorrt) (68.2.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy>=1.13.3->torch<2.9.0,>=2.8.0->torch-tensorrt) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<2.9.0,>=2.8.0->torch-tensorrt) (2.1.2)\n",
      "Downloading torch_tensorrt-2.8.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_34_x86_64.whl (15.0 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m15.0/15.0 MB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading tensorrt_cu12_bindings-10.12.0.36-cp310-none-manylinux_2_28_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m78.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading torch-2.8.0-cp310-cp310-manylinux_2_28_x86_64.whl (888.0 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m888.0/888.0 MB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl (594.3 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m594.3/594.3 MB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (10.2 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m212.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (88.0 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m88.0/88.0 MB\u001b[0m \u001b[31m90.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (954 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m954.8/954.8 kB\u001b[0m \u001b[31m141.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl (706.8 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m706.8/706.8 MB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (193.1 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m193.1/193.1 MB\u001b[0m \u001b[31m49.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m205.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl (63.6 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m63.6/63.6 MB\u001b[0m \u001b[31m124.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl (267.5 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m267.5/267.5 MB\u001b[0m \u001b[31m38.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (288.2 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m288.2/288.2 MB\u001b[0m \u001b[31m31.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl (287.2 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m287.2/287.2 MB\u001b[0m \u001b[31m38.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.27.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (322.4 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m322.4/322.4 MB\u001b[0m \u001b[31m37.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.3 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m39.3/39.3 MB\u001b[0m \u001b[31m114.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading triton-3.4.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (155.4 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m155.4/155.4 MB\u001b[0m \u001b[31m43.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading dllist-2.0.0-py3-none-any.whl (5.7 kB)\n",
      "Downloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m142.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: tensorrt, tensorrt_cu12, tensorrt-cu12-libs\n",
      "  Building wheel for tensorrt (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for tensorrt: filename=tensorrt-10.12.0.36-py2.py3-none-any.whl size=46641 sha256=d57cc94784aa4196bdd3846ba3ff8013dd83b3cd4a2e5e205b81a5834b8c23d2\n",
      "  Stored in directory: /root/.cache/pip/wheels/e1/12/2f/04cb0da2afe8d2a7f100f50814b46ef9ec24a308cc4c320f46\n",
      "  Building wheel for tensorrt_cu12 (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for tensorrt_cu12: filename=tensorrt_cu12-10.12.0.36-py2.py3-none-any.whl size=17477 sha256=fb702fd7eab146e17b13f147cc8ba8fa216da5e9b3a2638e21d4cefd61c6a41f\n",
      "  Stored in directory: /root/.cache/pip/wheels/a4/5b/0b/f12f75d8b1f2787e83666508a6f52c2e2cd64e76babc4abdf5\n",
      "  Building wheel for tensorrt-cu12-libs (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for tensorrt-cu12-libs: filename=tensorrt_cu12_libs-10.12.0.36-py2.py3-none-manylinux_2_28_x86_64.whl size=3095483544 sha256=3910039e1d49de0edfdc8bf273e40ad4b85a9d57c7c383fe0e22f75417df9610\n",
      "  Stored in directory: /root/.cache/pip/wheels/9f/30/0c/5deca08d1af19cbc0f6b9f806372edf64841c30ff47eff7a81\n",
      "Successfully built tensorrt tensorrt_cu12 tensorrt-cu12-libs\n",
      "Installing collected packages: tensorrt-cu12-bindings, nvidia-cusparselt-cu12, typing-extensions, triton, sympy, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, dllist, tensorrt-cu12-libs, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, tensorrt_cu12, nvidia-cusolver-cu12, torch, tensorrt, torch-tensorrt\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.4.0\n",
      "    Uninstalling typing_extensions-4.4.0:\n",
      "      Successfully uninstalled typing_extensions-4.4.0\n",
      "  Attempting uninstall: triton\n",
      "    Found existing installation: triton 2.1.0\n",
      "    Uninstalling triton-2.1.0:\n",
      "      Successfully uninstalled triton-2.1.0\n",
      "  Attempting uninstall: sympy\n",
      "    Found existing installation: sympy 1.12\n",
      "    Uninstalling sympy-1.12:\n",
      "      Successfully uninstalled sympy-1.12\n",
      "\u001b[31mERROR: Could not install packages due to an OSError: [Errno 28] No space left on device\n",
      "\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install nvidia-tensorrt --break-system-packages\n",
    "!pip install torch-tensorrt --break-system-packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f03e74e-ed2d-4e52-b6f5-039a14f99fcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: onnx==1.15.0 in /usr/local/lib/python3.10/dist-packages (1.15.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from onnx==1.15.0) (1.24.1)\n",
      "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from onnx==1.15.0) (6.32.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install onnx==1.15.0 --break-system-packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b351d1f1-2179-41cd-9095-0368d0b570c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ğŸ¥ é†«å­¸å½±åƒåˆ†å‰²ç³»çµ± - å°ˆæ¥­ç‰ˆ\n",
      "================================================================================\n",
      "âœ“ ä½¿ç”¨ PIL ç¹ªè£½æ–‡å­—\n",
      "âœ“ æ‰€æœ‰å¥—ä»¶å·²å°±ç·’\n",
      "\n",
      "================================================================================\n",
      "é–‹å§‹è™•ç†\n",
      "================================================================================\n",
      "\n",
      "ğŸ“Š æ­¥é©Ÿ 1: è¼‰å…¥è³‡æ–™\n",
      "\n",
      "ğŸ“Š è³‡æ–™é›†: Hippocampus\n",
      "   è¨“ç·´æ¨£æœ¬: 260\n",
      "âœ“ è¼‰å…¥ 3 å€‹æ¡ˆä¾‹\n",
      "\n",
      "ğŸ“Š æ­¥é©Ÿ 2: è™•ç†æ¡ˆä¾‹ 367\n",
      "   åŸå§‹å°ºå¯¸: (36, 57, 37)\n",
      "   åŸå§‹ spacing: [1. 1. 1.]\n",
      "\n",
      "ğŸ“Š æ­¥é©Ÿ 3: é è™•ç†\n",
      "   è™•ç†å¾Œå°ºå¯¸: (64, 64, 64)\n",
      "   æ–° spacing: [0.5625   0.890625 0.578125]\n",
      "   é«”ç´ é«”ç©: 0.29 mmÂ³/voxel\n",
      "\n",
      "ğŸ“Š æ­¥é©Ÿ 4: åŸ·è¡Œåˆ†å‰²\n",
      "   å·¦å´: 4639 é«”ç´ \n",
      "   å³å´: 13376 é«”ç´ \n",
      "\n",
      "ğŸ“Š æ­¥é©Ÿ 5: ç‰¹å¾µæå–\n",
      "   ç¸½é«”ç©: 5217.6 mmÂ³\n",
      "   å·¦å´é«”ç©: 1343.6 mmÂ³\n",
      "   å³å´é«”ç©: 3874.1 mmÂ³\n",
      "   Diceä¿‚æ•¸: 0.014\n",
      "\n",
      "ğŸ“Š æ­¥é©Ÿ 6: å‰µå»ºå¯è¦–åŒ–\n",
      "   âœ“ å·²ä¿å­˜: /workspace/outputs/result_367.png\n",
      "   âœ“ å·²ä¿å­˜: /workspace/outputs/report_367.json\n",
      "\n",
      "================================================================================\n",
      "âœ“ è™•ç†å®Œæˆ!\n",
      "================================================================================\n",
      "\n",
      "ğŸ“Š æœ€çµ‚çµæœ:\n",
      "  ç¸½é«”ç©: 5217.6 mmÂ³\n",
      "  Diceåˆ†æ•¸: 0.014\n",
      "\n",
      "è¼¸å‡ºä½ç½®: /workspace/outputs/\n",
      "\n",
      "âœ… ç³»çµ±é‹è¡ŒæˆåŠŸ!\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "ğŸ¥ é†«å­¸å½±åƒåˆ†å‰²ç³»çµ± - å°ˆæ¥­ä¿®æ­£ç‰ˆ\n",
    "ä¿®æ­£ï¼šspacingè¨ˆç®—ã€æ¨™è¨»è™•ç†ã€å‘é‡åŒ–ç¸®æ”¾ã€è§£å‰–å­¸æ¨™è¨»\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ğŸ¥ é†«å­¸å½±åƒåˆ†å‰²ç³»çµ± - å°ˆæ¥­ç‰ˆ\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "try:\n",
    "    from PIL import Image, ImageDraw, ImageFont\n",
    "    USE_PIL = True\n",
    "    print(\"âœ“ ä½¿ç”¨ PIL ç¹ªè£½æ–‡å­—\")\n",
    "except:\n",
    "    USE_PIL = False\n",
    "    print(\"âœ“ ä¸ä½¿ç”¨æ–‡å­—æ¨™è¨»\")\n",
    "\n",
    "print(\"âœ“ æ‰€æœ‰å¥—ä»¶å·²å°±ç·’\")\n",
    "\n",
    "# ==================== è³‡æ–™è¼‰å…¥ ====================\n",
    "\n",
    "class DataLoader:\n",
    "    def __init__(self, data_root):\n",
    "        self.data_root = Path(data_root)\n",
    "        dataset_json = self.data_root / 'dataset.json'\n",
    "        \n",
    "        if not dataset_json.exists():\n",
    "            raise FileNotFoundError(f\"æ‰¾ä¸åˆ° dataset.json: {dataset_json}\")\n",
    "        \n",
    "        with open(dataset_json, 'r') as f:\n",
    "            self.metadata = json.load(f)\n",
    "        \n",
    "        print(f\"\\nğŸ“Š è³‡æ–™é›†: {self.metadata['name']}\")\n",
    "        print(f\"   è¨“ç·´æ¨£æœ¬: {self.metadata['numTraining']}\")\n",
    "    \n",
    "    def load_nifti(self, filepath):\n",
    "        \"\"\"è¼‰å…¥ NIfTI æª”æ¡ˆ\"\"\"\n",
    "        if not filepath.exists():\n",
    "            raise FileNotFoundError(f\"æª”æ¡ˆä¸å­˜åœ¨: {filepath}\")\n",
    "        \n",
    "        img = nib.load(str(filepath))\n",
    "        data = img.get_fdata()\n",
    "        spacing = np.array(img.header.get_zooms()[:3])\n",
    "        \n",
    "        return data, {'spacing': spacing, 'shape': np.array(data.shape)}\n",
    "    \n",
    "    def get_cases(self, num=5):\n",
    "        \"\"\"å®‰å…¨åœ°ç²å–æ¡ˆä¾‹\"\"\"\n",
    "        cases = []\n",
    "        training_data = self.metadata.get('training', [])\n",
    "        \n",
    "        for item in training_data[:num]:\n",
    "            # ä¿å®ˆè™•ç†è·¯å¾‘\n",
    "            img_rel = item['image'].lstrip('./')\n",
    "            lbl_rel = item['label'].lstrip('./')\n",
    "            \n",
    "            img_path = self.data_root / img_rel\n",
    "            lbl_path = self.data_root / lbl_rel\n",
    "            \n",
    "            # æª¢æŸ¥å­˜åœ¨æ€§\n",
    "            if img_path.exists() and lbl_path.exists():\n",
    "                case_id = img_path.stem.replace('hippocampus_', '').replace('.nii', '')\n",
    "                cases.append({\n",
    "                    'image': img_path,\n",
    "                    'label': lbl_path,\n",
    "                    'id': case_id\n",
    "                })\n",
    "            else:\n",
    "                print(f\"âš ï¸ è·³éç¼ºå¤±æª”æ¡ˆ: {img_path.name}\")\n",
    "        \n",
    "        if not cases:\n",
    "            raise ValueError(\"æ²’æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„æ¡ˆä¾‹æª”æ¡ˆ\")\n",
    "        \n",
    "        return cases\n",
    "\n",
    "# ==================== å‘é‡åŒ–ç¸®æ”¾ ====================\n",
    "\n",
    "def resize_nearest_vectorized(image, target_shape):\n",
    "    \"\"\"å‘é‡åŒ–çš„æœ€è¿‘é„°ç¸®æ”¾ - O(target_size)\"\"\"\n",
    "    src_d, src_h, src_w = image.shape\n",
    "    dst_d, dst_h, dst_w = target_shape\n",
    "    \n",
    "    # è¨ˆç®—æºç´¢å¼• (å‘é‡åŒ–)\n",
    "    scale_d = src_d / dst_d\n",
    "    scale_h = src_h / dst_h\n",
    "    scale_w = src_w / dst_w\n",
    "    \n",
    "    idx_d = np.floor(np.arange(dst_d) * scale_d).astype(int)\n",
    "    idx_h = np.floor(np.arange(dst_h) * scale_h).astype(int)\n",
    "    idx_w = np.floor(np.arange(dst_w) * scale_w).astype(int)\n",
    "    \n",
    "    # é‚Šç•Œæª¢æŸ¥\n",
    "    idx_d = np.clip(idx_d, 0, src_d - 1)\n",
    "    idx_h = np.clip(idx_h, 0, src_h - 1)\n",
    "    idx_w = np.clip(idx_w, 0, src_w - 1)\n",
    "    \n",
    "    # ä¸€æ¬¡æ€§ç´¢å¼• (NumPy é«˜ç´šç´¢å¼•)\n",
    "    result = image[idx_d[:, None, None], idx_h[None, :, None], idx_w[None, None, :]]\n",
    "    \n",
    "    return result\n",
    "\n",
    "# ==================== é è™•ç†ï¼ˆåˆ†æµç‰ˆæœ¬ï¼‰====================\n",
    "\n",
    "def preprocess_image(image, target_size=64):\n",
    "    \"\"\"å½±åƒé è™•ç† - z-score æ¨™æº–åŒ–\"\"\"\n",
    "    # ç™¾åˆ†ä½è£åˆ‡\n",
    "    p1, p99 = np.percentile(image, [1, 99])\n",
    "    image = np.clip(image, p1, p99)\n",
    "    \n",
    "    # Z-score æ¨™æº–åŒ–\n",
    "    mean = np.mean(image)\n",
    "    std = np.std(image)\n",
    "    if std > 1e-8:\n",
    "        image = (image - mean) / std\n",
    "    \n",
    "    # ç¸®æ”¾\n",
    "    target_shape = (target_size, target_size, target_size)\n",
    "    result = resize_nearest_vectorized(image, target_shape)\n",
    "    \n",
    "    return result.astype(np.float32)\n",
    "\n",
    "def preprocess_label(label, target_size=64):\n",
    "    \"\"\"æ¨™è¨»é è™•ç† - åƒ…é‡æ¡æ¨£ï¼ˆä¿æŒæ•´æ•¸ï¼‰\"\"\"\n",
    "    # ç›´æ¥æœ€è¿‘é„°ç¸®æ”¾ï¼ˆä¸åšæ¨™æº–åŒ–ï¼‰\n",
    "    target_shape = (target_size, target_size, target_size)\n",
    "    result = resize_nearest_vectorized(label, target_shape)\n",
    "    \n",
    "    # å››æ¨äº”å…¥ä¸¦è½‰ç‚ºæ•´æ•¸\n",
    "    result = np.round(result).astype(np.uint8)\n",
    "    \n",
    "    return result\n",
    "\n",
    "def calculate_new_spacing(orig_spacing, orig_shape, target_shape):\n",
    "    \"\"\"è¨ˆç®—é‡æ¡æ¨£å¾Œçš„æ–° spacing\"\"\"\n",
    "    orig_spacing = np.array(orig_spacing)\n",
    "    orig_shape = np.array(orig_shape)\n",
    "    target_shape = np.array(target_shape)\n",
    "    \n",
    "    new_spacing = (orig_spacing * orig_shape) / target_shape\n",
    "    \n",
    "    return new_spacing\n",
    "\n",
    "# ==================== åˆ†å‰² ====================\n",
    "\n",
    "def segment(image):\n",
    "    \"\"\"ç°¡å–®é–¾å€¼åˆ†å‰²\"\"\"\n",
    "    threshold = np.percentile(image, 70)\n",
    "    binary = (image > threshold).astype(np.uint8)\n",
    "    \n",
    "    mid = binary.shape[0] // 2\n",
    "    center = binary.shape[2] // 2  # å¯¬åº¦è»¸ä¸­é»\n",
    "    prediction = np.zeros_like(binary)\n",
    "    \n",
    "    # æ³¨æ„ï¼šé€™è£¡ç”¨å·¦å³åˆ†å€ï¼Œéå‰å¾Œ\n",
    "    # åœ¨ä¸­é–“å±¤è™•ç†\n",
    "    for i in range(max(0, mid-10), min(binary.shape[0], mid+10)):\n",
    "        prediction[i, :, :center] = binary[i, :, :center] * 1  # å·¦å´\n",
    "        prediction[i, :, center:] = binary[i, :, center:] * 2  # å³å´\n",
    "    \n",
    "    return prediction\n",
    "\n",
    "# ==================== ç‰¹å¾µæå–ï¼ˆä¿®æ­£ spacingï¼‰====================\n",
    "\n",
    "def extract_features(pred, gt, new_spacing):\n",
    "    \"\"\"\n",
    "    ä½¿ç”¨é‡æ¡æ¨£å¾Œçš„ spacing è¨ˆç®—é«”ç©\n",
    "    new_spacing: å·²æ ¹æ“šç¸®æ”¾æ¯”ä¾‹èª¿æ•´éçš„ spacing\n",
    "    \"\"\"\n",
    "    voxel_volume = float(np.prod(new_spacing))\n",
    "    \n",
    "    left_vol = float(np.sum(pred == 1)) * voxel_volume\n",
    "    right_vol = float(np.sum(pred == 2)) * voxel_volume\n",
    "    total = left_vol + right_vol\n",
    "    \n",
    "    # Dice ä¿‚æ•¸\n",
    "    dice_scores = []\n",
    "    for i in [1, 2]:\n",
    "        p = (pred == i).astype(np.float32)\n",
    "        g = (gt == i).astype(np.float32)\n",
    "        inter = float(np.sum(p * g))\n",
    "        union = float(np.sum(p) + np.sum(g))\n",
    "        dice = 2 * inter / (union + 1e-8) if union > 0 else 0.0\n",
    "        dice_scores.append(dice)\n",
    "    \n",
    "    return {\n",
    "        'total_volume': total,\n",
    "        'left_vol': left_vol,\n",
    "        'right_vol': right_vol,\n",
    "        'dice': float(np.mean(dice_scores)),\n",
    "        'dice_left': dice_scores[0],\n",
    "        'dice_right': dice_scores[1]\n",
    "    }\n",
    "\n",
    "# ==================== å¯è¦–åŒ– ====================\n",
    "\n",
    "def add_text_pil(image_array, features):\n",
    "    \"\"\"ä½¿ç”¨PILæ·»åŠ æ–‡å­—\"\"\"\n",
    "    img = Image.fromarray(image_array)\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    \n",
    "    try:\n",
    "        font = ImageFont.truetype(\"/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf\", 12)\n",
    "    except:\n",
    "        font = ImageFont.load_default()\n",
    "    \n",
    "    texts = [\n",
    "        \"HIPPOCAMPUS SEGMENTATION\",\n",
    "        \"(Left-Right Split Demo)\",\n",
    "        \"\",\n",
    "        f\"Total: {features['total_volume']:.1f} mm3\",\n",
    "        f\"Left:  {features['left_vol']:.1f} mm3\",\n",
    "        f\"Right: {features['right_vol']:.1f} mm3\",\n",
    "        \"\",\n",
    "        f\"Dice: {features['dice']:.3f}\",\n",
    "        f\"  Left:  {features['dice_left']:.3f}\",\n",
    "        f\"  Right: {features['dice_right']:.3f}\",\n",
    "    ]\n",
    "    \n",
    "    y = 10\n",
    "    for text in texts:\n",
    "        draw.text((10, y), text, fill=(0, 0, 0), font=font)\n",
    "        y += 18\n",
    "    \n",
    "    return np.array(img)\n",
    "\n",
    "def visualize(image, pred, gt, features):\n",
    "    \"\"\"å‰µå»ºå¯è¦–åŒ–\"\"\"\n",
    "    mid = pred.shape[0] // 2\n",
    "    views = []\n",
    "    \n",
    "    for offset in [-8, 0, 8]:\n",
    "        idx = mid + offset\n",
    "        if 0 <= idx < pred.shape[0]:\n",
    "            # æ¨™æº–åŒ–åˆ°0-255\n",
    "            img_slice = image[idx]\n",
    "            img_min, img_max = float(img_slice.min()), float(img_slice.max())\n",
    "            if img_max > img_min:\n",
    "                img_norm = ((img_slice - img_min) / (img_max - img_min) * 255).astype(np.uint8)\n",
    "            else:\n",
    "                img_norm = np.zeros_like(img_slice, dtype=np.uint8)\n",
    "            \n",
    "            # è½‰ç‚ºRGB\n",
    "            img_rgb = np.stack([img_norm, img_norm, img_norm], axis=-1)\n",
    "            \n",
    "            # é æ¸¬ç–ŠåŠ ï¼ˆå·¦=ç´…ï¼Œå³=è—ï¼‰\n",
    "            pred_overlay = img_rgb.copy()\n",
    "            pred_overlay[pred[idx] == 1] = [255, 0, 0]  # å·¦å´-ç´…è‰²\n",
    "            pred_overlay[pred[idx] == 2] = [0, 0, 255]  # å³å´-è—è‰²\n",
    "            pred_result = (img_rgb * 0.7 + pred_overlay * 0.3).astype(np.uint8)\n",
    "            \n",
    "            # Ground Truthç–ŠåŠ ï¼ˆå·¦=ç¶ ï¼Œå³=é»ƒï¼‰\n",
    "            gt_overlay = img_rgb.copy()\n",
    "            gt_overlay[gt[idx] == 1] = [0, 255, 0]      # å·¦å´-ç¶ è‰²\n",
    "            gt_overlay[gt[idx] == 2] = [255, 255, 0]    # å³å´-é»ƒè‰²\n",
    "            gt_result = (img_rgb * 0.7 + gt_overlay * 0.3).astype(np.uint8)\n",
    "            \n",
    "            combined = np.hstack([pred_result, gt_result])\n",
    "            views.append(combined)\n",
    "    \n",
    "    result = np.vstack(views)\n",
    "    \n",
    "    # æ·»åŠ æ–‡å­—\n",
    "    if USE_PIL:\n",
    "        text_h = result.shape[0]\n",
    "        text_w = 300\n",
    "        text_area = np.ones((text_h, text_w, 3), dtype=np.uint8) * 255\n",
    "        text_area = add_text_pil(text_area, features)\n",
    "        result = np.hstack([result, text_area])\n",
    "    \n",
    "    return result\n",
    "\n",
    "def save_image(image_array, filepath):\n",
    "    \"\"\"ä½¿ç”¨PILä¿å­˜\"\"\"\n",
    "    img = Image.fromarray(image_array.astype(np.uint8))\n",
    "    img.save(filepath)\n",
    "\n",
    "# ==================== ä¸»ç¨‹åº ====================\n",
    "\n",
    "def main():\n",
    "    DATA_ROOT = Path('/workspace/Task04_Hippocampus')\n",
    "    OUTPUT_DIR = Path('/workspace/outputs')\n",
    "    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"é–‹å§‹è™•ç†\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # è¼‰å…¥è³‡æ–™\n",
    "    print(\"\\nğŸ“Š æ­¥é©Ÿ 1: è¼‰å…¥è³‡æ–™\")\n",
    "    loader = DataLoader(DATA_ROOT)\n",
    "    cases = loader.get_cases(num=3)\n",
    "    print(f\"âœ“ è¼‰å…¥ {len(cases)} å€‹æ¡ˆä¾‹\")\n",
    "    \n",
    "    # è™•ç†ç¬¬ä¸€å€‹æ¡ˆä¾‹\n",
    "    case = cases[0]\n",
    "    print(f\"\\nğŸ“Š æ­¥é©Ÿ 2: è™•ç†æ¡ˆä¾‹ {case['id']}\")\n",
    "    \n",
    "    image, img_meta = loader.load_nifti(case['image'])\n",
    "    label, lbl_meta = loader.load_nifti(case['label'])\n",
    "    print(f\"   åŸå§‹å°ºå¯¸: {image.shape}\")\n",
    "    print(f\"   åŸå§‹ spacing: {img_meta['spacing']}\")\n",
    "    \n",
    "    # é è™•ç†ï¼ˆåˆ†æµï¼‰\n",
    "    print(\"\\nğŸ“Š æ­¥é©Ÿ 3: é è™•ç†\")\n",
    "    target_size = 64\n",
    "    image_prep = preprocess_image(image, target_size)\n",
    "    label_prep = preprocess_label(label, target_size)\n",
    "    \n",
    "    # è¨ˆç®—æ–° spacing\n",
    "    new_spacing = calculate_new_spacing(\n",
    "        img_meta['spacing'], \n",
    "        img_meta['shape'], \n",
    "        (target_size, target_size, target_size)\n",
    "    )\n",
    "    \n",
    "    print(f\"   è™•ç†å¾Œå°ºå¯¸: {image_prep.shape}\")\n",
    "    print(f\"   æ–° spacing: {new_spacing}\")\n",
    "    print(f\"   é«”ç´ é«”ç©: {np.prod(new_spacing):.2f} mmÂ³/voxel\")\n",
    "    \n",
    "    # åˆ†å‰²\n",
    "    print(\"\\nğŸ“Š æ­¥é©Ÿ 4: åŸ·è¡Œåˆ†å‰²\")\n",
    "    prediction = segment(image_prep)\n",
    "    print(f\"   å·¦å´: {np.sum(prediction == 1)} é«”ç´ \")\n",
    "    print(f\"   å³å´: {np.sum(prediction == 2)} é«”ç´ \")\n",
    "    \n",
    "    # ç‰¹å¾µæå–ï¼ˆä½¿ç”¨æ–° spacingï¼‰\n",
    "    print(\"\\nğŸ“Š æ­¥é©Ÿ 5: ç‰¹å¾µæå–\")\n",
    "    features = extract_features(prediction, label_prep, new_spacing)\n",
    "    print(f\"   ç¸½é«”ç©: {features['total_volume']:.1f} mmÂ³\")\n",
    "    print(f\"   å·¦å´é«”ç©: {features['left_vol']:.1f} mmÂ³\")\n",
    "    print(f\"   å³å´é«”ç©: {features['right_vol']:.1f} mmÂ³\")\n",
    "    print(f\"   Diceä¿‚æ•¸: {features['dice']:.3f}\")\n",
    "    \n",
    "    # å¯è¦–åŒ–\n",
    "    print(\"\\nğŸ“Š æ­¥é©Ÿ 6: å‰µå»ºå¯è¦–åŒ–\")\n",
    "    viz = visualize(image_prep, prediction, label_prep, features)\n",
    "    \n",
    "    output_path = OUTPUT_DIR / f\"result_{case['id']}.png\"\n",
    "    save_image(viz, output_path)\n",
    "    print(f\"   âœ“ å·²ä¿å­˜: {output_path}\")\n",
    "    \n",
    "    # JSONå ±å‘Š\n",
    "    report = {\n",
    "        'case_id': case['id'],\n",
    "        'original_shape': img_meta['shape'].tolist(),\n",
    "        'original_spacing': img_meta['spacing'].tolist(),\n",
    "        'resampled_shape': [target_size] * 3,\n",
    "        'resampled_spacing': new_spacing.tolist(),\n",
    "        'features': features,\n",
    "        'timestamp': datetime.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    json_path = OUTPUT_DIR / f\"report_{case['id']}.json\"\n",
    "    with open(json_path, 'w') as f:\n",
    "        json.dump(report, f, indent=2)\n",
    "    print(f\"   âœ“ å·²ä¿å­˜: {json_path}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"âœ“ è™•ç†å®Œæˆ!\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"\\nğŸ“Š æœ€çµ‚çµæœ:\")\n",
    "    print(f\"  ç¸½é«”ç©: {features['total_volume']:.1f} mmÂ³\")\n",
    "    print(f\"  Diceåˆ†æ•¸: {features['dice']:.3f}\")\n",
    "    print(f\"\\nè¼¸å‡ºä½ç½®: {OUTPUT_DIR}/\")\n",
    "    print(\"\\nâœ… ç³»çµ±é‹è¡ŒæˆåŠŸ!\")\n",
    "    \n",
    "    return viz, features\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        result_viz, result_features = main()\n",
    "    except Exception as e:\n",
    "        print(f\"\\nâŒ éŒ¯èª¤: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "02b8fdfd-3d38-4e2a-9fb0-5fd2d2e195e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ğŸ§  é†«å­¸å½±åƒåˆ†å‰²è¨“ç·´ç³»çµ± - å°ˆæ¥­ç‰ˆ\n",
      "================================================================================\n",
      "âœ“ PyTorch 2.1.0+cu118\n",
      "âœ“ CUDA: True\n",
      "âœ“ cuDNN benchmark enabled\n",
      "\n",
      "ä½¿ç”¨è¨­å‚™: cuda\n",
      "\n",
      "æº–å‚™è³‡æ–™...\n",
      "  è¨“ç·´é›†: 208 å€‹æ¡ˆä¾‹\n",
      "  é©—è­‰é›†: 52 å€‹æ¡ˆä¾‹\n",
      "\n",
      "å»ºç«‹æ¨¡å‹...\n",
      "  åƒæ•¸: 1,402,003\n",
      "  AMP enabled: True\n",
      "\n",
      "é–‹å§‹è¨“ç·´ 10 epochs\n",
      "================================================================================\n",
      "\n",
      "Epoch 1/10\n",
      "----------------------------------------\n",
      "    Batch 0/104, Loss: 0.5706\n",
      "    Batch 10/104, Loss: 0.4696\n",
      "    Batch 20/104, Loss: 0.4542\n",
      "    Batch 30/104, Loss: 0.4323\n",
      "    Batch 40/104, Loss: 0.4178\n",
      "    Batch 50/104, Loss: 0.4056\n",
      "    Batch 60/104, Loss: 0.4053\n",
      "    Batch 70/104, Loss: 0.3969\n",
      "    Batch 80/104, Loss: 0.3498\n",
      "    Batch 90/104, Loss: 0.3299\n",
      "    Batch 100/104, Loss: 0.3091\n",
      "\n",
      "  è¨“ç·´æå¤±: 0.4097\n",
      "  é©—è­‰æå¤±: 0.3119\n",
      "  é©—è­‰ Dice: 0.7447\n",
      "  å­¸ç¿’ç‡: 0.001000\n",
      "  âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹ (Dice: 0.7447)\n",
      "\n",
      "Epoch 2/10\n",
      "----------------------------------------\n",
      "    Batch 0/104, Loss: 0.3355\n",
      "    Batch 10/104, Loss: 0.2731\n",
      "    Batch 20/104, Loss: 0.2825\n",
      "    Batch 30/104, Loss: 0.2375\n",
      "    Batch 40/104, Loss: 0.2094\n",
      "    Batch 50/104, Loss: 0.2021\n",
      "    Batch 60/104, Loss: 0.1952\n",
      "    Batch 70/104, Loss: 0.1852\n",
      "    Batch 80/104, Loss: 0.1330\n",
      "    Batch 90/104, Loss: 0.1436\n",
      "    Batch 100/104, Loss: 0.1489\n",
      "\n",
      "  è¨“ç·´æå¤±: 0.2093\n",
      "  é©—è­‰æå¤±: 0.1442\n",
      "  é©—è­‰ Dice: 0.8265\n",
      "  å­¸ç¿’ç‡: 0.001000\n",
      "  âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹ (Dice: 0.8265)\n",
      "\n",
      "Epoch 3/10\n",
      "----------------------------------------\n",
      "    Batch 0/104, Loss: 0.1185\n",
      "    Batch 10/104, Loss: 0.1186\n",
      "    Batch 20/104, Loss: 0.1181\n",
      "    Batch 30/104, Loss: 0.1157\n",
      "    Batch 40/104, Loss: 0.1015\n",
      "    Batch 50/104, Loss: 0.0916\n",
      "    Batch 60/104, Loss: 0.0949\n",
      "    Batch 70/104, Loss: 0.1114\n",
      "    Batch 80/104, Loss: 0.0838\n",
      "    Batch 90/104, Loss: 0.0835\n",
      "    Batch 100/104, Loss: 0.0836\n",
      "\n",
      "  è¨“ç·´æå¤±: 0.1086\n",
      "  é©—è­‰æå¤±: 0.0992\n",
      "  é©—è­‰ Dice: 0.8518\n",
      "  å­¸ç¿’ç‡: 0.001000\n",
      "  âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹ (Dice: 0.8518)\n",
      "\n",
      "Epoch 4/10\n",
      "----------------------------------------\n",
      "    Batch 0/104, Loss: 0.0792\n",
      "    Batch 10/104, Loss: 0.0927\n",
      "    Batch 20/104, Loss: 0.1016\n",
      "    Batch 30/104, Loss: 0.0692\n",
      "    Batch 40/104, Loss: 0.0928\n",
      "    Batch 50/104, Loss: 0.0895\n",
      "    Batch 60/104, Loss: 0.0750\n",
      "    Batch 70/104, Loss: 0.0633\n",
      "    Batch 80/104, Loss: 0.0996\n",
      "    Batch 90/104, Loss: 0.0953\n",
      "    Batch 100/104, Loss: 0.0859\n",
      "\n",
      "  è¨“ç·´æå¤±: 0.0869\n",
      "  é©—è­‰æå¤±: 0.0883\n",
      "  é©—è­‰ Dice: 0.8585\n",
      "  å­¸ç¿’ç‡: 0.001000\n",
      "  âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹ (Dice: 0.8585)\n",
      "\n",
      "Epoch 5/10\n",
      "----------------------------------------\n",
      "    Batch 0/104, Loss: 0.0762\n",
      "    Batch 10/104, Loss: 0.0963\n",
      "    Batch 20/104, Loss: 0.0751\n",
      "    Batch 30/104, Loss: 0.0911\n",
      "    Batch 40/104, Loss: 0.0719\n",
      "    Batch 50/104, Loss: 0.0832\n",
      "    Batch 60/104, Loss: 0.0663\n",
      "    Batch 70/104, Loss: 0.0628\n",
      "    Batch 80/104, Loss: 0.0784\n",
      "    Batch 90/104, Loss: 0.0645\n",
      "    Batch 100/104, Loss: 0.0893\n",
      "\n",
      "  è¨“ç·´æå¤±: 0.0792\n",
      "  é©—è­‰æå¤±: 0.0811\n",
      "  é©—è­‰ Dice: 0.8634\n",
      "  å­¸ç¿’ç‡: 0.001000\n",
      "  âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹ (Dice: 0.8634)\n",
      "\n",
      "Epoch 6/10\n",
      "----------------------------------------\n",
      "    Batch 0/104, Loss: 0.0692\n",
      "    Batch 10/104, Loss: 0.0694\n",
      "    Batch 20/104, Loss: 0.0898\n",
      "    Batch 30/104, Loss: 0.0547\n",
      "    Batch 40/104, Loss: 0.0662\n",
      "    Batch 50/104, Loss: 0.0735\n",
      "    Batch 60/104, Loss: 0.0797\n",
      "    Batch 70/104, Loss: 0.0604\n",
      "    Batch 80/104, Loss: 0.0779\n",
      "    Batch 90/104, Loss: 0.0737\n",
      "    Batch 100/104, Loss: 0.0794\n",
      "\n",
      "  è¨“ç·´æå¤±: 0.0726\n",
      "  é©—è­‰æå¤±: 0.0756\n",
      "  é©—è­‰ Dice: 0.8717\n",
      "  å­¸ç¿’ç‡: 0.001000\n",
      "  âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹ (Dice: 0.8717)\n",
      "\n",
      "Epoch 7/10\n",
      "----------------------------------------\n",
      "    Batch 0/104, Loss: 0.0598\n",
      "    Batch 10/104, Loss: 0.0668\n",
      "    Batch 20/104, Loss: 0.0569\n",
      "    Batch 30/104, Loss: 0.0498\n",
      "    Batch 40/104, Loss: 0.1033\n",
      "    Batch 50/104, Loss: 0.0520\n",
      "    Batch 60/104, Loss: 0.0857\n",
      "    Batch 70/104, Loss: 0.0803\n",
      "    Batch 80/104, Loss: 0.0573\n",
      "    Batch 90/104, Loss: 0.0463\n",
      "    Batch 100/104, Loss: 0.0631\n",
      "\n",
      "  è¨“ç·´æå¤±: 0.0709\n",
      "  é©—è­‰æå¤±: 0.0765\n",
      "  é©—è­‰ Dice: 0.8703\n",
      "  å­¸ç¿’ç‡: 0.001000\n",
      "\n",
      "Epoch 8/10\n",
      "----------------------------------------\n",
      "    Batch 0/104, Loss: 0.0643\n",
      "    Batch 10/104, Loss: 0.0831\n",
      "    Batch 20/104, Loss: 0.0615\n",
      "    Batch 30/104, Loss: 0.0527\n",
      "    Batch 40/104, Loss: 0.0835\n",
      "    Batch 50/104, Loss: 0.0886\n",
      "    Batch 60/104, Loss: 0.0516\n",
      "    Batch 70/104, Loss: 0.0918\n",
      "    Batch 80/104, Loss: 0.0716\n",
      "    Batch 90/104, Loss: 0.0531\n",
      "    Batch 100/104, Loss: 0.0618\n",
      "\n",
      "  è¨“ç·´æå¤±: 0.0678\n",
      "  é©—è­‰æå¤±: 0.0687\n",
      "  é©—è­‰ Dice: 0.8823\n",
      "  å­¸ç¿’ç‡: 0.001000\n",
      "  âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹ (Dice: 0.8823)\n",
      "\n",
      "Epoch 9/10\n",
      "----------------------------------------\n",
      "    Batch 0/104, Loss: 0.0508\n",
      "    Batch 10/104, Loss: 0.0525\n",
      "    Batch 20/104, Loss: 0.0465\n",
      "    Batch 30/104, Loss: 0.0612\n",
      "    Batch 40/104, Loss: 0.0553\n",
      "    Batch 50/104, Loss: 0.0788\n",
      "    Batch 60/104, Loss: 0.0563\n",
      "    Batch 70/104, Loss: 0.0825\n",
      "    Batch 80/104, Loss: 0.0746\n",
      "    Batch 90/104, Loss: 0.0683\n",
      "    Batch 100/104, Loss: 0.0690\n",
      "\n",
      "  è¨“ç·´æå¤±: 0.0655\n",
      "  é©—è­‰æå¤±: 0.0735\n",
      "  é©—è­‰ Dice: 0.8736\n",
      "  å­¸ç¿’ç‡: 0.001000\n",
      "\n",
      "Epoch 10/10\n",
      "----------------------------------------\n",
      "    Batch 0/104, Loss: 0.0703\n",
      "    Batch 10/104, Loss: 0.0749\n",
      "    Batch 20/104, Loss: 0.0480\n",
      "    Batch 30/104, Loss: 0.0742\n",
      "    Batch 40/104, Loss: 0.0709\n",
      "    Batch 50/104, Loss: 0.0446\n",
      "    Batch 60/104, Loss: 0.0728\n",
      "    Batch 70/104, Loss: 0.0553\n",
      "    Batch 80/104, Loss: 0.1032\n",
      "    Batch 90/104, Loss: 0.0688\n",
      "    Batch 100/104, Loss: 0.0559\n",
      "\n",
      "  è¨“ç·´æå¤±: 0.0644\n",
      "  é©—è­‰æå¤±: 0.0687\n",
      "  é©—è­‰ Dice: 0.8820\n",
      "  å­¸ç¿’ç‡: 0.000500\n",
      "\n",
      "================================================================================\n",
      "âœ“ è¨“ç·´å®Œæˆ! æœ€ä½³ Dice: 0.8823\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "ğŸ§  é†«å­¸å½±åƒåˆ†å‰²è¨“ç·´ç³»çµ± - å°ˆæ¥­å®Œæ•´ç‰ˆ\n",
    "æ•´åˆï¼šéš¨æ©Ÿåˆ‡åˆ†ã€é¡åˆ¥å¹³è¡¡ã€AMPã€æ¢¯åº¦è£å‰ªã€å„ªåŒ–çš„è©•ä¼°æŒ‡æ¨™\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ğŸ§  é†«å­¸å½±åƒåˆ†å‰²è¨“ç·´ç³»çµ± - å°ˆæ¥­ç‰ˆ\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "os.environ['TORCH_DISABLE_ONNX'] = '1'\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "print(f\"âœ“ PyTorch {torch.__version__}\")\n",
    "print(f\"âœ“ CUDA: {torch.cuda.is_available()}\")\n",
    "\n",
    "# cuDNN å„ªåŒ–\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    print(\"âœ“ cuDNN benchmark enabled\")\n",
    "\n",
    "# ==================== è³‡æ–™é›† ====================\n",
    "\n",
    "class HippocampusDataset(Dataset):\n",
    "    def __init__(self, data_root, indices, target_size=64):\n",
    "        self.data_root = Path(data_root)\n",
    "        self.target_size = target_size\n",
    "        \n",
    "        with open(self.data_root / 'dataset.json', 'r') as f:\n",
    "            metadata = json.load(f)\n",
    "        \n",
    "        training_data = metadata['training']\n",
    "        self.cases = [training_data[i] for i in indices]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.cases)\n",
    "    \n",
    "    def resize_3d(self, image, target_shape):\n",
    "        \"\"\"å‘é‡åŒ–æœ€è¿‘é„°ç¸®æ”¾\"\"\"\n",
    "        src_d, src_h, src_w = image.shape\n",
    "        dst_d, dst_h, dst_w = target_shape\n",
    "        \n",
    "        idx_d = np.clip(np.floor(np.arange(dst_d) * src_d / dst_d).astype(int), 0, src_d - 1)\n",
    "        idx_h = np.clip(np.floor(np.arange(dst_h) * src_h / dst_h).astype(int), 0, src_h - 1)\n",
    "        idx_w = np.clip(np.floor(np.arange(dst_w) * src_w / dst_w).astype(int), 0, src_w - 1)\n",
    "        \n",
    "        return image[idx_d[:, None, None], idx_h[None, :, None], idx_w[None, None, :]]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        case = self.cases[idx]\n",
    "        img_path = self.data_root / case['image'].lstrip('./')\n",
    "        lbl_path = self.data_root / case['label'].lstrip('./')\n",
    "        \n",
    "        # è¼‰å…¥å½±åƒ - ç›´æ¥è½‰ float32 ç¯€çœè¨˜æ†¶é«”\n",
    "        image = nib.load(str(img_path)).get_fdata(dtype=np.float32)\n",
    "        p1, p99 = np.percentile(image, [1, 99])\n",
    "        image = np.clip(image, p1, p99)\n",
    "        mean, std = image.mean(), image.std()\n",
    "        if std > 1e-8:\n",
    "            image = (image - mean) / std\n",
    "        \n",
    "        # è¼‰å…¥æ¨™è¨» - float32\n",
    "        label = nib.load(str(lbl_path)).get_fdata(dtype=np.float32)\n",
    "        \n",
    "        # ç¸®æ”¾\n",
    "        image = self.resize_3d(image, (self.target_size,) * 3)\n",
    "        label = self.resize_3d(label, (self.target_size,) * 3)\n",
    "        label = np.round(label).astype(np.int64)\n",
    "        \n",
    "        return torch.FloatTensor(image).unsqueeze(0), torch.LongTensor(label)\n",
    "\n",
    "# ==================== è³‡æ–™åˆ‡åˆ†ï¼ˆéš¨æ©Ÿï¼‰ ====================\n",
    "\n",
    "def get_train_val_split(num_samples, train_ratio=0.8, seed=42):\n",
    "    \"\"\"å›ºå®šéš¨æ©Ÿç¨®å­çš„è³‡æ–™åˆ‡åˆ†\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.permutation(num_samples)\n",
    "    split_idx = int(num_samples * train_ratio)\n",
    "    return indices[:split_idx], indices[split_idx:]\n",
    "\n",
    "# ==================== æ¨¡å‹ ====================\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv3d(in_ch, out_ch, 3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm3d(out_ch)\n",
    "        self.conv2 = nn.Conv3d(out_ch, out_ch, 3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm3d(out_ch)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.relu(self.bn2(self.conv2(self.relu(self.bn1(self.conv1(x))))))\n",
    "\n",
    "class UNet3D(nn.Module):\n",
    "    def __init__(self, in_ch=1, num_classes=3, base=16):\n",
    "        super().__init__()\n",
    "        self.enc1 = ConvBlock(in_ch, base)\n",
    "        self.enc2 = ConvBlock(base, base * 2)\n",
    "        self.enc3 = ConvBlock(base * 2, base * 4)\n",
    "        self.bottleneck = ConvBlock(base * 4, base * 8)\n",
    "        \n",
    "        self.up3 = nn.ConvTranspose3d(base * 8, base * 4, 2, stride=2)\n",
    "        self.dec3 = ConvBlock(base * 8, base * 4)\n",
    "        self.up2 = nn.ConvTranspose3d(base * 4, base * 2, 2, stride=2)\n",
    "        self.dec2 = ConvBlock(base * 4, base * 2)\n",
    "        self.up1 = nn.ConvTranspose3d(base * 2, base, 2, stride=2)\n",
    "        self.dec1 = ConvBlock(base * 2, base)\n",
    "        \n",
    "        self.out = nn.Conv3d(base, num_classes, 1)\n",
    "        self.pool = nn.MaxPool3d(2, 2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        e1 = self.enc1(x)\n",
    "        e2 = self.enc2(self.pool(e1))\n",
    "        e3 = self.enc3(self.pool(e2))\n",
    "        b = self.bottleneck(self.pool(e3))\n",
    "        \n",
    "        d3 = self.dec3(torch.cat([self.up3(b), e3], 1))\n",
    "        d2 = self.dec2(torch.cat([self.up2(d3), e2], 1))\n",
    "        d1 = self.dec1(torch.cat([self.up1(d2), e1], 1))\n",
    "        \n",
    "        return self.out(d1)\n",
    "\n",
    "# ==================== æå¤±å‡½æ•¸ï¼ˆé¡åˆ¥å¹³è¡¡ï¼‰====================\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    \"\"\"Focal Loss è™•ç†é¡åˆ¥ä¸å¹³è¡¡\"\"\"\n",
    "    def __init__(self, alpha=None, gamma=2.0):\n",
    "        super().__init__()\n",
    "        if alpha is not None:\n",
    "            self.register_buffer('alpha', alpha)\n",
    "        else:\n",
    "            self.alpha = None\n",
    "        self.gamma = gamma\n",
    "    \n",
    "    def forward(self, pred, target):\n",
    "        ce_loss = F.cross_entropy(pred, target, reduction='none')\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = ((1 - pt) ** self.gamma) * ce_loss\n",
    "        \n",
    "        if self.alpha is not None:\n",
    "            alpha_t = self.alpha[target]\n",
    "            focal_loss = alpha_t * focal_loss\n",
    "        \n",
    "        return focal_loss.mean()\n",
    "\n",
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, smooth=1.0):\n",
    "        super().__init__()\n",
    "        self.smooth = smooth\n",
    "    \n",
    "    def forward(self, pred, target):\n",
    "        pred = F.softmax(pred, dim=1)\n",
    "        target_one_hot = F.one_hot(target, pred.shape[1]).permute(0, 4, 1, 2, 3).float()\n",
    "        \n",
    "        inter = (pred * target_one_hot).sum(dim=(2, 3, 4))\n",
    "        union = pred.sum(dim=(2, 3, 4)) + target_one_hot.sum(dim=(2, 3, 4))\n",
    "        dice = (2. * inter + self.smooth) / (union + self.smooth)\n",
    "        \n",
    "        return 1 - dice.mean()\n",
    "\n",
    "class CombinedLoss(nn.Module):\n",
    "    def __init__(self, use_focal=True):\n",
    "        super().__init__()\n",
    "        if use_focal:\n",
    "            # èƒŒæ™¯æ¬Šé‡è¼ƒä½\n",
    "            alpha = torch.tensor([0.2, 1.0, 1.0])\n",
    "            self.ce = FocalLoss(alpha=alpha, gamma=2.0)\n",
    "        else:\n",
    "            # å‚³çµ±åŠ æ¬Š CE\n",
    "            weight = torch.tensor([0.2, 1.0, 1.0])\n",
    "            self.ce = nn.CrossEntropyLoss(weight=weight)\n",
    "        \n",
    "        self.dice = DiceLoss()\n",
    "    \n",
    "    def forward(self, pred, target):\n",
    "        return 0.3 * self.ce(pred, target) + 0.7 * self.dice(pred, target)\n",
    "\n",
    "# ==================== Adam with Weight Decay ====================\n",
    "\n",
    "class AdamW:\n",
    "    \"\"\"æ‰‹å‹•å¯¦ç¾ AdamWï¼Œæ”¯æ´ weight decay\"\"\"\n",
    "    def __init__(self, params, lr=1e-3, weight_decay=1e-4):\n",
    "        self.params = list(params)\n",
    "        self.lr = lr\n",
    "        self.weight_decay = weight_decay\n",
    "        self.beta1 = 0.9\n",
    "        self.beta2 = 0.999\n",
    "        self.eps = 1e-8\n",
    "        self.t = 0\n",
    "        \n",
    "        # ç‚ºäº†ç›¸å®¹ GradScalerï¼Œæ·»åŠ  param_groups\n",
    "        self.param_groups = [{'params': self.params, 'lr': lr}]\n",
    "        \n",
    "        self.m = [torch.zeros_like(p.data) for p in self.params]\n",
    "        self.v = [torch.zeros_like(p.data) for p in self.params]\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        for p in self.params:\n",
    "            if p.grad is not None:\n",
    "                p.grad.zero_()\n",
    "    \n",
    "    def step(self, closure=None):\n",
    "        \"\"\"æ·»åŠ  closure åƒæ•¸ä»¥ç›¸å®¹ optimizer ä»‹é¢\"\"\"\n",
    "        self.t += 1\n",
    "        for i, p in enumerate(self.params):\n",
    "            if p.grad is None:\n",
    "                continue\n",
    "            \n",
    "            grad = p.grad.data\n",
    "            \n",
    "            # Weight decay\n",
    "            if self.weight_decay > 0:\n",
    "                p.data.mul_(1 - self.lr * self.weight_decay)\n",
    "            \n",
    "            # Adam æ›´æ–°\n",
    "            self.m[i] = self.beta1 * self.m[i] + (1 - self.beta1) * grad\n",
    "            self.v[i] = self.beta2 * self.v[i] + (1 - self.beta2) * grad ** 2\n",
    "            \n",
    "            m_hat = self.m[i] / (1 - self.beta1 ** self.t)\n",
    "            v_hat = self.v[i] / (1 - self.beta2 ** self.t)\n",
    "            \n",
    "            p.data -= self.lr * m_hat / (torch.sqrt(v_hat) + self.eps)\n",
    "    \n",
    "    def set_lr(self, lr):\n",
    "        \"\"\"å‹•æ…‹èª¿æ•´å­¸ç¿’ç‡\"\"\"\n",
    "        self.lr = lr\n",
    "        self.param_groups[0]['lr'] = lr\n",
    "\n",
    "# ==================== è¨“ç·´å™¨ï¼ˆå„ªåŒ–ç‰ˆï¼‰====================\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model, train_loader, val_loader, device, output_dir, use_amp=True):\n",
    "        self.model = model.to(device)\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.device = device\n",
    "        self.output_dir = Path(output_dir)\n",
    "        self.output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        self.optimizer = AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "        self.criterion = CombinedLoss(use_focal=True).to(device)\n",
    "        \n",
    "        # AMP\n",
    "        self.use_amp = use_amp and torch.cuda.is_available()\n",
    "        self.scaler = GradScaler() if self.use_amp else None\n",
    "        \n",
    "        self.history = {'train_loss': [], 'val_loss': [], 'val_dice': []}\n",
    "        self.best_dice = 0.0\n",
    "        self.lr = 1e-3\n",
    "        \n",
    "        print(f\"  AMP enabled: {self.use_amp}\")\n",
    "    \n",
    "    def train_epoch(self):\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        for batch_idx, (images, labels) in enumerate(self.train_loader):\n",
    "            images, labels = images.to(self.device), labels.to(self.device)\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "            # AMP forward\n",
    "            if self.use_amp:\n",
    "                with autocast():\n",
    "                    outputs = self.model(images)\n",
    "                    loss = self.criterion(outputs, labels)\n",
    "                \n",
    "                self.scaler.scale(loss).backward()\n",
    "                \n",
    "                # æ¢¯åº¦è£å‰ª\n",
    "                self.scaler.unscale_(self.optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "                \n",
    "                self.scaler.step(self.optimizer)\n",
    "                self.scaler.update()\n",
    "            else:\n",
    "                outputs = self.model(images)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                \n",
    "                # æ¢¯åº¦è£å‰ª\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "                \n",
    "                self.optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            if batch_idx % 10 == 0:\n",
    "                print(f\"    Batch {batch_idx}/{len(self.train_loader)}, Loss: {loss.item():.4f}\")\n",
    "        \n",
    "        return total_loss / len(self.train_loader)\n",
    "    \n",
    "    def validate(self):\n",
    "        \"\"\"ç´¯ç©å¼ Dice è¨ˆç®—\"\"\"\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        \n",
    "        # ç´¯ç© intersection å’Œ union\n",
    "        total_inter = {1: 0.0, 2: 0.0}\n",
    "        total_union = {1: 0.0, 2: 0.0}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels in self.val_loader:\n",
    "                images, labels = images.to(self.device), labels.to(self.device)\n",
    "                \n",
    "                if self.use_amp:\n",
    "                    with autocast():\n",
    "                        outputs = self.model(images)\n",
    "                        loss = self.criterion(outputs, labels)\n",
    "                else:\n",
    "                    outputs = self.model(images)\n",
    "                    loss = self.criterion(outputs, labels)\n",
    "                \n",
    "                preds = torch.argmax(outputs, dim=1)\n",
    "                \n",
    "                # ç´¯ç©æ¯å€‹é¡åˆ¥çš„ intersection å’Œ union\n",
    "                for cls in [1, 2]:\n",
    "                    pred_mask = (preds == cls).float()\n",
    "                    target_mask = (labels == cls).float()\n",
    "                    \n",
    "                    total_inter[cls] += (pred_mask * target_mask).sum().item()\n",
    "                    total_union[cls] += (pred_mask.sum() + target_mask.sum()).item()\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "        \n",
    "        # è¨ˆç®—æ•´é«” Dice\n",
    "        dice_scores = []\n",
    "        for cls in [1, 2]:\n",
    "            dice = (2.0 * total_inter[cls]) / (total_union[cls] + 1e-8)\n",
    "            dice_scores.append(dice)\n",
    "        \n",
    "        avg_dice = np.mean(dice_scores)\n",
    "        \n",
    "        return total_loss / len(self.val_loader), avg_dice\n",
    "    \n",
    "    def train(self, num_epochs):\n",
    "        print(f\"\\né–‹å§‹è¨“ç·´ {num_epochs} epochs\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        for epoch in range(1, num_epochs + 1):\n",
    "            print(f\"\\nEpoch {epoch}/{num_epochs}\")\n",
    "            print(\"-\" * 40)\n",
    "            \n",
    "            train_loss = self.train_epoch()\n",
    "            val_loss, val_dice = self.validate()\n",
    "            \n",
    "            self.history['train_loss'].append(train_loss)\n",
    "            self.history['val_loss'].append(val_loss)\n",
    "            self.history['val_dice'].append(val_dice)\n",
    "            \n",
    "            # å­¸ç¿’ç‡è¡°æ¸›\n",
    "            if epoch % 10 == 0:\n",
    "                self.lr *= 0.5\n",
    "                self.optimizer.set_lr(self.lr)\n",
    "            \n",
    "            print(f\"\\n  è¨“ç·´æå¤±: {train_loss:.4f}\")\n",
    "            print(f\"  é©—è­‰æå¤±: {val_loss:.4f}\")\n",
    "            print(f\"  é©—è­‰ Dice: {val_dice:.4f}\")\n",
    "            print(f\"  å­¸ç¿’ç‡: {self.lr:.6f}\")\n",
    "            \n",
    "            # ä¿å­˜æœ€ä½³\n",
    "            if val_dice > self.best_dice:\n",
    "                self.best_dice = val_dice\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model': self.model.state_dict(),\n",
    "                    'dice': val_dice,\n",
    "                    'history': self.history\n",
    "                }, self.output_dir / 'best_model.pth')\n",
    "                print(f\"  âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹ (Dice: {val_dice:.4f})\")\n",
    "        \n",
    "        # ä¿å­˜æ­·å²\n",
    "        with open(self.output_dir / 'history.json', 'w') as f:\n",
    "            json.dump(self.history, f, indent=2)\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(f\"âœ“ è¨“ç·´å®Œæˆ! æœ€ä½³ Dice: {self.best_dice:.4f}\")\n",
    "\n",
    "# ==================== ä¸»ç¨‹åº ====================\n",
    "\n",
    "def main():\n",
    "    DATA_ROOT = '/workspace/Task04_Hippocampus'\n",
    "    OUTPUT_DIR = '/workspace/outputs/training_pro'\n",
    "    BATCH_SIZE = 2\n",
    "    NUM_EPOCHS = 10\n",
    "    TARGET_SIZE = 64\n",
    "    USE_AMP = True\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"\\nä½¿ç”¨è¨­å‚™: {device}\")\n",
    "    \n",
    "    # éš¨æ©Ÿåˆ‡åˆ†è³‡æ–™\n",
    "    print(\"\\næº–å‚™è³‡æ–™...\")\n",
    "    with open(Path(DATA_ROOT) / 'dataset.json') as f:\n",
    "        num_samples = len(json.load(f)['training'])\n",
    "    \n",
    "    train_indices, val_indices = get_train_val_split(num_samples, train_ratio=0.8, seed=42)\n",
    "    print(f\"  è¨“ç·´é›†: {len(train_indices)} å€‹æ¡ˆä¾‹\")\n",
    "    print(f\"  é©—è­‰é›†: {len(val_indices)} å€‹æ¡ˆä¾‹\")\n",
    "    \n",
    "    train_dataset = HippocampusDataset(DATA_ROOT, train_indices, TARGET_SIZE)\n",
    "    val_dataset = HippocampusDataset(DATA_ROOT, val_indices, TARGET_SIZE)\n",
    "    \n",
    "    # å„ªåŒ–çš„ DataLoader åƒæ•¸\n",
    "    num_workers = min(4, os.cpu_count() or 1) if torch.cuda.is_available() else 0\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=torch.cuda.is_available(),\n",
    "        persistent_workers=num_workers > 0,\n",
    "        prefetch_factor=2 if num_workers > 0 else None\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=torch.cuda.is_available()\n",
    "    )\n",
    "    \n",
    "    print(\"\\nå»ºç«‹æ¨¡å‹...\")\n",
    "    model = UNet3D(1, 3, 16)\n",
    "    print(f\"  åƒæ•¸: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    \n",
    "    trainer = Trainer(model, train_loader, val_loader, device, OUTPUT_DIR, use_amp=USE_AMP)\n",
    "    trainer.train(NUM_EPOCHS)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        main()\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nè¨“ç·´ä¸­æ–·\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\néŒ¯èª¤: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b98f35d1-5879-40aa-80fc-ee20d7878b8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ğŸ§  é†«å­¸å½±åƒåˆ†å‰²è¨“ç·´ç³»çµ± - ç”Ÿç”¢ç´š\n",
      "================================================================================\n",
      "\n",
      "ğŸ”§ è¨­ç½®ç’°å¢ƒ...\n",
      "  âœ“ å·²çŸ­è·¯ torch._compile å’Œ torch.onnx\n",
      "\n",
      "âœ“ PyTorch 2.1.0+cu118\n",
      "âœ“ CUDA: True\n",
      "âœ“ cuDNN benchmark enabled\n",
      "\n",
      "ğŸ² è¨­ç½®éš¨æ©Ÿç¨®å­: 42\n",
      "ä½¿ç”¨è¨­å‚™: cuda\n",
      "\n",
      "æº–å‚™è³‡æ–™...\n",
      "  è¨“ç·´é›†: 208 å€‹æ¡ˆä¾‹\n",
      "  é©—è­‰é›†: 52 å€‹æ¡ˆä¾‹\n",
      "  DataLoader workers: 4\n",
      "  å½±åƒæ’å€¼: trilinear\n",
      "  æ¨™è¨»æ’å€¼: nearest\n",
      "\n",
      "å»ºç«‹æ¨¡å‹...\n",
      "  åƒæ•¸: 1,402,003\n",
      "  ä½¿ç”¨ GroupNorm (ç©©å®šæ–¼å° batch)\n",
      "  ä½¿ç”¨ SGD + Nesterov momentum\n",
      "  ä½¿ç”¨ Cosine Annealing LR (T_max=30)\n",
      "\n",
      "é–‹å§‹è¨“ç·´ 30 epochs\n",
      "================================================================================\n",
      "\n",
      "Epoch 1/30\n",
      "----------------------------------------\n",
      "    Batch 0/104, Loss: 0.6117\n",
      "    Batch 10/104, Loss: 0.5528\n",
      "    Batch 20/104, Loss: 0.5192\n",
      "    Batch 30/104, Loss: 0.4926\n",
      "    Batch 40/104, Loss: 0.4747\n",
      "    Batch 50/104, Loss: 0.4702\n",
      "    Batch 60/104, Loss: 0.4432\n",
      "    Batch 70/104, Loss: 0.4337\n",
      "    Batch 80/104, Loss: 0.4114\n",
      "    Batch 90/104, Loss: 0.4004\n",
      "    Batch 100/104, Loss: 0.3695\n",
      "\n",
      "  è¨“ç·´æå¤±: 0.4646\n",
      "  é©—è­‰æå¤±: 0.3680\n",
      "  é©—è­‰ Dice: 0.0298\n",
      "  å­¸ç¿’ç‡: 0.009973\n",
      "  âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹ (Dice: 0.0298)\n",
      "\n",
      "Epoch 2/30\n",
      "----------------------------------------\n",
      "    Batch 0/104, Loss: 0.3700\n",
      "    Batch 10/104, Loss: 0.3577\n",
      "    Batch 20/104, Loss: 0.3397\n",
      "    Batch 30/104, Loss: 0.3447\n",
      "    Batch 40/104, Loss: 0.3026\n",
      "    Batch 50/104, Loss: 0.2982\n",
      "    Batch 60/104, Loss: 0.2739\n",
      "    Batch 70/104, Loss: 0.2777\n",
      "    Batch 80/104, Loss: 0.2624\n",
      "    Batch 90/104, Loss: 0.2459\n",
      "    Batch 100/104, Loss: 0.2423\n",
      "\n",
      "  è¨“ç·´æå¤±: 0.2968\n",
      "  é©—è­‰æå¤±: 0.2326\n",
      "  é©—è­‰ Dice: 0.5295\n",
      "  å­¸ç¿’ç‡: 0.009891\n",
      "  âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹ (Dice: 0.5295)\n",
      "\n",
      "Epoch 3/30\n",
      "----------------------------------------\n",
      "    Batch 0/104, Loss: 0.2316\n",
      "    Batch 10/104, Loss: 0.2167\n",
      "    Batch 20/104, Loss: 0.2134\n",
      "    Batch 30/104, Loss: 0.1640\n",
      "    Batch 40/104, Loss: 0.1266\n",
      "    Batch 50/104, Loss: 0.1207\n",
      "    Batch 60/104, Loss: 0.1205\n",
      "    Batch 70/104, Loss: 0.1110\n",
      "    Batch 80/104, Loss: 0.1016\n",
      "    Batch 90/104, Loss: 0.1049\n",
      "    Batch 100/104, Loss: 0.0846\n",
      "\n",
      "  è¨“ç·´æå¤±: 0.1438\n",
      "  é©—è­‰æå¤±: 0.0982\n",
      "  é©—è­‰ Dice: 0.8400\n",
      "  å­¸ç¿’ç‡: 0.009756\n",
      "  âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹ (Dice: 0.8400)\n",
      "\n",
      "Epoch 4/30\n",
      "----------------------------------------\n",
      "    Batch 0/104, Loss: 0.1001\n",
      "    Batch 10/104, Loss: 0.0889\n",
      "    Batch 20/104, Loss: 0.1027\n",
      "    Batch 30/104, Loss: 0.1054\n",
      "    Batch 40/104, Loss: 0.0920\n",
      "    Batch 50/104, Loss: 0.0986\n",
      "    Batch 60/104, Loss: 0.0916\n",
      "    Batch 70/104, Loss: 0.0857\n",
      "    Batch 80/104, Loss: 0.0809\n",
      "    Batch 90/104, Loss: 0.0928\n",
      "    Batch 100/104, Loss: 0.1099\n",
      "\n",
      "  è¨“ç·´æå¤±: 0.0912\n",
      "  é©—è­‰æå¤±: 0.0880\n",
      "  é©—è­‰ Dice: 0.8475\n",
      "  å­¸ç¿’ç‡: 0.009568\n",
      "  âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹ (Dice: 0.8475)\n",
      "\n",
      "Epoch 5/30\n",
      "----------------------------------------\n",
      "    Batch 0/104, Loss: 0.0744\n",
      "    Batch 10/104, Loss: 0.0809\n",
      "    Batch 20/104, Loss: 0.0780\n",
      "    Batch 30/104, Loss: 0.0777\n",
      "    Batch 40/104, Loss: 0.0817\n",
      "    Batch 50/104, Loss: 0.0834\n",
      "    Batch 60/104, Loss: 0.0762\n",
      "    Batch 70/104, Loss: 0.0729\n",
      "    Batch 80/104, Loss: 0.0729\n",
      "    Batch 90/104, Loss: 0.0869\n",
      "    Batch 100/104, Loss: 0.0868\n",
      "\n",
      "  è¨“ç·´æå¤±: 0.0830\n",
      "  é©—è­‰æå¤±: 0.0842\n",
      "  é©—è­‰ Dice: 0.8532\n",
      "  å­¸ç¿’ç‡: 0.009331\n",
      "  âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹ (Dice: 0.8532)\n",
      "\n",
      "Epoch 6/30\n",
      "----------------------------------------\n",
      "    Batch 0/104, Loss: 0.0666\n",
      "    Batch 10/104, Loss: 0.0669\n",
      "    Batch 20/104, Loss: 0.0872\n",
      "    Batch 30/104, Loss: 0.0969\n",
      "    Batch 40/104, Loss: 0.0750\n",
      "    Batch 50/104, Loss: 0.1094\n",
      "    Batch 60/104, Loss: 0.0908\n",
      "    Batch 70/104, Loss: 0.0876\n",
      "    Batch 80/104, Loss: 0.0626\n",
      "    Batch 90/104, Loss: 0.0664\n",
      "    Batch 100/104, Loss: 0.0846\n",
      "\n",
      "  è¨“ç·´æå¤±: 0.0800\n",
      "  é©—è­‰æå¤±: 0.0817\n",
      "  é©—è­‰ Dice: 0.8547\n",
      "  å­¸ç¿’ç‡: 0.009046\n",
      "  âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹ (Dice: 0.8547)\n",
      "\n",
      "Epoch 7/30\n",
      "----------------------------------------\n",
      "    Batch 0/104, Loss: 0.0619\n",
      "    Batch 10/104, Loss: 0.0647\n",
      "    Batch 20/104, Loss: 0.0826\n",
      "    Batch 30/104, Loss: 0.1051\n",
      "    Batch 40/104, Loss: 0.0647\n",
      "    Batch 50/104, Loss: 0.0729\n",
      "    Batch 60/104, Loss: 0.0734\n",
      "    Batch 70/104, Loss: 0.0765\n",
      "    Batch 80/104, Loss: 0.0770\n",
      "    Batch 90/104, Loss: 0.0629\n",
      "    Batch 100/104, Loss: 0.0861\n",
      "\n",
      "  è¨“ç·´æå¤±: 0.0777\n",
      "  é©—è­‰æå¤±: 0.0815\n",
      "  é©—è­‰ Dice: 0.8580\n",
      "  å­¸ç¿’ç‡: 0.008717\n",
      "  âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹ (Dice: 0.8580)\n",
      "\n",
      "Epoch 8/30\n",
      "----------------------------------------\n",
      "    Batch 0/104, Loss: 0.0612\n",
      "    Batch 10/104, Loss: 0.0807\n",
      "    Batch 20/104, Loss: 0.0680\n",
      "    Batch 30/104, Loss: 0.0834\n",
      "    Batch 40/104, Loss: 0.0864\n",
      "    Batch 50/104, Loss: 0.0680\n",
      "    Batch 60/104, Loss: 0.0863\n",
      "    Batch 70/104, Loss: 0.0923\n",
      "    Batch 80/104, Loss: 0.0693\n",
      "    Batch 90/104, Loss: 0.0831\n",
      "    Batch 100/104, Loss: 0.0760\n",
      "\n",
      "  è¨“ç·´æå¤±: 0.0758\n",
      "  é©—è­‰æå¤±: 0.0801\n",
      "  é©—è­‰ Dice: 0.8579\n",
      "  å­¸ç¿’ç‡: 0.008347\n",
      "\n",
      "Epoch 9/30\n",
      "----------------------------------------\n",
      "    Batch 0/104, Loss: 0.0801\n",
      "    Batch 10/104, Loss: 0.0758\n",
      "    Batch 20/104, Loss: 0.0810\n",
      "    Batch 30/104, Loss: 0.0854\n",
      "    Batch 40/104, Loss: 0.0678\n",
      "    Batch 50/104, Loss: 0.0924\n",
      "    Batch 60/104, Loss: 0.0690\n",
      "    Batch 70/104, Loss: 0.0764\n",
      "    Batch 80/104, Loss: 0.0774\n",
      "    Batch 90/104, Loss: 0.0687\n",
      "    Batch 100/104, Loss: 0.0902\n",
      "\n",
      "  è¨“ç·´æå¤±: 0.0740\n",
      "  é©—è­‰æå¤±: 0.0788\n",
      "  é©—è­‰ Dice: 0.8605\n",
      "  å­¸ç¿’ç‡: 0.007941\n",
      "  âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹ (Dice: 0.8605)\n",
      "\n",
      "Epoch 10/30\n",
      "----------------------------------------\n",
      "    Batch 0/104, Loss: 0.0679\n",
      "    Batch 10/104, Loss: 0.0587\n",
      "    Batch 20/104, Loss: 0.0869\n",
      "    Batch 30/104, Loss: 0.0621\n",
      "    Batch 40/104, Loss: 0.0614\n",
      "    Batch 50/104, Loss: 0.0771\n",
      "    Batch 60/104, Loss: 0.0845\n",
      "    Batch 70/104, Loss: 0.0679\n",
      "    Batch 80/104, Loss: 0.0867\n",
      "    Batch 90/104, Loss: 0.0691\n",
      "    Batch 100/104, Loss: 0.0701\n",
      "\n",
      "  è¨“ç·´æå¤±: 0.0724\n",
      "  é©—è­‰æå¤±: 0.0772\n",
      "  é©—è­‰ Dice: 0.8647\n",
      "  å­¸ç¿’ç‡: 0.007503\n",
      "  âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹ (Dice: 0.8647)\n",
      "\n",
      "Epoch 11/30\n",
      "----------------------------------------\n",
      "    Batch 0/104, Loss: 0.0745\n",
      "    Batch 10/104, Loss: 0.0824\n",
      "    Batch 20/104, Loss: 0.0683\n",
      "    Batch 30/104, Loss: 0.0803\n",
      "    Batch 40/104, Loss: 0.0765\n",
      "    Batch 50/104, Loss: 0.0776\n",
      "    Batch 60/104, Loss: 0.0655\n",
      "    Batch 70/104, Loss: 0.0680\n",
      "    Batch 80/104, Loss: 0.0784\n",
      "    Batch 90/104, Loss: 0.0765\n",
      "    Batch 100/104, Loss: 0.0757\n",
      "\n",
      "  è¨“ç·´æå¤±: 0.0715\n",
      "  é©—è­‰æå¤±: 0.0775\n",
      "  é©—è­‰ Dice: 0.8641\n",
      "  å­¸ç¿’ç‡: 0.007037\n",
      "\n",
      "Epoch 12/30\n",
      "----------------------------------------\n",
      "    Batch 0/104, Loss: 0.0682\n",
      "    Batch 10/104, Loss: 0.0868\n",
      "    Batch 20/104, Loss: 0.0730\n",
      "    Batch 30/104, Loss: 0.0771\n",
      "    Batch 40/104, Loss: 0.0608\n",
      "    Batch 50/104, Loss: 0.0650\n",
      "    Batch 60/104, Loss: 0.0761\n",
      "    Batch 70/104, Loss: 0.0828\n",
      "    Batch 80/104, Loss: 0.0560\n",
      "    Batch 90/104, Loss: 0.0761\n",
      "    Batch 100/104, Loss: 0.0721\n",
      "\n",
      "  è¨“ç·´æå¤±: 0.0698\n",
      "  é©—è­‰æå¤±: 0.0762\n",
      "  é©—è­‰ Dice: 0.8662\n",
      "  å­¸ç¿’ç‡: 0.006549\n",
      "  âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹ (Dice: 0.8662)\n",
      "\n",
      "Epoch 13/30\n",
      "----------------------------------------\n",
      "    Batch 0/104, Loss: 0.0617\n",
      "    Batch 10/104, Loss: 0.0681\n",
      "    Batch 20/104, Loss: 0.0680\n",
      "    Batch 30/104, Loss: 0.0744\n",
      "    Batch 40/104, Loss: 0.0865\n",
      "    Batch 50/104, Loss: 0.0696\n",
      "    Batch 60/104, Loss: 0.0585\n",
      "    Batch 70/104, Loss: 0.0688\n",
      "    Batch 80/104, Loss: 0.0776\n",
      "    Batch 90/104, Loss: 0.0665\n",
      "    Batch 100/104, Loss: 0.0782\n",
      "\n",
      "  è¨“ç·´æå¤±: 0.0688\n",
      "  é©—è­‰æå¤±: 0.0756\n",
      "  é©—è­‰ Dice: 0.8666\n",
      "  å­¸ç¿’ç‡: 0.006044\n",
      "  âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹ (Dice: 0.8666)\n",
      "\n",
      "Epoch 14/30\n",
      "----------------------------------------\n",
      "    Batch 0/104, Loss: 0.0617\n",
      "    Batch 10/104, Loss: 0.0643\n",
      "    Batch 20/104, Loss: 0.0833\n",
      "    Batch 30/104, Loss: 0.0656\n",
      "    Batch 40/104, Loss: 0.0717\n",
      "    Batch 50/104, Loss: 0.0711\n",
      "    Batch 60/104, Loss: 0.0675\n",
      "    Batch 70/104, Loss: 0.0670\n",
      "    Batch 80/104, Loss: 0.0647\n",
      "    Batch 90/104, Loss: 0.0756\n",
      "    Batch 100/104, Loss: 0.0910\n",
      "\n",
      "  è¨“ç·´æå¤±: 0.0673\n",
      "  é©—è­‰æå¤±: 0.0764\n",
      "  é©—è­‰ Dice: 0.8658\n",
      "  å­¸ç¿’ç‡: 0.005527\n",
      "\n",
      "Epoch 15/30\n",
      "----------------------------------------\n",
      "    Batch 0/104, Loss: 0.0791\n",
      "    Batch 10/104, Loss: 0.0700\n",
      "    Batch 20/104, Loss: 0.0617\n",
      "    Batch 30/104, Loss: 0.0699\n",
      "    Batch 40/104, Loss: 0.0547\n",
      "    Batch 50/104, Loss: 0.0570\n",
      "    Batch 60/104, Loss: 0.0836\n",
      "    Batch 70/104, Loss: 0.0904\n",
      "    Batch 80/104, Loss: 0.0848\n",
      "    Batch 90/104, Loss: 0.0695\n",
      "    Batch 100/104, Loss: 0.0605\n",
      "\n",
      "  è¨“ç·´æå¤±: 0.0659\n",
      "  é©—è­‰æå¤±: 0.0775\n",
      "  é©—è­‰ Dice: 0.8624\n",
      "  å­¸ç¿’ç‡: 0.005005\n",
      "\n",
      "Epoch 16/30\n",
      "----------------------------------------\n",
      "    Batch 0/104, Loss: 0.0538\n",
      "    Batch 10/104, Loss: 0.0634\n",
      "    Batch 20/104, Loss: 0.0639\n",
      "    Batch 30/104, Loss: 0.0755\n",
      "    Batch 40/104, Loss: 0.0797\n",
      "    Batch 50/104, Loss: 0.0739\n",
      "    Batch 60/104, Loss: 0.0546\n",
      "    Batch 70/104, Loss: 0.0516\n",
      "    Batch 80/104, Loss: 0.0575\n",
      "    Batch 90/104, Loss: 0.0638\n",
      "    Batch 100/104, Loss: 0.0612\n",
      "\n",
      "  è¨“ç·´æå¤±: 0.0647\n",
      "  é©—è­‰æå¤±: 0.0754\n",
      "  é©—è­‰ Dice: 0.8671\n",
      "  å­¸ç¿’ç‡: 0.004483\n",
      "  âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹ (Dice: 0.8671)\n",
      "\n",
      "Epoch 17/30\n",
      "----------------------------------------\n",
      "    Batch 0/104, Loss: 0.0582\n",
      "    Batch 10/104, Loss: 0.0547\n",
      "    Batch 20/104, Loss: 0.0647\n",
      "    Batch 30/104, Loss: 0.0578\n",
      "    Batch 40/104, Loss: 0.0663\n",
      "    Batch 50/104, Loss: 0.0521\n",
      "    Batch 60/104, Loss: 0.0525\n",
      "    Batch 70/104, Loss: 0.0692\n",
      "    Batch 80/104, Loss: 0.0657\n",
      "    Batch 90/104, Loss: 0.0683\n",
      "    Batch 100/104, Loss: 0.0747\n",
      "\n",
      "  è¨“ç·´æå¤±: 0.0635\n",
      "  é©—è­‰æå¤±: 0.0761\n",
      "  é©—è­‰ Dice: 0.8665\n",
      "  å­¸ç¿’ç‡: 0.003966\n",
      "\n",
      "Epoch 18/30\n",
      "----------------------------------------\n",
      "    Batch 0/104, Loss: 0.0513\n",
      "    Batch 10/104, Loss: 0.0613\n",
      "    Batch 20/104, Loss: 0.0765\n",
      "    Batch 30/104, Loss: 0.0634\n",
      "    Batch 40/104, Loss: 0.0514\n",
      "    Batch 50/104, Loss: 0.0484\n",
      "    Batch 60/104, Loss: 0.0530\n",
      "    Batch 70/104, Loss: 0.0512\n",
      "    Batch 80/104, Loss: 0.0613\n",
      "    Batch 90/104, Loss: 0.0711\n",
      "    Batch 100/104, Loss: 0.0569\n",
      "\n",
      "  è¨“ç·´æå¤±: 0.0621\n",
      "  é©—è­‰æå¤±: 0.0765\n",
      "  é©—è­‰ Dice: 0.8660\n",
      "  å­¸ç¿’ç‡: 0.003461\n",
      "\n",
      "Epoch 19/30\n",
      "----------------------------------------\n",
      "    Batch 0/104, Loss: 0.0606\n",
      "    Batch 10/104, Loss: 0.0572\n",
      "    Batch 20/104, Loss: 0.0615\n",
      "    Batch 30/104, Loss: 0.0499\n",
      "    Batch 40/104, Loss: 0.0534\n",
      "    Batch 50/104, Loss: 0.0783\n",
      "    Batch 60/104, Loss: 0.0496\n",
      "    Batch 70/104, Loss: 0.0502\n",
      "    Batch 80/104, Loss: 0.0472\n",
      "    Batch 90/104, Loss: 0.0591\n",
      "    Batch 100/104, Loss: 0.0732\n",
      "\n",
      "  è¨“ç·´æå¤±: 0.0607\n",
      "  é©—è­‰æå¤±: 0.0766\n",
      "  é©—è­‰ Dice: 0.8660\n",
      "  å­¸ç¿’ç‡: 0.002973\n",
      "\n",
      "Epoch 20/30\n",
      "----------------------------------------\n",
      "    Batch 0/104, Loss: 0.0546\n",
      "    Batch 10/104, Loss: 0.0581\n",
      "    Batch 20/104, Loss: 0.0540\n",
      "    Batch 30/104, Loss: 0.0624\n",
      "    Batch 40/104, Loss: 0.0742\n",
      "    Batch 50/104, Loss: 0.0568\n",
      "    Batch 60/104, Loss: 0.0529\n",
      "    Batch 70/104, Loss: 0.0567\n",
      "    Batch 80/104, Loss: 0.0617\n",
      "    Batch 90/104, Loss: 0.0492\n",
      "    Batch 100/104, Loss: 0.0730\n",
      "\n",
      "  è¨“ç·´æå¤±: 0.0596\n",
      "  é©—è­‰æå¤±: 0.0761\n",
      "  é©—è­‰ Dice: 0.8657\n",
      "  å­¸ç¿’ç‡: 0.002508\n",
      "\n",
      "Epoch 21/30\n",
      "----------------------------------------\n",
      "    Batch 0/104, Loss: 0.0466\n",
      "    Batch 10/104, Loss: 0.0568\n",
      "    Batch 20/104, Loss: 0.0546\n",
      "    Batch 30/104, Loss: 0.0648\n",
      "    Batch 40/104, Loss: 0.0680\n",
      "    Batch 50/104, Loss: 0.0596\n",
      "    Batch 60/104, Loss: 0.0671\n",
      "    Batch 70/104, Loss: 0.0554\n",
      "    Batch 80/104, Loss: 0.0502\n",
      "    Batch 90/104, Loss: 0.0590\n",
      "    Batch 100/104, Loss: 0.0772\n",
      "\n",
      "  è¨“ç·´æå¤±: 0.0582\n",
      "  é©—è­‰æå¤±: 0.0764\n",
      "  é©—è­‰ Dice: 0.8658\n",
      "  å­¸ç¿’ç‡: 0.002069\n",
      "\n",
      "Epoch 22/30\n",
      "----------------------------------------\n",
      "    Batch 0/104, Loss: 0.0629\n",
      "    Batch 10/104, Loss: 0.0446\n",
      "    Batch 20/104, Loss: 0.0461\n",
      "    Batch 30/104, Loss: 0.0560\n",
      "    Batch 40/104, Loss: 0.0464\n",
      "    Batch 50/104, Loss: 0.0757\n",
      "    Batch 60/104, Loss: 0.0472\n",
      "    Batch 70/104, Loss: 0.0468\n",
      "    Batch 80/104, Loss: 0.0504\n",
      "    Batch 90/104, Loss: 0.0545\n",
      "    Batch 100/104, Loss: 0.0507\n",
      "\n",
      "  è¨“ç·´æå¤±: 0.0573\n",
      "  é©—è­‰æå¤±: 0.0768\n",
      "  é©—è­‰ Dice: 0.8655\n",
      "  å­¸ç¿’ç‡: 0.001663\n",
      "\n",
      "Epoch 23/30\n",
      "----------------------------------------\n",
      "    Batch 0/104, Loss: 0.0474\n",
      "    Batch 10/104, Loss: 0.0564\n",
      "    Batch 20/104, Loss: 0.0471\n",
      "    Batch 30/104, Loss: 0.0443\n",
      "    Batch 40/104, Loss: 0.0502\n",
      "    Batch 50/104, Loss: 0.0549\n",
      "    Batch 60/104, Loss: 0.0654\n",
      "    Batch 70/104, Loss: 0.0475\n",
      "    Batch 80/104, Loss: 0.0500\n",
      "    Batch 90/104, Loss: 0.0550\n",
      "    Batch 100/104, Loss: 0.0468\n",
      "\n",
      "  è¨“ç·´æå¤±: 0.0564\n",
      "  é©—è­‰æå¤±: 0.0762\n",
      "  é©—è­‰ Dice: 0.8657\n",
      "  å­¸ç¿’ç‡: 0.001293\n",
      "\n",
      "Epoch 24/30\n",
      "----------------------------------------\n",
      "    Batch 0/104, Loss: 0.0577\n",
      "    Batch 10/104, Loss: 0.0537\n",
      "    Batch 20/104, Loss: 0.0483\n",
      "    Batch 30/104, Loss: 0.0594\n",
      "    Batch 40/104, Loss: 0.0577\n",
      "    Batch 50/104, Loss: 0.0475\n",
      "    Batch 60/104, Loss: 0.0496\n",
      "    Batch 70/104, Loss: 0.0545\n",
      "    Batch 80/104, Loss: 0.0656\n",
      "    Batch 90/104, Loss: 0.0532\n",
      "    Batch 100/104, Loss: 0.0781\n",
      "\n",
      "  è¨“ç·´æå¤±: 0.0556\n",
      "  é©—è­‰æå¤±: 0.0772\n",
      "  é©—è­‰ Dice: 0.8640\n",
      "  å­¸ç¿’ç‡: 0.000964\n",
      "\n",
      "Epoch 25/30\n",
      "----------------------------------------\n",
      "    Batch 0/104, Loss: 0.0560\n",
      "    Batch 10/104, Loss: 0.0454\n",
      "    Batch 20/104, Loss: 0.0569\n",
      "    Batch 30/104, Loss: 0.0601\n",
      "    Batch 40/104, Loss: 0.0522\n",
      "    Batch 50/104, Loss: 0.0560\n",
      "    Batch 60/104, Loss: 0.0498\n",
      "    Batch 70/104, Loss: 0.0626\n",
      "    Batch 80/104, Loss: 0.0545\n",
      "    Batch 90/104, Loss: 0.0556\n",
      "    Batch 100/104, Loss: 0.0642\n",
      "\n",
      "  è¨“ç·´æå¤±: 0.0550\n",
      "  é©—è­‰æå¤±: 0.0771\n",
      "  é©—è­‰ Dice: 0.8646\n",
      "  å­¸ç¿’ç‡: 0.000679\n",
      "\n",
      "Epoch 26/30\n",
      "----------------------------------------\n",
      "    Batch 0/104, Loss: 0.0524\n",
      "    Batch 10/104, Loss: 0.0538\n",
      "    Batch 20/104, Loss: 0.0441\n",
      "    Batch 30/104, Loss: 0.0577\n",
      "    Batch 40/104, Loss: 0.0481\n",
      "    Batch 50/104, Loss: 0.0510\n",
      "    Batch 60/104, Loss: 0.0435\n",
      "    Batch 70/104, Loss: 0.0550\n",
      "    Batch 80/104, Loss: 0.0457\n",
      "    Batch 90/104, Loss: 0.0562\n",
      "    Batch 100/104, Loss: 0.0513\n",
      "\n",
      "  è¨“ç·´æå¤±: 0.0544\n",
      "  é©—è­‰æå¤±: 0.0776\n",
      "  é©—è­‰ Dice: 0.8643\n",
      "  å­¸ç¿’ç‡: 0.000442\n",
      "\n",
      "Epoch 27/30\n",
      "----------------------------------------\n",
      "    Batch 0/104, Loss: 0.0493\n",
      "    Batch 10/104, Loss: 0.0523\n",
      "    Batch 20/104, Loss: 0.0497\n",
      "    Batch 30/104, Loss: 0.0425\n",
      "    Batch 40/104, Loss: 0.0552\n",
      "    Batch 50/104, Loss: 0.0490\n",
      "    Batch 60/104, Loss: 0.0649\n",
      "    Batch 70/104, Loss: 0.0485\n",
      "    Batch 80/104, Loss: 0.0615\n",
      "    Batch 90/104, Loss: 0.0453\n",
      "    Batch 100/104, Loss: 0.0583\n",
      "\n",
      "  è¨“ç·´æå¤±: 0.0541\n",
      "  é©—è­‰æå¤±: 0.0772\n",
      "  é©—è­‰ Dice: 0.8645\n",
      "  å­¸ç¿’ç‡: 0.000254\n",
      "\n",
      "Epoch 28/30\n",
      "----------------------------------------\n",
      "    Batch 0/104, Loss: 0.0470\n",
      "    Batch 10/104, Loss: 0.0492\n",
      "    Batch 20/104, Loss: 0.0617\n",
      "    Batch 30/104, Loss: 0.0660\n",
      "    Batch 40/104, Loss: 0.0486\n",
      "    Batch 50/104, Loss: 0.0562\n",
      "    Batch 60/104, Loss: 0.0578\n",
      "    Batch 70/104, Loss: 0.0432\n",
      "    Batch 80/104, Loss: 0.0559\n",
      "    Batch 90/104, Loss: 0.0433\n",
      "    Batch 100/104, Loss: 0.0547\n",
      "\n",
      "  è¨“ç·´æå¤±: 0.0538\n",
      "  é©—è­‰æå¤±: 0.0771\n",
      "  é©—è­‰ Dice: 0.8642\n",
      "  å­¸ç¿’ç‡: 0.000119\n",
      "\n",
      "Epoch 29/30\n",
      "----------------------------------------\n",
      "    Batch 0/104, Loss: 0.0536\n",
      "    Batch 10/104, Loss: 0.0495\n",
      "    Batch 20/104, Loss: 0.0598\n",
      "    Batch 30/104, Loss: 0.0554\n",
      "    Batch 40/104, Loss: 0.0504\n",
      "    Batch 50/104, Loss: 0.0497\n",
      "    Batch 60/104, Loss: 0.0508\n",
      "    Batch 70/104, Loss: 0.0516\n",
      "    Batch 80/104, Loss: 0.0526\n",
      "    Batch 90/104, Loss: 0.0462\n",
      "    Batch 100/104, Loss: 0.0506\n",
      "\n",
      "  è¨“ç·´æå¤±: 0.0536\n",
      "  é©—è­‰æå¤±: 0.0773\n",
      "  é©—è­‰ Dice: 0.8645\n",
      "  å­¸ç¿’ç‡: 0.000037\n",
      "\n",
      "Epoch 30/30\n",
      "----------------------------------------\n",
      "    Batch 0/104, Loss: 0.0488\n",
      "    Batch 10/104, Loss: 0.0703\n",
      "    Batch 20/104, Loss: 0.0502\n",
      "    Batch 30/104, Loss: 0.0541\n",
      "    Batch 40/104, Loss: 0.0576\n",
      "    Batch 50/104, Loss: 0.0565\n",
      "    Batch 60/104, Loss: 0.0454\n",
      "    Batch 70/104, Loss: 0.0517\n",
      "    Batch 80/104, Loss: 0.0520\n",
      "    Batch 90/104, Loss: 0.0553\n",
      "    Batch 100/104, Loss: 0.0450\n",
      "\n",
      "  è¨“ç·´æå¤±: 0.0535\n",
      "  é©—è­‰æå¤±: 0.0774\n",
      "  é©—è­‰ Dice: 0.8644\n",
      "  å­¸ç¿’ç‡: 0.000010\n",
      "\n",
      "================================================================================\n",
      "âœ“ è¨“ç·´å®Œæˆ! æœ€ä½³ Dice: 0.8671\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "ğŸ§  é†«å­¸å½±åƒåˆ†å‰²è¨“ç·´ç³»çµ± - ç”Ÿç”¢ç´šç‰ˆæœ¬\n",
    "ç²¾æº–çŸ­è·¯ torch._compile/onnxã€trilinear æ’å€¼ã€å®Œå…¨å¯é‡ç¾ã€Cosine LR\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ğŸ§  é†«å­¸å½±åƒåˆ†å‰²è¨“ç·´ç³»çµ± - ç”Ÿç”¢ç´š\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ==================== ç²¾æº–çŸ­è·¯ï¼šé¿å…è¼‰å…¥ transformers ====================\n",
    "print(\"\\nğŸ”§ è¨­ç½®ç’°å¢ƒ...\")\n",
    "\n",
    "# å‰µå»º stub æ¨¡çµ„ä¾†çŸ­è·¯å°å…¥éˆ\n",
    "class _StubModule:\n",
    "    \"\"\"Stub æ¨¡çµ„ï¼Œé˜»æ­¢å¯¦éš›è¼‰å…¥ä½†ä¸ç ´å£å°å…¥éˆ\"\"\"\n",
    "    def __getattr__(self, name):\n",
    "        return _StubModule()\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        return _StubModule()\n",
    "\n",
    "# çŸ­è·¯ torch._compile å’Œ torch.onnxï¼Œé¿å…è§¸ç™¼ transformers\n",
    "sys.modules['torch._compile'] = _StubModule()\n",
    "sys.modules['torch.onnx'] = _StubModule()\n",
    "print(\"  âœ“ å·²çŸ­è·¯ torch._compile å’Œ torch.onnx\")\n",
    "\n",
    "# ==================== å°å…¥å¥—ä»¶ ====================\n",
    "\n",
    "import json\n",
    "import math\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "print(f\"\\nâœ“ PyTorch {torch.__version__}\")\n",
    "print(f\"âœ“ CUDA: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    print(\"âœ“ cuDNN benchmark enabled\")\n",
    "\n",
    "# ==================== å®Œå…¨å¯é‡ç¾è¨­ç½® ====================\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    \"\"\"è¨­ç½®æ‰€æœ‰éš¨æ©Ÿç¨®å­ä»¥ç¢ºä¿å¯é‡ç¾\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    # ç‚ºäº†å®Œå…¨å¯é‡ç¾ï¼ˆæœƒç¨å¾®é™ä½æ€§èƒ½ï¼‰\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# ==================== è³‡æ–™é›†ï¼ˆå„ªåŒ–æ’å€¼ï¼‰====================\n",
    "\n",
    "class HippocampusDataset(Dataset):\n",
    "    def __init__(self, data_root, indices, target_size=64):\n",
    "        self.data_root = Path(data_root)\n",
    "        self.target_size = target_size\n",
    "        \n",
    "        with open(self.data_root / 'dataset.json', 'r') as f:\n",
    "            metadata = json.load(f)\n",
    "        \n",
    "        training_data = metadata['training']\n",
    "        self.cases = [training_data[i] for i in indices]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.cases)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        case = self.cases[idx]\n",
    "        img_path = self.data_root / case['image'].lstrip('./')\n",
    "        lbl_path = self.data_root / case['label'].lstrip('./')\n",
    "        \n",
    "        # è¼‰å…¥å½±åƒ - float32\n",
    "        image = nib.load(str(img_path)).get_fdata(dtype=np.float32)\n",
    "        \n",
    "        # Z-score æ­£è¦åŒ–\n",
    "        p1, p99 = np.percentile(image, [1, 99])\n",
    "        image = np.clip(image, p1, p99)\n",
    "        mean, std = image.mean(), image.std()\n",
    "        if std > 1e-8:\n",
    "            image = (image - mean) / std\n",
    "        \n",
    "        # è¼‰å…¥æ¨™è¨» - float32\n",
    "        label = nib.load(str(lbl_path)).get_fdata(dtype=np.float32)\n",
    "        \n",
    "        # è½‰ç‚º torch tensorï¼ˆæ·»åŠ  batch å’Œ channel ç¶­åº¦ï¼‰\n",
    "        image_t = torch.from_numpy(image).unsqueeze(0).unsqueeze(0)  # [1, 1, D, H, W]\n",
    "        label_t = torch.from_numpy(label).unsqueeze(0).unsqueeze(0)  # [1, 1, D, H, W]\n",
    "        \n",
    "        # ä½¿ç”¨ PyTorch çš„ F.interpolate\n",
    "        # å½±åƒï¼štrilinearï¼ˆæ›´å¹³æ»‘ï¼‰\n",
    "        image_resized = F.interpolate(\n",
    "            image_t,\n",
    "            size=(self.target_size, self.target_size, self.target_size),\n",
    "            mode='trilinear',\n",
    "            align_corners=False\n",
    "        ).squeeze(0)  # [1, D, H, W]\n",
    "        \n",
    "        # æ¨™è¨»ï¼šnearestï¼ˆä¿æŒæ•´æ•¸å€¼ï¼‰\n",
    "        label_resized = F.interpolate(\n",
    "            label_t,\n",
    "            size=(self.target_size, self.target_size, self.target_size),\n",
    "            mode='nearest'\n",
    "        ).squeeze(0).squeeze(0)  # [D, H, W]\n",
    "        \n",
    "        # æ¸…æ´—æ¨™è¨»\n",
    "        label_resized = torch.clamp(label_resized.long(), 0, 2)\n",
    "        \n",
    "        return image_resized, label_resized\n",
    "\n",
    "def get_train_val_split(num_samples, train_ratio=0.8, seed=42):\n",
    "    \"\"\"å›ºå®šéš¨æ©Ÿç¨®å­çš„è³‡æ–™åˆ‡åˆ†\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.permutation(num_samples)\n",
    "    split_idx = int(num_samples * train_ratio)\n",
    "    return indices[:split_idx], indices[split_idx:]\n",
    "\n",
    "# ==================== æ¨¡å‹ ====================\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, num_groups=8):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv3d(in_ch, out_ch, 3, padding=1)\n",
    "        self.gn1 = nn.GroupNorm(min(num_groups, out_ch), out_ch)\n",
    "        self.conv2 = nn.Conv3d(out_ch, out_ch, 3, padding=1)\n",
    "        self.gn2 = nn.GroupNorm(min(num_groups, out_ch), out_ch)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.gn1(self.conv1(x)))\n",
    "        x = self.relu(self.gn2(self.conv2(x)))\n",
    "        return x\n",
    "\n",
    "class UNet3D(nn.Module):\n",
    "    def __init__(self, in_ch=1, num_classes=3, base=16):\n",
    "        super().__init__()\n",
    "        self.enc1 = ConvBlock(in_ch, base)\n",
    "        self.enc2 = ConvBlock(base, base * 2)\n",
    "        self.enc3 = ConvBlock(base * 2, base * 4)\n",
    "        self.bottleneck = ConvBlock(base * 4, base * 8)\n",
    "        \n",
    "        self.up3 = nn.ConvTranspose3d(base * 8, base * 4, 2, stride=2)\n",
    "        self.dec3 = ConvBlock(base * 8, base * 4)\n",
    "        self.up2 = nn.ConvTranspose3d(base * 4, base * 2, 2, stride=2)\n",
    "        self.dec2 = ConvBlock(base * 4, base * 2)\n",
    "        self.up1 = nn.ConvTranspose3d(base * 2, base, 2, stride=2)\n",
    "        self.dec1 = ConvBlock(base * 2, base)\n",
    "        \n",
    "        self.out = nn.Conv3d(base, num_classes, 1)\n",
    "        self.pool = nn.MaxPool3d(2, 2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        e1 = self.enc1(x)\n",
    "        e2 = self.enc2(self.pool(e1))\n",
    "        e3 = self.enc3(self.pool(e2))\n",
    "        b = self.bottleneck(self.pool(e3))\n",
    "        \n",
    "        d3 = self.dec3(torch.cat([self.up3(b), e3], 1))\n",
    "        d2 = self.dec2(torch.cat([self.up2(d3), e2], 1))\n",
    "        d1 = self.dec1(torch.cat([self.up1(d2), e1], 1))\n",
    "        \n",
    "        return self.out(d1)\n",
    "\n",
    "# ==================== æå¤±å‡½æ•¸ ====================\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=None, gamma=2.0):\n",
    "        super().__init__()\n",
    "        if alpha is not None:\n",
    "            self.register_buffer('alpha', alpha)\n",
    "        else:\n",
    "            self.alpha = None\n",
    "        self.gamma = gamma\n",
    "    \n",
    "    def forward(self, pred, target):\n",
    "        ce_loss = F.cross_entropy(pred, target, reduction='none')\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = ((1 - pt) ** self.gamma) * ce_loss\n",
    "        \n",
    "        if self.alpha is not None:\n",
    "            alpha_t = self.alpha[target]\n",
    "            focal_loss = alpha_t * focal_loss\n",
    "        \n",
    "        return focal_loss.mean()\n",
    "\n",
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, smooth=1.0):\n",
    "        super().__init__()\n",
    "        self.smooth = smooth\n",
    "    \n",
    "    def forward(self, pred, target):\n",
    "        pred = F.softmax(pred, dim=1)\n",
    "        target_one_hot = F.one_hot(target, pred.shape[1]).permute(0, 4, 1, 2, 3).float()\n",
    "        \n",
    "        inter = (pred * target_one_hot).sum(dim=(2, 3, 4))\n",
    "        union = pred.sum(dim=(2, 3, 4)) + target_one_hot.sum(dim=(2, 3, 4))\n",
    "        dice = (2. * inter + self.smooth) / (union + self.smooth)\n",
    "        \n",
    "        return 1 - dice.mean()\n",
    "\n",
    "class CombinedLoss(nn.Module):\n",
    "    def __init__(self, device, use_focal=True):\n",
    "        super().__init__()\n",
    "        if use_focal:\n",
    "            alpha = torch.tensor([0.2, 1.0, 1.0], device=device)\n",
    "            self.ce = FocalLoss(alpha=alpha, gamma=2.0)\n",
    "        else:\n",
    "            weight = torch.tensor([0.2, 1.0, 1.0], device=device)\n",
    "            self.ce = nn.CrossEntropyLoss(weight=weight)\n",
    "        \n",
    "        self.dice = DiceLoss()\n",
    "    \n",
    "    def forward(self, pred, target):\n",
    "        return 0.3 * self.ce(pred, target) + 0.7 * self.dice(pred, target)\n",
    "\n",
    "# ==================== Cosine Annealing å­¸ç¿’ç‡ ====================\n",
    "\n",
    "class CosineAnnealingLR:\n",
    "    \"\"\"æ‰‹å‹•å¯¦ç¾ Cosine Annealing LR Scheduler\"\"\"\n",
    "    def __init__(self, optimizer, T_max, eta_min=0):\n",
    "        self.optimizer = optimizer\n",
    "        self.T_max = T_max\n",
    "        self.eta_min = eta_min\n",
    "        self.base_lr = optimizer.param_groups[0]['lr']\n",
    "        self.current_epoch = 0\n",
    "    \n",
    "    def step(self):\n",
    "        \"\"\"æ›´æ–°å­¸ç¿’ç‡\"\"\"\n",
    "        self.current_epoch += 1\n",
    "        lr = self.eta_min + (self.base_lr - self.eta_min) * \\\n",
    "             (1 + math.cos(math.pi * self.current_epoch / self.T_max)) / 2\n",
    "        \n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "    \n",
    "    def get_lr(self):\n",
    "        \"\"\"ç²å–ç•¶å‰å­¸ç¿’ç‡\"\"\"\n",
    "        return self.optimizer.param_groups[0]['lr']\n",
    "\n",
    "# ==================== è¨“ç·´å™¨ ====================\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model, train_loader, val_loader, device, output_dir, num_epochs):\n",
    "        self.model = model.to(device)\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.device = device\n",
    "        self.output_dir = Path(output_dir)\n",
    "        self.output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # ä½¿ç”¨ SGD with momentum\n",
    "        self.optimizer = torch.optim.SGD(\n",
    "            model.parameters(),\n",
    "            lr=1e-2,\n",
    "            momentum=0.9,\n",
    "            weight_decay=1e-4,\n",
    "            nesterov=True\n",
    "        )\n",
    "        \n",
    "        # Cosine Annealing LR\n",
    "        self.scheduler = CosineAnnealingLR(\n",
    "            self.optimizer,\n",
    "            T_max=num_epochs,\n",
    "            eta_min=1e-5\n",
    "        )\n",
    "        \n",
    "        self.criterion = CombinedLoss(device=device, use_focal=True)\n",
    "        self.history = {'train_loss': [], 'val_loss': [], 'val_dice': [], 'lr': []}\n",
    "        self.best_dice = 0.0\n",
    "        \n",
    "        print(f\"  ä½¿ç”¨ SGD + Nesterov momentum\")\n",
    "        print(f\"  ä½¿ç”¨ Cosine Annealing LR (T_max={num_epochs})\")\n",
    "    \n",
    "    def train_epoch(self):\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        for batch_idx, (images, labels) in enumerate(self.train_loader):\n",
    "            images, labels = images.to(self.device), labels.to(self.device)\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            outputs = self.model(images)\n",
    "            loss = self.criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            if batch_idx % 10 == 0:\n",
    "                print(f\"    Batch {batch_idx}/{len(self.train_loader)}, Loss: {loss.item():.4f}\")\n",
    "        \n",
    "        return total_loss / len(self.train_loader)\n",
    "    \n",
    "    def validate(self):\n",
    "        \"\"\"ç´¯ç©å¼ Dice è¨ˆç®—\"\"\"\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        total_inter = {1: 0.0, 2: 0.0}\n",
    "        total_union = {1: 0.0, 2: 0.0}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels in self.val_loader:\n",
    "                images, labels = images.to(self.device), labels.to(self.device)\n",
    "                outputs = self.model(images)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                preds = torch.argmax(outputs, dim=1)\n",
    "                \n",
    "                for cls in [1, 2]:\n",
    "                    pred_mask = (preds == cls).float()\n",
    "                    target_mask = (labels == cls).float()\n",
    "                    total_inter[cls] += (pred_mask * target_mask).sum().item()\n",
    "                    total_union[cls] += (pred_mask.sum() + target_mask.sum()).item()\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "        \n",
    "        dice_scores = []\n",
    "        for cls in [1, 2]:\n",
    "            dice = (2.0 * total_inter[cls]) / (total_union[cls] + 1e-8)\n",
    "            dice_scores.append(dice)\n",
    "        \n",
    "        return total_loss / len(self.val_loader), np.mean(dice_scores)\n",
    "    \n",
    "    def train(self, num_epochs):\n",
    "        print(f\"\\né–‹å§‹è¨“ç·´ {num_epochs} epochs\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        for epoch in range(1, num_epochs + 1):\n",
    "            print(f\"\\nEpoch {epoch}/{num_epochs}\")\n",
    "            print(\"-\" * 40)\n",
    "            \n",
    "            train_loss = self.train_epoch()\n",
    "            val_loss, val_dice = self.validate()\n",
    "            \n",
    "            # æ›´æ–°å­¸ç¿’ç‡\n",
    "            self.scheduler.step()\n",
    "            current_lr = self.scheduler.get_lr()\n",
    "            \n",
    "            self.history['train_loss'].append(train_loss)\n",
    "            self.history['val_loss'].append(val_loss)\n",
    "            self.history['val_dice'].append(val_dice)\n",
    "            self.history['lr'].append(current_lr)\n",
    "            \n",
    "            print(f\"\\n  è¨“ç·´æå¤±: {train_loss:.4f}\")\n",
    "            print(f\"  é©—è­‰æå¤±: {val_loss:.4f}\")\n",
    "            print(f\"  é©—è­‰ Dice: {val_dice:.4f}\")\n",
    "            print(f\"  å­¸ç¿’ç‡: {current_lr:.6f}\")\n",
    "            \n",
    "            if val_dice > self.best_dice:\n",
    "                self.best_dice = val_dice\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model': self.model.state_dict(),\n",
    "                    'optimizer': self.optimizer.state_dict(),\n",
    "                    'dice': val_dice,\n",
    "                    'history': self.history\n",
    "                }, self.output_dir / 'best_model.pth')\n",
    "                print(f\"  âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹ (Dice: {val_dice:.4f})\")\n",
    "        \n",
    "        with open(self.output_dir / 'history.json', 'w') as f:\n",
    "            json.dump(self.history, f, indent=2)\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(f\"âœ“ è¨“ç·´å®Œæˆ! æœ€ä½³ Dice: {self.best_dice:.4f}\")\n",
    "\n",
    "# ==================== ä¸»ç¨‹åº ====================\n",
    "\n",
    "def main():\n",
    "    # è¨­ç½®éš¨æ©Ÿç¨®å­ï¼ˆç¢ºä¿å¯é‡ç¾ï¼‰\n",
    "    SEED = 42\n",
    "    set_seed(SEED)\n",
    "    print(f\"\\nğŸ² è¨­ç½®éš¨æ©Ÿç¨®å­: {SEED}\")\n",
    "    \n",
    "    DATA_ROOT = '/workspace/Task04_Hippocampus'\n",
    "    OUTPUT_DIR = '/workspace/outputs/training_production'\n",
    "    BATCH_SIZE = 2\n",
    "    NUM_EPOCHS = 30  # SGD éœ€è¦æ›´å¤š epochs æ‰èƒ½é”åˆ°å¥½çš„æ”¶æ–‚\n",
    "    TARGET_SIZE = 64\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"ä½¿ç”¨è¨­å‚™: {device}\")\n",
    "    \n",
    "    print(\"\\næº–å‚™è³‡æ–™...\")\n",
    "    with open(Path(DATA_ROOT) / 'dataset.json') as f:\n",
    "        num_samples = len(json.load(f)['training'])\n",
    "    \n",
    "    train_indices, val_indices = get_train_val_split(num_samples, train_ratio=0.8, seed=SEED)\n",
    "    print(f\"  è¨“ç·´é›†: {len(train_indices)} å€‹æ¡ˆä¾‹\")\n",
    "    print(f\"  é©—è­‰é›†: {len(val_indices)} å€‹æ¡ˆä¾‹\")\n",
    "    \n",
    "    train_dataset = HippocampusDataset(DATA_ROOT, train_indices, TARGET_SIZE)\n",
    "    val_dataset = HippocampusDataset(DATA_ROOT, val_indices, TARGET_SIZE)\n",
    "    \n",
    "    # DataLoader é…ç½®ï¼ˆå¯é¸å„ªåŒ–ï¼‰\n",
    "    use_multiprocessing = torch.cuda.is_available()\n",
    "    num_workers = min(4, os.cpu_count() or 1) if use_multiprocessing else 0\n",
    "    \n",
    "    train_loader_kwargs = {\n",
    "        'batch_size': BATCH_SIZE,\n",
    "        'shuffle': True,\n",
    "        'num_workers': num_workers,\n",
    "    }\n",
    "    if num_workers > 0:\n",
    "        train_loader_kwargs.update({\n",
    "            'pin_memory': True,\n",
    "            'persistent_workers': True,\n",
    "            'prefetch_factor': 2\n",
    "        })\n",
    "    \n",
    "    val_loader_kwargs = {\n",
    "        'batch_size': BATCH_SIZE,\n",
    "        'shuffle': False,\n",
    "        'num_workers': num_workers,\n",
    "    }\n",
    "    if num_workers > 0:\n",
    "        val_loader_kwargs['pin_memory'] = True\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, **train_loader_kwargs)\n",
    "    val_loader = DataLoader(val_dataset, **val_loader_kwargs)\n",
    "    \n",
    "    print(f\"  DataLoader workers: {num_workers}\")\n",
    "    print(f\"  å½±åƒæ’å€¼: trilinear\")\n",
    "    print(f\"  æ¨™è¨»æ’å€¼: nearest\")\n",
    "    \n",
    "    print(\"\\nå»ºç«‹æ¨¡å‹...\")\n",
    "    model = UNet3D(1, 3, 16)\n",
    "    print(f\"  åƒæ•¸: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    print(f\"  ä½¿ç”¨ GroupNorm (ç©©å®šæ–¼å° batch)\")\n",
    "    \n",
    "    trainer = Trainer(model, train_loader, val_loader, device, OUTPUT_DIR, NUM_EPOCHS)\n",
    "    trainer.train(NUM_EPOCHS)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        main()\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nè¨“ç·´ä¸­æ–·\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\néŒ¯èª¤: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "010e274c-dc8b-409d-baad-e9318d262059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "æå‡ Dice çš„æ”¹é€²ç‰ˆè¨“ç·´ç³»çµ±\n",
      "================================================================================\n",
      "\n",
      "PyTorch 2.1.0+cu118\n",
      "CUDA: True\n",
      "\n",
      "è¨­ç½®ç’°å¢ƒå®Œæˆ\n",
      "  TF32 enabled\n",
      "éš¨æ©Ÿç¨®å­: 42\n",
      "ä½¿ç”¨è¨­å‚™: cuda\n",
      "\n",
      "æº–å‚™è³‡æ–™...\n",
      "  è¨“ç·´é›†: 208\n",
      "  é©—è­‰é›†: 52\n",
      "\n",
      "å»ºç«‹æ¨¡å‹...\n",
      "  åƒæ•¸: 3,152,913\n",
      "  AdamW (lr=0.0005) + Warmup(5) + Cosine\n",
      "  æ›´å¤§æ¨¡å‹ (base=24)\n",
      "  æ›´å¼·å¢å¼· (noise + blur)\n",
      "  æ›´é•·è¨“ç·´ (50 epochs)\n",
      "  Dice æ¬Šé‡: 0.8\n",
      "\n",
      "é–‹å§‹è¨“ç·´ 50 epochs\n",
      "================================================================================\n",
      "\n",
      "Epoch 1/50\n",
      "----------------------------------------\n",
      "    Batch 0/104, Loss: 1.7866\n",
      "    Batch 10/104, Loss: 1.6767\n",
      "    Batch 20/104, Loss: 1.5896\n",
      "    Batch 30/104, Loss: 1.4519\n",
      "    Batch 40/104, Loss: 1.2524\n",
      "    Batch 50/104, Loss: 1.1830\n",
      "    Batch 60/104, Loss: 1.1920\n",
      "    Batch 70/104, Loss: 1.1376\n",
      "    Batch 80/104, Loss: 1.1990\n",
      "    Batch 90/104, Loss: 1.1095\n",
      "    Batch 100/104, Loss: 1.1447\n",
      "\n",
      "  è¨“ç·´æå¤±: 1.3372\n",
      "  é©—è­‰æå¤±: 0.7977\n",
      "  é©—è­‰ Dice: 0.0584 ğŸ¯ æ–°ç´€éŒ„!\n",
      "    Class 1 - Dice: 0.0770, IoU: 0.0400\n",
      "    Class 2 - Dice: 0.0399, IoU: 0.0203\n",
      "  å­¸ç¿’ç‡: 0.000100\n",
      "  âœ“ å·²ä¿å­˜æœ€ä½³æ¨¡å‹\n",
      "\n",
      "Epoch 2/50\n",
      "----------------------------------------\n",
      "    Batch 0/104, Loss: 1.0202\n",
      "    Batch 10/104, Loss: 0.9235\n",
      "    Batch 20/104, Loss: 0.8873\n",
      "    Batch 30/104, Loss: 0.9362\n",
      "    Batch 40/104, Loss: 0.8607\n",
      "    Batch 50/104, Loss: 0.8954\n",
      "    Batch 60/104, Loss: 0.8031\n",
      "    Batch 70/104, Loss: 0.7979\n",
      "    Batch 80/104, Loss: 0.7960\n",
      "    Batch 90/104, Loss: 0.8016\n",
      "    Batch 100/104, Loss: 0.8371\n",
      "\n",
      "  è¨“ç·´æå¤±: 0.8759\n",
      "  é©—è­‰æå¤±: 0.7962\n",
      "  é©—è­‰ Dice: 0.0587 ğŸ¯ æ–°ç´€éŒ„!\n",
      "    Class 1 - Dice: 0.0793, IoU: 0.0413\n",
      "    Class 2 - Dice: 0.0382, IoU: 0.0194\n",
      "  å­¸ç¿’ç‡: 0.000200\n",
      "  âœ“ å·²ä¿å­˜æœ€ä½³æ¨¡å‹\n",
      "\n",
      "Epoch 3/50\n",
      "----------------------------------------\n",
      "    Batch 0/104, Loss: 0.7485\n",
      "    Batch 10/104, Loss: 0.7573\n",
      "    Batch 20/104, Loss: 0.8227\n",
      "    Batch 30/104, Loss: 0.7090\n",
      "    Batch 40/104, Loss: 0.7257\n",
      "    Batch 50/104, Loss: 0.8371\n",
      "    Batch 60/104, Loss: 0.7577\n",
      "    Batch 70/104, Loss: 0.7153\n",
      "    Batch 80/104, Loss: 0.7443\n",
      "    Batch 90/104, Loss: 0.7384\n",
      "    Batch 100/104, Loss: 0.6273\n",
      "\n",
      "  è¨“ç·´æå¤±: 0.7608\n",
      "  é©—è­‰æå¤±: 0.7946\n",
      "  é©—è­‰ Dice: 0.0590 ğŸ¯ æ–°ç´€éŒ„!\n",
      "    Class 1 - Dice: 0.0817, IoU: 0.0426\n",
      "    Class 2 - Dice: 0.0364, IoU: 0.0185\n",
      "  å­¸ç¿’ç‡: 0.000300\n",
      "  âœ“ å·²ä¿å­˜æœ€ä½³æ¨¡å‹\n",
      "\n",
      "Epoch 4/50\n",
      "----------------------------------------\n",
      "    Batch 0/104, Loss: 0.6731\n",
      "    Batch 10/104, Loss: 0.6977\n",
      "    Batch 20/104, Loss: 0.7056\n",
      "    Batch 30/104, Loss: 0.7820\n",
      "    Batch 40/104, Loss: 0.6758\n",
      "    Batch 50/104, Loss: 0.8133\n",
      "    Batch 60/104, Loss: 0.6095\n",
      "    Batch 70/104, Loss: 0.5929\n",
      "    Batch 80/104, Loss: 0.6542\n",
      "    Batch 90/104, Loss: 0.5933\n",
      "    Batch 100/104, Loss: 0.6879\n",
      "\n",
      "  è¨“ç·´æå¤±: 0.6546\n",
      "  é©—è­‰æå¤±: 0.7930\n",
      "  é©—è­‰ Dice: 0.0592 ğŸ¯ æ–°ç´€éŒ„!\n",
      "    Class 1 - Dice: 0.0838, IoU: 0.0437\n",
      "    Class 2 - Dice: 0.0347, IoU: 0.0177\n",
      "  å­¸ç¿’ç‡: 0.000400\n",
      "  âœ“ å·²ä¿å­˜æœ€ä½³æ¨¡å‹\n",
      "\n",
      "Epoch 5/50\n",
      "----------------------------------------\n",
      "    Batch 0/104, Loss: 0.5565\n",
      "    Batch 10/104, Loss: 0.6084\n",
      "    Batch 20/104, Loss: 0.6174\n",
      "    Batch 30/104, Loss: 0.5921\n",
      "    Batch 40/104, Loss: 0.5743\n",
      "    Batch 50/104, Loss: 0.5208\n",
      "    Batch 60/104, Loss: 0.5356\n",
      "    Batch 70/104, Loss: 0.5128\n",
      "    Batch 80/104, Loss: 0.5019\n",
      "    Batch 90/104, Loss: 0.5194\n",
      "    Batch 100/104, Loss: 0.4410\n",
      "\n",
      "  è¨“ç·´æå¤±: 0.5600\n",
      "  é©—è­‰æå¤±: 0.7914\n",
      "  é©—è­‰ Dice: 0.0599 ğŸ¯ æ–°ç´€éŒ„!\n",
      "    Class 1 - Dice: 0.0865, IoU: 0.0452\n",
      "    Class 2 - Dice: 0.0333, IoU: 0.0169\n",
      "  å­¸ç¿’ç‡: 0.000500\n",
      "  âœ“ å·²ä¿å­˜æœ€ä½³æ¨¡å‹\n",
      "\n",
      "Epoch 6/50\n",
      "----------------------------------------\n",
      "    Batch 0/104, Loss: 0.5224\n",
      "    Batch 10/104, Loss: 0.4697\n",
      "    Batch 20/104, Loss: 0.6036\n",
      "    Batch 30/104, Loss: 0.4547\n",
      "    Batch 40/104, Loss: 0.4635\n",
      "    Batch 50/104, Loss: 0.4807\n",
      "    Batch 60/104, Loss: 0.5257\n",
      "    Batch 70/104, Loss: 0.4363\n",
      "    Batch 80/104, Loss: 0.4716\n",
      "    Batch 90/104, Loss: 0.4329\n",
      "    Batch 100/104, Loss: 0.5041\n",
      "\n",
      "  è¨“ç·´æå¤±: 0.4848\n",
      "  é©—è­‰æå¤±: 0.7896\n",
      "  é©—è­‰ Dice: 0.0611 ğŸ¯ æ–°ç´€éŒ„!\n",
      "    Class 1 - Dice: 0.0899, IoU: 0.0471\n",
      "    Class 2 - Dice: 0.0324, IoU: 0.0165\n",
      "  å­¸ç¿’ç‡: 0.000499\n",
      "  âœ“ å·²ä¿å­˜æœ€ä½³æ¨¡å‹\n",
      "\n",
      "Epoch 7/50\n",
      "----------------------------------------\n",
      "    Batch 0/104, Loss: 0.4934\n",
      "    Batch 10/104, Loss: 0.4246\n",
      "    Batch 20/104, Loss: 0.4884\n",
      "    Batch 30/104, Loss: 0.3733\n",
      "    Batch 40/104, Loss: 0.3949\n",
      "    Batch 50/104, Loss: 0.4314\n",
      "    Batch 60/104, Loss: 0.4212\n",
      "    Batch 70/104, Loss: 0.5153\n",
      "    Batch 80/104, Loss: 0.4134\n",
      "    Batch 90/104, Loss: 0.4264\n",
      "    Batch 100/104, Loss: 0.4164\n",
      "\n",
      "  è¨“ç·´æå¤±: 0.4498\n",
      "  é©—è­‰æå¤±: 0.7877\n",
      "  é©—è­‰ Dice: 0.0632 ğŸ¯ æ–°ç´€éŒ„!\n",
      "    Class 1 - Dice: 0.0937, IoU: 0.0491\n",
      "    Class 2 - Dice: 0.0327, IoU: 0.0166\n",
      "  å­¸ç¿’ç‡: 0.000498\n",
      "  âœ“ å·²ä¿å­˜æœ€ä½³æ¨¡å‹\n",
      "\n",
      "Epoch 8/50\n",
      "----------------------------------------\n",
      "    Batch 0/104, Loss: 0.4077\n",
      "    Batch 10/104, Loss: 0.3951\n",
      "    Batch 20/104, Loss: 0.4076\n",
      "    Batch 30/104, Loss: 0.4007\n",
      "    Batch 40/104, Loss: 0.4371\n",
      "    Batch 50/104, Loss: 0.4323\n",
      "    Batch 60/104, Loss: 0.3640\n",
      "    Batch 70/104, Loss: 0.4325\n",
      "    Batch 80/104, Loss: 0.3258\n",
      "    Batch 90/104, Loss: 0.6407\n",
      "    Batch 100/104, Loss: 0.4187\n",
      "\n",
      "  è¨“ç·´æå¤±: 0.4242\n",
      "  é©—è­‰æå¤±: 0.7858\n",
      "  é©—è­‰ Dice: 0.0659 ğŸ¯ æ–°ç´€éŒ„!\n",
      "    Class 1 - Dice: 0.0974, IoU: 0.0512\n",
      "    Class 2 - Dice: 0.0345, IoU: 0.0175\n",
      "  å­¸ç¿’ç‡: 0.000495\n",
      "  âœ“ å·²ä¿å­˜æœ€ä½³æ¨¡å‹\n",
      "\n",
      "Epoch 9/50\n",
      "----------------------------------------\n",
      "    Batch 0/104, Loss: 0.4313\n",
      "    Batch 10/104, Loss: 0.4341\n",
      "    Batch 20/104, Loss: 0.3675\n",
      "    Batch 30/104, Loss: 0.4182\n",
      "    Batch 40/104, Loss: 0.5088\n",
      "    Batch 50/104, Loss: 0.3924\n",
      "    Batch 60/104, Loss: 0.3765\n",
      "    Batch 70/104, Loss: 0.3548\n",
      "    Batch 80/104, Loss: 0.3686\n",
      "    Batch 90/104, Loss: 0.4675\n",
      "    Batch 100/104, Loss: 0.3769\n",
      "\n",
      "  è¨“ç·´æå¤±: 0.3982\n",
      "  é©—è­‰æå¤±: 0.7838\n",
      "  é©—è­‰ Dice: 0.0692 ğŸ¯ æ–°ç´€éŒ„!\n",
      "    Class 1 - Dice: 0.1008, IoU: 0.0531\n",
      "    Class 2 - Dice: 0.0376, IoU: 0.0191\n",
      "  å­¸ç¿’ç‡: 0.000490\n",
      "  âœ“ å·²ä¿å­˜æœ€ä½³æ¨¡å‹\n",
      "\n",
      "Epoch 10/50\n",
      "----------------------------------------\n",
      "    Batch 0/104, Loss: 0.3272\n",
      "    Batch 10/104, Loss: 0.3729\n",
      "    Batch 20/104, Loss: 0.3418\n",
      "    Batch 30/104, Loss: 0.3138\n",
      "    Batch 40/104, Loss: 0.3136\n",
      "    Batch 50/104, Loss: 0.4421\n",
      "    Batch 60/104, Loss: 0.3102\n",
      "    Batch 70/104, Loss: 0.4670\n",
      "    Batch 80/104, Loss: 0.4468\n",
      "    Batch 90/104, Loss: 0.3234\n",
      "    Batch 100/104, Loss: 0.3220\n",
      "\n",
      "  è¨“ç·´æå¤±: 0.3828\n",
      "  é©—è­‰æå¤±: 0.7817\n",
      "  é©—è­‰ Dice: 0.0719 ğŸ¯ æ–°ç´€éŒ„!\n",
      "    Class 1 - Dice: 0.1040, IoU: 0.0548\n",
      "    Class 2 - Dice: 0.0398, IoU: 0.0203\n",
      "  å­¸ç¿’ç‡: 0.000485\n",
      "  âœ“ å·²ä¿å­˜æœ€ä½³æ¨¡å‹\n",
      "\n",
      "Epoch 11/50\n",
      "----------------------------------------\n",
      "    Batch 0/104, Loss: 0.2544\n",
      "    Batch 10/104, Loss: 0.3094\n",
      "    Batch 20/104, Loss: 0.2413\n",
      "    Batch 30/104, Loss: 0.2683\n",
      "    Batch 40/104, Loss: 0.2381\n",
      "    Batch 50/104, Loss: 0.2988\n",
      "    Batch 60/104, Loss: 0.3805\n",
      "    Batch 70/104, Loss: 0.2196\n",
      "    Batch 80/104, Loss: 0.2488\n",
      "    Batch 90/104, Loss: 0.2211\n",
      "    Batch 100/104, Loss: 0.2547\n",
      "\n",
      "  è¨“ç·´æå¤±: 0.2797\n",
      "  é©—è­‰æå¤±: 0.7797\n",
      "  é©—è­‰ Dice: 0.0729 ğŸ¯ æ–°ç´€éŒ„!\n",
      "    Class 1 - Dice: 0.1067, IoU: 0.0563\n",
      "    Class 2 - Dice: 0.0391, IoU: 0.0200\n",
      "  å­¸ç¿’ç‡: 0.000478\n",
      "  âœ“ å·²ä¿å­˜æœ€ä½³æ¨¡å‹\n",
      "\n",
      "Epoch 12/50\n",
      "----------------------------------------\n",
      "    Batch 0/104, Loss: 0.2914\n",
      "    Batch 10/104, Loss: 0.2132\n",
      "    Batch 20/104, Loss: 0.2481\n",
      "    Batch 30/104, Loss: 0.2870\n",
      "    Batch 40/104, Loss: 0.3225\n",
      "    Batch 50/104, Loss: 0.3364\n",
      "    Batch 60/104, Loss: 0.2674\n",
      "    Batch 70/104, Loss: 0.2727\n",
      "    Batch 80/104, Loss: 0.2760\n",
      "    Batch 90/104, Loss: 0.2716\n",
      "    Batch 100/104, Loss: 0.2005\n",
      "\n",
      "  è¨“ç·´æå¤±: 0.2766\n",
      "  é©—è­‰æå¤±: 0.7777\n",
      "  é©—è­‰ Dice: 0.0724 \n",
      "    Class 1 - Dice: 0.1088, IoU: 0.0575\n",
      "    Class 2 - Dice: 0.0361, IoU: 0.0184\n",
      "  å­¸ç¿’ç‡: 0.000471\n",
      "\n",
      "Epoch 13/50\n",
      "----------------------------------------\n",
      "    Batch 0/104, Loss: 0.2295\n",
      "    Batch 10/104, Loss: 0.2509\n",
      "    Batch 20/104, Loss: 0.2019\n",
      "    Batch 30/104, Loss: 0.2945\n",
      "    Batch 40/104, Loss: 0.2981\n",
      "    Batch 50/104, Loss: 0.2761\n",
      "    Batch 60/104, Loss: 0.2390\n",
      "    Batch 70/104, Loss: 0.2346\n",
      "    Batch 80/104, Loss: 0.2629\n",
      "    Batch 90/104, Loss: 0.2649\n",
      "    Batch 100/104, Loss: 0.2290\n",
      "\n",
      "  è¨“ç·´æå¤±: 0.2669\n",
      "  é©—è­‰æå¤±: 0.7756\n",
      "  é©—è­‰ Dice: 0.0711 \n",
      "    Class 1 - Dice: 0.1105, IoU: 0.0585\n",
      "    Class 2 - Dice: 0.0317, IoU: 0.0161\n",
      "  å­¸ç¿’ç‡: 0.000462\n",
      "\n",
      "Epoch 14/50\n",
      "----------------------------------------\n",
      "    Batch 0/104, Loss: 0.3164\n",
      "    Batch 10/104, Loss: 0.2573\n",
      "    Batch 20/104, Loss: 0.2233\n",
      "    Batch 30/104, Loss: 0.2932\n",
      "    Batch 40/104, Loss: 0.2898\n",
      "    Batch 50/104, Loss: 0.2213\n",
      "    Batch 60/104, Loss: 0.3180\n",
      "    Batch 70/104, Loss: 0.3422\n",
      "    Batch 80/104, Loss: 0.2778\n",
      "    Batch 90/104, Loss: 0.2618\n",
      "    Batch 100/104, Loss: 0.3831\n",
      "\n",
      "  è¨“ç·´æå¤±: 0.2647\n",
      "  é©—è­‰æå¤±: 0.7735\n",
      "  é©—è­‰ Dice: 0.0696 \n",
      "    Class 1 - Dice: 0.1123, IoU: 0.0595\n",
      "    Class 2 - Dice: 0.0269, IoU: 0.0136\n",
      "  å­¸ç¿’ç‡: 0.000452\n",
      "\n",
      "Epoch 15/50\n",
      "----------------------------------------\n",
      "    Batch 0/104, Loss: 0.2396\n",
      "    Batch 10/104, Loss: 0.2187\n",
      "    Batch 20/104, Loss: 0.3318\n",
      "    Batch 30/104, Loss: 0.2304\n",
      "    Batch 40/104, Loss: 0.2531\n",
      "    Batch 50/104, Loss: 0.2132\n",
      "    Batch 60/104, Loss: 0.2599\n",
      "    Batch 70/104, Loss: 0.2362\n",
      "    Batch 80/104, Loss: 0.2545\n",
      "    Batch 90/104, Loss: 0.2125\n",
      "    Batch 100/104, Loss: 0.2325\n",
      "\n",
      "  è¨“ç·´æå¤±: 0.2610\n",
      "  é©—è­‰æå¤±: 0.7713\n",
      "  é©—è­‰ Dice: 0.0685 \n",
      "    Class 1 - Dice: 0.1143, IoU: 0.0606\n",
      "    Class 2 - Dice: 0.0227, IoU: 0.0115\n",
      "  å­¸ç¿’ç‡: 0.000442\n",
      "\n",
      "Epoch 16/50\n",
      "----------------------------------------\n",
      "    Batch 0/104, Loss: 0.2242\n",
      "    Batch 10/104, Loss: 0.2549\n",
      "    Batch 20/104, Loss: 0.2785\n",
      "    Batch 30/104, Loss: 0.2259\n",
      "    Batch 40/104, Loss: 0.2290\n",
      "    Batch 50/104, Loss: 0.2439\n",
      "    Batch 60/104, Loss: 0.2010\n",
      "    Batch 70/104, Loss: 0.2541\n",
      "    Batch 80/104, Loss: 0.2043\n",
      "    Batch 90/104, Loss: 0.2293\n",
      "    Batch 100/104, Loss: 0.2581\n",
      "\n",
      "  è¨“ç·´æå¤±: 0.2546\n",
      "  é©—è­‰æå¤±: 0.7690\n",
      "  é©—è­‰ Dice: 0.0681 \n",
      "    Class 1 - Dice: 0.1168, IoU: 0.0620\n",
      "    Class 2 - Dice: 0.0194, IoU: 0.0098\n",
      "  å­¸ç¿’ç‡: 0.000430\n",
      "\n",
      "Epoch 17/50\n",
      "----------------------------------------\n",
      "    Batch 0/104, Loss: 0.2394\n",
      "    Batch 10/104, Loss: 0.2775\n",
      "    Batch 20/104, Loss: 0.2287\n",
      "    Batch 30/104, Loss: 0.2712\n",
      "    Batch 40/104, Loss: 0.2531\n",
      "    Batch 50/104, Loss: 0.3008\n",
      "    Batch 60/104, Loss: 0.2857\n",
      "    Batch 70/104, Loss: 0.2356\n",
      "    Batch 80/104, Loss: 0.2743\n",
      "    Batch 90/104, Loss: 0.2902\n",
      "    Batch 100/104, Loss: 0.2245\n",
      "\n",
      "  è¨“ç·´æå¤±: 0.2538\n",
      "  é©—è­‰æå¤±: 0.7665\n",
      "  é©—è­‰ Dice: 0.0689 \n",
      "    Class 1 - Dice: 0.1201, IoU: 0.0639\n",
      "    Class 2 - Dice: 0.0177, IoU: 0.0089\n",
      "  å­¸ç¿’ç‡: 0.000417\n",
      "\n",
      "Epoch 18/50\n",
      "----------------------------------------\n",
      "    Batch 0/104, Loss: 0.2322\n",
      "    Batch 10/104, Loss: 0.3227\n",
      "    Batch 20/104, Loss: 0.2175\n",
      "    Batch 30/104, Loss: 0.2676\n",
      "    Batch 40/104, Loss: 0.2006\n",
      "    Batch 50/104, Loss: 0.2423\n",
      "    Batch 60/104, Loss: 0.2598\n",
      "    Batch 70/104, Loss: 0.2395\n",
      "    Batch 80/104, Loss: 0.2704\n",
      "    Batch 90/104, Loss: 0.2796\n",
      "    Batch 100/104, Loss: 0.2378\n",
      "\n",
      "  è¨“ç·´æå¤±: 0.2480\n",
      "  é©—è­‰æå¤±: 0.7637\n",
      "  é©—è­‰ Dice: 0.0713 \n",
      "    Class 1 - Dice: 0.1245, IoU: 0.0664\n",
      "    Class 2 - Dice: 0.0181, IoU: 0.0091\n",
      "  å­¸ç¿’ç‡: 0.000404\n",
      "\n",
      "Epoch 19/50\n",
      "----------------------------------------\n",
      "    Batch 0/104, Loss: 0.2901\n",
      "    Batch 10/104, Loss: 0.2343\n",
      "    Batch 20/104, Loss: 0.2328\n",
      "    Batch 30/104, Loss: 0.2845\n",
      "    Batch 40/104, Loss: 0.3247\n",
      "    Batch 50/104, Loss: 0.2026\n",
      "    Batch 60/104, Loss: 0.2370\n",
      "    Batch 70/104, Loss: 0.2112\n",
      "    Batch 80/104, Loss: 0.1974\n",
      "    Batch 90/104, Loss: 0.2405\n",
      "    Batch 100/104, Loss: 0.2947\n",
      "\n",
      "  è¨“ç·´æå¤±: 0.2453\n",
      "  é©—è­‰æå¤±: 0.7603\n",
      "  é©—è­‰ Dice: 0.0752 ğŸ¯ æ–°ç´€éŒ„!\n",
      "    Class 1 - Dice: 0.1303, IoU: 0.0697\n",
      "    Class 2 - Dice: 0.0201, IoU: 0.0102\n",
      "  å­¸ç¿’ç‡: 0.000390\n",
      "  âœ“ å·²ä¿å­˜æœ€ä½³æ¨¡å‹\n",
      "\n",
      "Epoch 20/50\n",
      "----------------------------------------\n",
      "    Batch 0/104, Loss: 0.2877\n",
      "    Batch 10/104, Loss: 0.2862\n",
      "    Batch 20/104, Loss: 0.3119\n",
      "    Batch 30/104, Loss: 0.2196\n",
      "    Batch 40/104, Loss: 0.2615\n",
      "    Batch 50/104, Loss: 0.2973\n",
      "    Batch 60/104, Loss: 0.2301\n",
      "    Batch 70/104, Loss: 0.2136\n",
      "    Batch 80/104, Loss: 0.2626\n",
      "    Batch 90/104, Loss: 0.2357\n",
      "    Batch 100/104, Loss: 0.2185\n",
      "\n",
      "  è¨“ç·´æå¤±: 0.2449\n",
      "  é©—è­‰æå¤±: 0.7564\n",
      "  é©—è­‰ Dice: 0.0822 ğŸ¯ æ–°ç´€éŒ„!\n",
      "    Class 1 - Dice: 0.1381, IoU: 0.0742\n",
      "    Class 2 - Dice: 0.0263, IoU: 0.0133\n",
      "  å­¸ç¿’ç‡: 0.000375\n",
      "  âœ“ å·²ä¿å­˜æœ€ä½³æ¨¡å‹\n",
      "\n",
      "Epoch 21/50\n",
      "----------------------------------------\n",
      "    Batch 0/104, Loss: 0.2212\n",
      "    Batch 10/104, Loss: 0.2526\n",
      "    Batch 20/104, Loss: 0.2059\n",
      "    Batch 30/104, Loss: 0.3114\n",
      "    Batch 40/104, Loss: 0.2749\n",
      "    Batch 50/104, Loss: 0.2167\n",
      "    Batch 60/104, Loss: 0.2415\n",
      "    Batch 70/104, Loss: 0.2892\n",
      "    Batch 80/104, Loss: 0.3211\n",
      "    Batch 90/104, Loss: 0.2130\n",
      "    Batch 100/104, Loss: 0.2421\n",
      "\n",
      "  è¨“ç·´æå¤±: 0.2390\n",
      "  é©—è­‰æå¤±: 0.7516\n",
      "  é©—è­‰ Dice: 0.0933 ğŸ¯ æ–°ç´€éŒ„!\n",
      "    Class 1 - Dice: 0.1487, IoU: 0.0803\n",
      "    Class 2 - Dice: 0.0379, IoU: 0.0193\n",
      "  å­¸ç¿’ç‡: 0.000360\n",
      "  âœ“ å·²ä¿å­˜æœ€ä½³æ¨¡å‹\n",
      "\n",
      "Epoch 22/50\n",
      "----------------------------------------\n",
      "    Batch 0/104, Loss: 0.2440\n",
      "    Batch 10/104, Loss: 0.2588\n",
      "    Batch 20/104, Loss: 0.2226\n",
      "    Batch 30/104, Loss: 0.2204\n",
      "    Batch 40/104, Loss: 0.2135\n",
      "    Batch 50/104, Loss: 0.1846\n",
      "    Batch 60/104, Loss: 0.2836\n",
      "    Batch 70/104, Loss: 0.2168\n",
      "    Batch 80/104, Loss: 0.2517\n",
      "    Batch 90/104, Loss: 0.2058\n",
      "    Batch 100/104, Loss: 0.2937\n",
      "\n",
      "  è¨“ç·´æå¤±: 0.2377\n",
      "  é©—è­‰æå¤±: 0.7458\n",
      "  é©—è­‰ Dice: 0.1121 ğŸ¯ æ–°ç´€éŒ„!\n",
      "    Class 1 - Dice: 0.1647, IoU: 0.0898\n",
      "    Class 2 - Dice: 0.0595, IoU: 0.0307\n",
      "  å­¸ç¿’ç‡: 0.000344\n",
      "  âœ“ å·²ä¿å­˜æœ€ä½³æ¨¡å‹\n",
      "\n",
      "Epoch 23/50\n",
      "----------------------------------------\n",
      "    Batch 0/104, Loss: 0.2837\n",
      "    Batch 10/104, Loss: 0.2183\n",
      "    Batch 20/104, Loss: 0.2120\n",
      "    Batch 30/104, Loss: 0.1842\n",
      "    Batch 40/104, Loss: 0.1926\n",
      "    Batch 50/104, Loss: 0.2034\n",
      "    Batch 60/104, Loss: 0.2200\n",
      "    Batch 70/104, Loss: 0.2494\n",
      "    Batch 80/104, Loss: 0.2307\n",
      "    Batch 90/104, Loss: 0.2246\n",
      "    Batch 100/104, Loss: 0.2343\n",
      "\n",
      "  è¨“ç·´æå¤±: 0.2362\n",
      "  é©—è­‰æå¤±: 0.7391\n",
      "  é©—è­‰ Dice: 0.1409 ğŸ¯ æ–°ç´€éŒ„!\n",
      "    Class 1 - Dice: 0.1894, IoU: 0.1046\n",
      "    Class 2 - Dice: 0.0924, IoU: 0.0485\n",
      "  å­¸ç¿’ç‡: 0.000328\n",
      "  âœ“ å·²ä¿å­˜æœ€ä½³æ¨¡å‹\n",
      "\n",
      "Epoch 24/50\n",
      "----------------------------------------\n",
      "    Batch 0/104, Loss: 0.1992\n",
      "    Batch 10/104, Loss: 0.1964\n",
      "    Batch 20/104, Loss: 0.1947\n",
      "    Batch 30/104, Loss: 0.2055\n",
      "    Batch 40/104, Loss: 0.2276\n",
      "    Batch 50/104, Loss: 0.2825\n",
      "    Batch 60/104, Loss: 0.2565\n",
      "    Batch 70/104, Loss: 0.2206\n",
      "    Batch 80/104, Loss: 0.2626\n",
      "    Batch 90/104, Loss: 0.2463\n",
      "    Batch 100/104, Loss: 0.1931\n",
      "\n",
      "  è¨“ç·´æå¤±: 0.2333\n",
      "  é©—è­‰æå¤±: 0.7312\n",
      "  é©—è­‰ Dice: 0.1855 ğŸ¯ æ–°ç´€éŒ„!\n",
      "    Class 1 - Dice: 0.2325, IoU: 0.1316\n",
      "    Class 2 - Dice: 0.1384, IoU: 0.0744\n",
      "  å­¸ç¿’ç‡: 0.000311\n",
      "  âœ“ å·²ä¿å­˜æœ€ä½³æ¨¡å‹\n",
      "\n",
      "Epoch 25/50\n",
      "----------------------------------------\n",
      "    Batch 0/104, Loss: 0.2146\n",
      "    Batch 10/104, Loss: 0.2140\n",
      "    Batch 20/104, Loss: 0.2795\n",
      "    Batch 30/104, Loss: 0.2267\n",
      "    Batch 40/104, Loss: 0.2660\n",
      "    Batch 50/104, Loss: 0.2701\n",
      "    Batch 60/104, Loss: 0.2465\n",
      "    Batch 70/104, Loss: 0.2331\n",
      "    Batch 80/104, Loss: 0.2043\n",
      "    Batch 90/104, Loss: 0.1948\n",
      "    Batch 100/104, Loss: 0.2545\n",
      "\n",
      "  è¨“ç·´æå¤±: 0.2369\n",
      "  é©—è­‰æå¤±: 0.7220\n",
      "  é©—è­‰ Dice: 0.2504 ğŸ¯ æ–°ç´€éŒ„!\n",
      "    Class 1 - Dice: 0.3024, IoU: 0.1781\n",
      "    Class 2 - Dice: 0.1983, IoU: 0.1101\n",
      "  å­¸ç¿’ç‡: 0.000294\n",
      "  âœ“ å·²ä¿å­˜æœ€ä½³æ¨¡å‹\n",
      "\n",
      "Epoch 26/50\n",
      "----------------------------------------\n",
      "    Batch 0/104, Loss: 0.2381\n",
      "    Batch 10/104, Loss: 0.2503\n",
      "    Batch 20/104, Loss: 0.2591\n",
      "    Batch 30/104, Loss: 0.2002\n",
      "    Batch 40/104, Loss: 0.3141\n",
      "    Batch 50/104, Loss: 0.2641\n",
      "    Batch 60/104, Loss: 0.2348\n",
      "    Batch 70/104, Loss: 0.1723\n",
      "    Batch 80/104, Loss: 0.2514\n",
      "    Batch 90/104, Loss: 0.2236\n",
      "    Batch 100/104, Loss: 0.2134\n",
      "\n",
      "  è¨“ç·´æå¤±: 0.2324\n",
      "  é©—è­‰æå¤±: 0.7115\n",
      "  é©—è­‰ Dice: 0.3329 ğŸ¯ æ–°ç´€éŒ„!\n",
      "    Class 1 - Dice: 0.3923, IoU: 0.2440\n",
      "    Class 2 - Dice: 0.2735, IoU: 0.1584\n",
      "  å­¸ç¿’ç‡: 0.000277\n",
      "  âœ“ å·²ä¿å­˜æœ€ä½³æ¨¡å‹\n",
      "\n",
      "Epoch 27/50\n",
      "----------------------------------------\n",
      "    Batch 0/104, Loss: 0.2181\n",
      "    Batch 10/104, Loss: 0.2616\n",
      "    Batch 20/104, Loss: 0.2291\n",
      "    Batch 30/104, Loss: 0.2366\n",
      "    Batch 40/104, Loss: 0.1910\n",
      "    Batch 50/104, Loss: 0.2052\n",
      "    Batch 60/104, Loss: 0.1818\n",
      "    Batch 70/104, Loss: 0.2212\n",
      "    Batch 80/104, Loss: 0.2374\n",
      "    Batch 90/104, Loss: 0.2368\n",
      "    Batch 100/104, Loss: 0.2098\n",
      "\n",
      "  è¨“ç·´æå¤±: 0.2250\n",
      "  é©—è­‰æå¤±: 0.6998\n",
      "  é©—è­‰ Dice: 0.4239 ğŸ¯ æ–°ç´€éŒ„!\n",
      "    Class 1 - Dice: 0.4918, IoU: 0.3261\n",
      "    Class 2 - Dice: 0.3559, IoU: 0.2165\n",
      "  å­¸ç¿’ç‡: 0.000259\n",
      "  âœ“ å·²ä¿å­˜æœ€ä½³æ¨¡å‹\n",
      "\n",
      "Epoch 28/50\n",
      "----------------------------------------\n",
      "    Batch 0/104, Loss: 0.2300\n",
      "    Batch 10/104, Loss: 0.2794\n",
      "    Batch 20/104, Loss: 0.2334\n",
      "    Batch 30/104, Loss: 0.2348\n",
      "    Batch 40/104, Loss: 0.2009\n",
      "    Batch 50/104, Loss: 0.2249\n",
      "    Batch 60/104, Loss: 0.2110\n",
      "    Batch 70/104, Loss: 0.1797\n",
      "    Batch 80/104, Loss: 0.1967\n",
      "    Batch 90/104, Loss: 0.2029\n",
      "    Batch 100/104, Loss: 0.2239\n",
      "\n",
      "  è¨“ç·´æå¤±: 0.2274\n",
      "  é©—è­‰æå¤±: 0.6872\n",
      "  é©—è­‰ Dice: 0.5072 ğŸ¯ æ–°ç´€éŒ„!\n",
      "    Class 1 - Dice: 0.5810, IoU: 0.4095\n",
      "    Class 2 - Dice: 0.4334, IoU: 0.2766\n",
      "  å­¸ç¿’ç‡: 0.000242\n",
      "  âœ“ å·²ä¿å­˜æœ€ä½³æ¨¡å‹\n",
      "\n",
      "Epoch 29/50\n",
      "----------------------------------------\n",
      "    Batch 0/104, Loss: 0.2206\n",
      "    Batch 10/104, Loss: 0.2100\n",
      "    Batch 20/104, Loss: 0.2225\n",
      "    Batch 30/104, Loss: 0.2134\n",
      "    Batch 40/104, Loss: 0.1796\n",
      "    Batch 50/104, Loss: 0.2209\n",
      "    Batch 60/104, Loss: 0.2114\n",
      "    Batch 70/104, Loss: 0.2150\n",
      "    Batch 80/104, Loss: 0.2537\n",
      "    Batch 90/104, Loss: 0.2403\n",
      "    Batch 100/104, Loss: 0.1911\n",
      "\n",
      "  è¨“ç·´æå¤±: 0.2265\n",
      "  é©—è­‰æå¤±: 0.6736\n",
      "  é©—è­‰ Dice: 0.5773 ğŸ¯ æ–°ç´€éŒ„!\n",
      "    Class 1 - Dice: 0.6514, IoU: 0.4830\n",
      "    Class 2 - Dice: 0.5032, IoU: 0.3361\n",
      "  å­¸ç¿’ç‡: 0.000224\n",
      "  âœ“ å·²ä¿å­˜æœ€ä½³æ¨¡å‹\n",
      "\n",
      "Epoch 30/50\n",
      "----------------------------------------\n",
      "    Batch 0/104, Loss: 0.2239\n",
      "    Batch 10/104, Loss: 0.1889\n",
      "    Batch 20/104, Loss: 0.2242\n",
      "    Batch 30/104, Loss: 0.2728\n",
      "    Batch 40/104, Loss: 0.2397\n",
      "    Batch 50/104, Loss: 0.2104\n",
      "    Batch 60/104, Loss: 0.2289\n",
      "    Batch 70/104, Loss: 0.2171\n",
      "    Batch 80/104, Loss: 0.2183\n",
      "    Batch 90/104, Loss: 0.2339\n",
      "    Batch 100/104, Loss: 0.2886\n",
      "\n",
      "  è¨“ç·´æå¤±: 0.2233\n",
      "  é©—è­‰æå¤±: 0.6594\n",
      "  é©—è­‰ Dice: 0.6315 ğŸ¯ æ–°ç´€éŒ„!\n",
      "    Class 1 - Dice: 0.7003, IoU: 0.5388\n",
      "    Class 2 - Dice: 0.5627, IoU: 0.3915\n",
      "  å­¸ç¿’ç‡: 0.000207\n",
      "  âœ“ å·²ä¿å­˜æœ€ä½³æ¨¡å‹\n",
      "\n",
      "Epoch 31/50\n",
      "----------------------------------------\n",
      "    Batch 0/104, Loss: 0.2196\n",
      "    Batch 10/104, Loss: 0.1933\n",
      "    Batch 20/104, Loss: 0.2245\n",
      "    Batch 30/104, Loss: 0.1959\n",
      "    Batch 40/104, Loss: 0.2194\n",
      "    Batch 50/104, Loss: 0.1892\n",
      "    Batch 60/104, Loss: 0.2116\n",
      "    Batch 70/104, Loss: 0.2428\n",
      "    Batch 80/104, Loss: 0.2187\n",
      "    Batch 90/104, Loss: 0.2211\n",
      "    Batch 100/104, Loss: 0.2213\n",
      "\n",
      "  è¨“ç·´æå¤±: 0.2232\n",
      "  é©—è­‰æå¤±: 0.6448\n",
      "  é©—è­‰ Dice: 0.6730 ğŸ¯ æ–°ç´€éŒ„!\n",
      "    Class 1 - Dice: 0.7340, IoU: 0.5798\n",
      "    Class 2 - Dice: 0.6119, IoU: 0.4409\n",
      "  å­¸ç¿’ç‡: 0.000190\n",
      "  âœ“ å·²ä¿å­˜æœ€ä½³æ¨¡å‹\n",
      "\n",
      "Epoch 32/50\n",
      "----------------------------------------\n",
      "    Batch 0/104, Loss: 0.2939\n",
      "    Batch 10/104, Loss: 0.1784\n",
      "    Batch 20/104, Loss: 0.1645\n",
      "    Batch 30/104, Loss: 0.1842\n",
      "    Batch 40/104, Loss: 0.2014\n",
      "    Batch 50/104, Loss: 0.1714\n",
      "    Batch 60/104, Loss: 0.2546\n",
      "    Batch 70/104, Loss: 0.2327\n",
      "    Batch 80/104, Loss: 0.2571\n",
      "    Batch 90/104, Loss: 0.2183\n",
      "    Batch 100/104, Loss: 0.2146\n",
      "\n",
      "  è¨“ç·´æå¤±: 0.2197\n",
      "  é©—è­‰æå¤±: 0.6302\n",
      "  é©—è­‰ Dice: 0.7046 ğŸ¯ æ–°ç´€éŒ„!\n",
      "    Class 1 - Dice: 0.7574, IoU: 0.6095\n",
      "    Class 2 - Dice: 0.6518, IoU: 0.4835\n",
      "  å­¸ç¿’ç‡: 0.000173\n",
      "  âœ“ å·²ä¿å­˜æœ€ä½³æ¨¡å‹\n",
      "\n",
      "Epoch 33/50\n",
      "----------------------------------------\n",
      "    Batch 0/104, Loss: 0.2289\n",
      "    Batch 10/104, Loss: 0.2630\n",
      "    Batch 20/104, Loss: 0.1977\n",
      "    Batch 30/104, Loss: 0.1829\n",
      "    Batch 40/104, Loss: 0.2387\n",
      "    Batch 50/104, Loss: 0.2988\n",
      "    Batch 60/104, Loss: 0.2607\n",
      "    Batch 70/104, Loss: 0.2223\n",
      "    Batch 80/104, Loss: 0.1841\n",
      "    Batch 90/104, Loss: 0.2338\n",
      "    Batch 100/104, Loss: 0.2310\n",
      "\n",
      "  è¨“ç·´æå¤±: 0.2198\n",
      "  é©—è­‰æå¤±: 0.6154\n",
      "  é©—è­‰ Dice: 0.7298 ğŸ¯ æ–°ç´€éŒ„!\n",
      "    Class 1 - Dice: 0.7748, IoU: 0.6324\n",
      "    Class 2 - Dice: 0.6848, IoU: 0.5207\n",
      "  å­¸ç¿’ç‡: 0.000157\n",
      "  âœ“ å·²ä¿å­˜æœ€ä½³æ¨¡å‹\n",
      "\n",
      "Epoch 34/50\n",
      "----------------------------------------\n",
      "    Batch 0/104, Loss: 0.1568\n",
      "    Batch 10/104, Loss: 0.2826\n",
      "    Batch 20/104, Loss: 0.2204\n",
      "    Batch 30/104, Loss: 0.2435\n",
      "    Batch 40/104, Loss: 0.2346\n",
      "    Batch 50/104, Loss: 0.2160\n",
      "    Batch 60/104, Loss: 0.1821\n",
      "    Batch 70/104, Loss: 0.1780\n",
      "    Batch 80/104, Loss: 0.1909\n",
      "    Batch 90/104, Loss: 0.1761\n",
      "    Batch 100/104, Loss: 0.2313\n",
      "\n",
      "  è¨“ç·´æå¤±: 0.2181\n",
      "  é©—è­‰æå¤±: 0.6004\n",
      "  é©—è­‰ Dice: 0.7499 ğŸ¯ æ–°ç´€éŒ„!\n",
      "    Class 1 - Dice: 0.7880, IoU: 0.6502\n",
      "    Class 2 - Dice: 0.7118, IoU: 0.5526\n",
      "  å­¸ç¿’ç‡: 0.000141\n",
      "  âœ“ å·²ä¿å­˜æœ€ä½³æ¨¡å‹\n",
      "\n",
      "Epoch 35/50\n",
      "----------------------------------------\n",
      "    Batch 0/104, Loss: 0.2709\n",
      "    Batch 10/104, Loss: 0.1938\n",
      "    Batch 20/104, Loss: 0.2119\n",
      "    Batch 30/104, Loss: 0.2362\n",
      "    Batch 40/104, Loss: 0.2471\n",
      "    Batch 50/104, Loss: 0.2256\n",
      "    Batch 60/104, Loss: 0.1878\n",
      "    Batch 70/104, Loss: 0.1884\n",
      "    Batch 80/104, Loss: 0.2291\n",
      "    Batch 90/104, Loss: 0.2143\n",
      "    Batch 100/104, Loss: 0.2073\n",
      "\n",
      "  è¨“ç·´æå¤±: 0.2150\n",
      "  é©—è­‰æå¤±: 0.5858\n",
      "  é©—è­‰ Dice: 0.7659 ğŸ¯ æ–°ç´€éŒ„!\n",
      "    Class 1 - Dice: 0.7984, IoU: 0.6644\n",
      "    Class 2 - Dice: 0.7333, IoU: 0.5790\n",
      "  å­¸ç¿’ç‡: 0.000126\n",
      "  âœ“ å·²ä¿å­˜æœ€ä½³æ¨¡å‹\n",
      "\n",
      "Epoch 36/50\n",
      "----------------------------------------\n",
      "    Batch 0/104, Loss: 0.2196\n",
      "    Batch 10/104, Loss: 0.1850\n",
      "    Batch 20/104, Loss: 0.2300\n",
      "    Batch 30/104, Loss: 0.2414\n",
      "    Batch 40/104, Loss: 0.2320\n",
      "    Batch 50/104, Loss: 0.1761\n",
      "    Batch 60/104, Loss: 0.1973\n",
      "    Batch 70/104, Loss: 0.2137\n",
      "    Batch 80/104, Loss: 0.1982\n",
      "    Batch 90/104, Loss: 0.2240\n",
      "    Batch 100/104, Loss: 0.1770\n",
      "\n",
      "  è¨“ç·´æå¤±: 0.2144\n",
      "  é©—è­‰æå¤±: 0.5711\n",
      "  é©—è­‰ Dice: 0.7791 ğŸ¯ æ–°ç´€éŒ„!\n",
      "    Class 1 - Dice: 0.8072, IoU: 0.6767\n",
      "    Class 2 - Dice: 0.7510, IoU: 0.6013\n",
      "  å­¸ç¿’ç‡: 0.000111\n",
      "  âœ“ å·²ä¿å­˜æœ€ä½³æ¨¡å‹\n",
      "\n",
      "Epoch 37/50\n",
      "----------------------------------------\n",
      "    Batch 0/104, Loss: 0.1947\n",
      "    Batch 10/104, Loss: 0.1856\n",
      "    Batch 20/104, Loss: 0.1898\n",
      "    Batch 30/104, Loss: 0.2181\n",
      "    Batch 40/104, Loss: 0.1699\n",
      "    Batch 50/104, Loss: 0.2052\n",
      "    Batch 60/104, Loss: 0.2149\n",
      "    Batch 70/104, Loss: 0.1729\n",
      "    Batch 80/104, Loss: 0.1985\n",
      "    Batch 90/104, Loss: 0.2876\n",
      "    Batch 100/104, Loss: 0.2363\n",
      "\n",
      "  è¨“ç·´æå¤±: 0.2136\n",
      "  é©—è­‰æå¤±: 0.5566\n",
      "  é©—è­‰ Dice: 0.7896 ğŸ¯ æ–°ç´€éŒ„!\n",
      "    Class 1 - Dice: 0.8141, IoU: 0.6865\n",
      "    Class 2 - Dice: 0.7651, IoU: 0.6195\n",
      "  å­¸ç¿’ç‡: 0.000097\n",
      "  âœ“ å·²ä¿å­˜æœ€ä½³æ¨¡å‹\n",
      "\n",
      "Epoch 38/50\n",
      "----------------------------------------\n",
      "    Batch 0/104, Loss: 0.1966\n",
      "    Batch 10/104, Loss: 0.2136\n",
      "    Batch 20/104, Loss: 0.2020\n",
      "    Batch 30/104, Loss: 0.2928\n",
      "    Batch 40/104, Loss: 0.3115\n",
      "    Batch 50/104, Loss: 0.2134\n",
      "    Batch 60/104, Loss: 0.2295\n",
      "    Batch 70/104, Loss: 0.2317\n",
      "    Batch 80/104, Loss: 0.1850\n",
      "    Batch 90/104, Loss: 0.2013\n",
      "    Batch 100/104, Loss: 0.1974\n",
      "\n",
      "  è¨“ç·´æå¤±: 0.2128\n",
      "  é©—è­‰æå¤±: 0.5423\n",
      "  é©—è­‰ Dice: 0.7979 ğŸ¯ æ–°ç´€éŒ„!\n",
      "    Class 1 - Dice: 0.8197, IoU: 0.6945\n",
      "    Class 2 - Dice: 0.7761, IoU: 0.6341\n",
      "  å­¸ç¿’ç‡: 0.000084\n",
      "  âœ“ å·²ä¿å­˜æœ€ä½³æ¨¡å‹\n",
      "\n",
      "Epoch 39/50\n",
      "----------------------------------------\n",
      "    Batch 0/104, Loss: 0.2373\n",
      "    Batch 10/104, Loss: 0.1845\n",
      "    Batch 20/104, Loss: 0.2014\n",
      "    Batch 30/104, Loss: 0.2139\n",
      "    Batch 40/104, Loss: 0.2932\n",
      "    Batch 50/104, Loss: 0.2282\n",
      "    Batch 60/104, Loss: 0.2688\n",
      "    Batch 70/104, Loss: 0.1982\n",
      "    Batch 80/104, Loss: 0.2369\n",
      "    Batch 90/104, Loss: 0.2028\n",
      "    Batch 100/104, Loss: 0.2194\n",
      "\n",
      "  è¨“ç·´æå¤±: 0.2103\n",
      "  é©—è­‰æå¤±: 0.5280\n",
      "  é©—è­‰ Dice: 0.8051 ğŸ¯ æ–°ç´€éŒ„!\n",
      "    Class 1 - Dice: 0.8247, IoU: 0.7018\n",
      "    Class 2 - Dice: 0.7855, IoU: 0.6468\n",
      "  å­¸ç¿’ç‡: 0.000071\n",
      "  âœ“ å·²ä¿å­˜æœ€ä½³æ¨¡å‹\n",
      "\n",
      "Epoch 40/50\n",
      "----------------------------------------\n",
      "    Batch 0/104, Loss: 0.2020\n",
      "    Batch 10/104, Loss: 0.2095\n",
      "    Batch 20/104, Loss: 0.2193\n",
      "    Batch 30/104, Loss: 0.2163\n",
      "    Batch 40/104, Loss: 0.2202\n",
      "    Batch 50/104, Loss: 0.2518\n",
      "    Batch 60/104, Loss: 0.2182\n",
      "    Batch 70/104, Loss: 0.1847\n",
      "    Batch 80/104, Loss: 0.1960\n",
      "    Batch 90/104, Loss: 0.2228\n",
      "    Batch 100/104, Loss: 0.1759\n",
      "\n",
      "  è¨“ç·´æå¤±: 0.2111\n",
      "  é©—è­‰æå¤±: 0.5140\n",
      "  é©—è­‰ Dice: 0.8111 ğŸ¯ æ–°ç´€éŒ„!\n",
      "    Class 1 - Dice: 0.8290, IoU: 0.7080\n",
      "    Class 2 - Dice: 0.7932, IoU: 0.6573\n",
      "  å­¸ç¿’ç‡: 0.000059\n",
      "  âœ“ å·²ä¿å­˜æœ€ä½³æ¨¡å‹\n",
      "\n",
      "Epoch 41/50\n",
      "----------------------------------------\n",
      "    Batch 0/104, Loss: 0.1949\n",
      "    Batch 10/104, Loss: 0.2110\n",
      "    Batch 20/104, Loss: 0.1492\n",
      "    Batch 30/104, Loss: 0.2184\n",
      "    Batch 40/104, Loss: 0.1737\n",
      "    Batch 50/104, Loss: 0.2295\n",
      "    Batch 60/104, Loss: 0.1804\n",
      "    Batch 70/104, Loss: 0.2152\n",
      "    Batch 80/104, Loss: 0.2528\n",
      "    Batch 90/104, Loss: 0.2404\n",
      "    Batch 100/104, Loss: 0.1844\n",
      "\n",
      "  è¨“ç·´æå¤±: 0.2080\n",
      "  é©—è­‰æå¤±: 0.4999\n",
      "  é©—è­‰ Dice: 0.8162 ğŸ¯ æ–°ç´€éŒ„!\n",
      "    Class 1 - Dice: 0.8327, IoU: 0.7133\n",
      "    Class 2 - Dice: 0.7997, IoU: 0.6663\n",
      "  å­¸ç¿’ç‡: 0.000049\n",
      "  âœ“ å·²ä¿å­˜æœ€ä½³æ¨¡å‹\n",
      "\n",
      "Epoch 42/50\n",
      "----------------------------------------\n",
      "    Batch 0/104, Loss: 0.2156\n",
      "    Batch 10/104, Loss: 0.2141\n",
      "    Batch 20/104, Loss: 0.1997\n",
      "    Batch 30/104, Loss: 0.1931\n",
      "    Batch 40/104, Loss: 0.1901\n",
      "    Batch 50/104, Loss: 0.2282\n",
      "    Batch 60/104, Loss: 0.2228\n",
      "    Batch 70/104, Loss: 0.2054\n",
      "    Batch 80/104, Loss: 0.1647\n",
      "    Batch 90/104, Loss: 0.1729\n",
      "    Batch 100/104, Loss: 0.2099\n",
      "\n",
      "  è¨“ç·´æå¤±: 0.2070\n",
      "  é©—è­‰æå¤±: 0.4862\n",
      "  é©—è­‰ Dice: 0.8204 ğŸ¯ æ–°ç´€éŒ„!\n",
      "    Class 1 - Dice: 0.8357, IoU: 0.7178\n",
      "    Class 2 - Dice: 0.8051, IoU: 0.6738\n",
      "  å­¸ç¿’ç‡: 0.000039\n",
      "  âœ“ å·²ä¿å­˜æœ€ä½³æ¨¡å‹\n",
      "\n",
      "Epoch 43/50\n",
      "----------------------------------------\n",
      "    Batch 0/104, Loss: 0.2384\n",
      "    Batch 10/104, Loss: 0.1584\n",
      "    Batch 20/104, Loss: 0.2302\n",
      "    Batch 30/104, Loss: 0.2301\n",
      "    Batch 40/104, Loss: 0.2351\n",
      "    Batch 50/104, Loss: 0.1801\n",
      "    Batch 60/104, Loss: 0.1902\n",
      "    Batch 70/104, Loss: 0.2013\n",
      "    Batch 80/104, Loss: 0.1699\n",
      "    Batch 90/104, Loss: 0.1499\n",
      "    Batch 100/104, Loss: 0.1625\n",
      "\n",
      "  è¨“ç·´æå¤±: 0.2061\n",
      "  é©—è­‰æå¤±: 0.4727\n",
      "  é©—è­‰ Dice: 0.8238 ğŸ¯ æ–°ç´€éŒ„!\n",
      "    Class 1 - Dice: 0.8383, IoU: 0.7216\n",
      "    Class 2 - Dice: 0.8092, IoU: 0.6796\n",
      "  å­¸ç¿’ç‡: 0.000030\n",
      "  âœ“ å·²ä¿å­˜æœ€ä½³æ¨¡å‹\n",
      "\n",
      "Epoch 44/50\n",
      "----------------------------------------\n",
      "    Batch 0/104, Loss: 0.1735\n",
      "    Batch 10/104, Loss: 0.2346\n",
      "    Batch 20/104, Loss: 0.2042\n",
      "    Batch 30/104, Loss: 0.1756\n",
      "    Batch 40/104, Loss: 0.2186\n",
      "    Batch 50/104, Loss: 0.1972\n",
      "    Batch 60/104, Loss: 0.2540\n",
      "    Batch 70/104, Loss: 0.1921\n",
      "    Batch 80/104, Loss: 0.2261\n",
      "    Batch 90/104, Loss: 0.1931\n",
      "    Batch 100/104, Loss: 0.1813\n",
      "\n",
      "  è¨“ç·´æå¤±: 0.2056\n",
      "  é©—è­‰æå¤±: 0.4595\n",
      "  é©—è­‰ Dice: 0.8269 ğŸ¯ æ–°ç´€éŒ„!\n",
      "    Class 1 - Dice: 0.8407, IoU: 0.7252\n",
      "    Class 2 - Dice: 0.8130, IoU: 0.6849\n",
      "  å­¸ç¿’ç‡: 0.000023\n",
      "  âœ“ å·²ä¿å­˜æœ€ä½³æ¨¡å‹\n",
      "\n",
      "Epoch 45/50\n",
      "----------------------------------------\n",
      "    Batch 0/104, Loss: 0.2393\n",
      "    Batch 10/104, Loss: 0.1707\n",
      "    Batch 20/104, Loss: 0.1851\n",
      "    Batch 30/104, Loss: 0.2214\n",
      "    Batch 40/104, Loss: 0.1730\n",
      "    Batch 50/104, Loss: 0.2112\n",
      "    Batch 60/104, Loss: 0.2002\n",
      "    Batch 70/104, Loss: 0.2533\n",
      "    Batch 80/104, Loss: 0.1706\n",
      "    Batch 90/104, Loss: 0.2037\n",
      "    Batch 100/104, Loss: 0.1925\n",
      "\n",
      "  è¨“ç·´æå¤±: 0.2055\n",
      "  é©—è­‰æå¤±: 0.4465\n",
      "  é©—è­‰ Dice: 0.8295 ğŸ¯ æ–°ç´€éŒ„!\n",
      "    Class 1 - Dice: 0.8430, IoU: 0.7285\n",
      "    Class 2 - Dice: 0.8160, IoU: 0.6892\n",
      "  å­¸ç¿’ç‡: 0.000016\n",
      "  âœ“ å·²ä¿å­˜æœ€ä½³æ¨¡å‹\n",
      "\n",
      "Epoch 46/50\n",
      "----------------------------------------\n",
      "    Batch 0/104, Loss: 0.2067\n",
      "    Batch 10/104, Loss: 0.1807\n",
      "    Batch 20/104, Loss: 0.1796\n",
      "    Batch 30/104, Loss: 0.1915\n",
      "    Batch 40/104, Loss: 0.1896\n",
      "    Batch 50/104, Loss: 0.2398\n",
      "    Batch 60/104, Loss: 0.2256\n",
      "    Batch 70/104, Loss: 0.1722\n",
      "    Batch 80/104, Loss: 0.2005\n",
      "    Batch 90/104, Loss: 0.2193\n",
      "    Batch 100/104, Loss: 0.1823\n",
      "\n",
      "  è¨“ç·´æå¤±: 0.2053\n",
      "  é©—è­‰æå¤±: 0.4338\n",
      "  é©—è­‰ Dice: 0.8319 ğŸ¯ æ–°ç´€éŒ„!\n",
      "    Class 1 - Dice: 0.8450, IoU: 0.7315\n",
      "    Class 2 - Dice: 0.8189, IoU: 0.6933\n",
      "  å­¸ç¿’ç‡: 0.000011\n",
      "  âœ“ å·²ä¿å­˜æœ€ä½³æ¨¡å‹\n",
      "\n",
      "Epoch 47/50\n",
      "----------------------------------------\n",
      "    Batch 0/104, Loss: 0.1704\n",
      "    Batch 10/104, Loss: 0.2298\n",
      "    Batch 20/104, Loss: 0.2136\n",
      "    Batch 30/104, Loss: 0.1664\n",
      "    Batch 40/104, Loss: 0.1729\n",
      "    Batch 50/104, Loss: 0.1861\n",
      "    Batch 60/104, Loss: 0.1929\n",
      "    Batch 70/104, Loss: 0.1877\n",
      "    Batch 80/104, Loss: 0.1951\n",
      "    Batch 90/104, Loss: 0.2318\n",
      "    Batch 100/104, Loss: 0.1530\n",
      "\n",
      "  è¨“ç·´æå¤±: 0.2045\n",
      "  é©—è­‰æå¤±: 0.4213\n",
      "  é©—è­‰ Dice: 0.8340 ğŸ¯ æ–°ç´€éŒ„!\n",
      "    Class 1 - Dice: 0.8467, IoU: 0.7341\n",
      "    Class 2 - Dice: 0.8213, IoU: 0.6968\n",
      "  å­¸ç¿’ç‡: 0.000006\n",
      "  âœ“ å·²ä¿å­˜æœ€ä½³æ¨¡å‹\n",
      "\n",
      "Epoch 48/50\n",
      "----------------------------------------\n",
      "    Batch 0/104, Loss: 0.1936\n",
      "    Batch 10/104, Loss: 0.2081\n",
      "    Batch 20/104, Loss: 0.2073\n",
      "    Batch 30/104, Loss: 0.1898\n",
      "    Batch 40/104, Loss: 0.2686\n",
      "    Batch 50/104, Loss: 0.2246\n",
      "    Batch 60/104, Loss: 0.1784\n",
      "    Batch 70/104, Loss: 0.2031\n",
      "    Batch 80/104, Loss: 0.2000\n",
      "    Batch 90/104, Loss: 0.2161\n",
      "    Batch 100/104, Loss: 0.1895\n",
      "\n",
      "  è¨“ç·´æå¤±: 0.2046\n",
      "  é©—è­‰æå¤±: 0.4093\n",
      "  é©—è­‰ Dice: 0.8359 ğŸ¯ æ–°ç´€éŒ„!\n",
      "    Class 1 - Dice: 0.8484, IoU: 0.7366\n",
      "    Class 2 - Dice: 0.8234, IoU: 0.6998\n",
      "  å­¸ç¿’ç‡: 0.000003\n",
      "  âœ“ å·²ä¿å­˜æœ€ä½³æ¨¡å‹\n",
      "\n",
      "Epoch 49/50\n",
      "----------------------------------------\n",
      "    Batch 0/104, Loss: 0.1793\n",
      "    Batch 10/104, Loss: 0.2091\n",
      "    Batch 20/104, Loss: 0.1865\n",
      "    Batch 30/104, Loss: 0.2234\n",
      "    Batch 40/104, Loss: 0.1670\n",
      "    Batch 50/104, Loss: 0.2257\n",
      "    Batch 60/104, Loss: 0.1839\n",
      "    Batch 70/104, Loss: 0.1777\n",
      "    Batch 80/104, Loss: 0.1976\n",
      "    Batch 90/104, Loss: 0.1614\n",
      "    Batch 100/104, Loss: 0.1725\n",
      "\n",
      "  è¨“ç·´æå¤±: 0.2041\n",
      "  é©—è­‰æå¤±: 0.3976\n",
      "  é©—è­‰ Dice: 0.8376 ğŸ¯ æ–°ç´€éŒ„!\n",
      "    Class 1 - Dice: 0.8498, IoU: 0.7389\n",
      "    Class 2 - Dice: 0.8253, IoU: 0.7026\n",
      "  å­¸ç¿’ç‡: 0.000002\n",
      "  âœ“ å·²ä¿å­˜æœ€ä½³æ¨¡å‹\n",
      "\n",
      "Epoch 50/50\n",
      "----------------------------------------\n",
      "    Batch 0/104, Loss: 0.1909\n",
      "    Batch 10/104, Loss: 0.1830\n",
      "    Batch 20/104, Loss: 0.2051\n",
      "    Batch 30/104, Loss: 0.1920\n",
      "    Batch 40/104, Loss: 0.2020\n",
      "    Batch 50/104, Loss: 0.2119\n",
      "    Batch 60/104, Loss: 0.2359\n",
      "    Batch 70/104, Loss: 0.1863\n",
      "    Batch 80/104, Loss: 0.1811\n",
      "    Batch 90/104, Loss: 0.2286\n",
      "    Batch 100/104, Loss: 0.2509\n",
      "\n",
      "  è¨“ç·´æå¤±: 0.2027\n",
      "  é©—è­‰æå¤±: 0.3861\n",
      "  é©—è­‰ Dice: 0.8391 ğŸ¯ æ–°ç´€éŒ„!\n",
      "    Class 1 - Dice: 0.8512, IoU: 0.7409\n",
      "    Class 2 - Dice: 0.8270, IoU: 0.7050\n",
      "  å­¸ç¿’ç‡: 0.000001\n",
      "  âœ“ å·²ä¿å­˜æœ€ä½³æ¨¡å‹\n",
      "\n",
      "================================================================================\n",
      "è¨“ç·´å®Œæˆ! æœ€ä½³ Dice: 0.8391\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "ğŸš€ æå‡ Dice çš„æ”¹é€²ç‰ˆè¨“ç·´ç³»çµ±\n",
    "ç­–ç•¥ï¼šæ›´å¤§æ¨¡å‹ + æ›´å¼·å¢å¼· + LR warmup + æ›´é•·è¨“ç·´\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"æå‡ Dice çš„æ”¹é€²ç‰ˆè¨“ç·´ç³»çµ±\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "import json\n",
    "import math\n",
    "import copy\n",
    "import random\n",
    "from types import ModuleType\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "print(f\"\\nPyTorch {torch.__version__}\")\n",
    "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
    "\n",
    "# ==================== Stub ====================\n",
    "\n",
    "def create_stub_module(name, attributes):\n",
    "    mod = ModuleType(name)\n",
    "    for attr_name, attr_value in attributes.items():\n",
    "        setattr(mod, attr_name, attr_value)\n",
    "    return mod\n",
    "\n",
    "sys.modules['torch._compile'] = create_stub_module('torch._compile', {'inner': lambda func: func})\n",
    "sys.modules['torch.onnx'] = create_stub_module('torch.onnx', {'is_in_onnx_export': False})\n",
    "\n",
    "print(\"\\nè¨­ç½®ç’°å¢ƒå®Œæˆ\")\n",
    "\n",
    "# ==================== æ”¹é€²çš„ Config ====================\n",
    "\n",
    "class ImprovedConfig:\n",
    "    def __init__(self):\n",
    "        self.seed = 42\n",
    "        self.data_root = '/workspace/Task04_Hippocampus'\n",
    "        self.output_dir = '/workspace/outputs/training_improved'\n",
    "        \n",
    "        # æ›´é•·è¨“ç·´\n",
    "        self.batch_size = 2\n",
    "        self.num_epochs = 50  # å¾ 30 å¢åŠ åˆ° 50\n",
    "        self.target_size = 64\n",
    "        \n",
    "        # æ”¹é€²çš„å­¸ç¿’ç‡ç­–ç•¥\n",
    "        self.lr = 5e-4  # å¾ 1e-3 é™åˆ° 5e-4ï¼ˆæ›´ç©©å®šï¼‰\n",
    "        self.weight_decay = 1e-4\n",
    "        self.lr_min = 1e-6  # å¾ 1e-5 é™åˆ° 1e-6\n",
    "        self.warmup_epochs = 5  # æ–°å¢ï¼šå‰ 5 epochs warmup\n",
    "        \n",
    "        # æ›´å¤§çš„æ¨¡å‹\n",
    "        self.model_base = 24  # å¾ 16 å¢åŠ åˆ° 24\n",
    "        self.num_classes = 3\n",
    "        self.deep_supervision = True\n",
    "        self.ds_weights = [1.0, 0.5, 0.25]\n",
    "        self.ds_warmup_epochs = 10  # å»¶é•·æ·±åº¦ç›£ç£æš–èº«\n",
    "        self.ds_warmup_weights = [1.0, 0.75, 0.5]\n",
    "        \n",
    "        self.use_amp = True\n",
    "        self.use_ema = True\n",
    "        self.ema_decay = 0.9999\n",
    "        self.grad_clip = 1.0\n",
    "        \n",
    "        # èª¿æ•´æå¤±æ¬Šé‡ï¼ˆæ›´é‡è¦– Diceï¼‰\n",
    "        self.use_focal = True\n",
    "        self.focal_alpha = [0.2, 1.0, 1.0]\n",
    "        self.focal_gamma = 2.0\n",
    "        self.loss_weights = [0.2, 0.8]  # CE:Dice = 0.2:0.8ï¼ˆå¾ 0.3:0.7ï¼‰\n",
    "        self.dice_ignore_background = True\n",
    "        \n",
    "        # æ›´å¼·çš„æ•¸æ“šå¢å¼·\n",
    "        self.aug_flip_prob = 0.5\n",
    "        self.aug_rotate_prob = 0.5\n",
    "        self.aug_gamma_prob = 0.5\n",
    "        self.aug_gamma_range = [0.7, 1.3]  # å¾ [0.8, 1.2] æ“´å¤§\n",
    "        self.aug_intensity_shift_prob = 0.5\n",
    "        self.aug_intensity_shift_range = [-0.15, 0.15]  # å¾ [-0.1, 0.1] æ“´å¤§\n",
    "        self.aug_intensity_scale_prob = 0.5\n",
    "        self.aug_intensity_scale_range = [0.85, 1.15]  # å¾ [0.9, 1.1] æ“´å¤§\n",
    "        self.aug_noise_prob = 0.3  # æ–°å¢ï¼šé«˜æ–¯å™ªè²\n",
    "        self.aug_noise_std = 0.1\n",
    "        self.aug_blur_prob = 0.2  # æ–°å¢ï¼šæ¨¡ç³Š\n",
    "        \n",
    "        self.num_workers = 4\n",
    "        self.pin_memory = True\n",
    "        self.persistent_workers = True\n",
    "        self.prefetch_factor = 2\n",
    "        \n",
    "        self.use_tf32 = True\n",
    "        self.use_channels_last = False\n",
    "        self.use_deterministic_algorithms = False\n",
    "    \n",
    "    def save(self, path):\n",
    "        with open(path, 'w') as f:\n",
    "            json.dump(self.__dict__, f, indent=2)\n",
    "    \n",
    "    @classmethod\n",
    "    def load(cls, path):\n",
    "        config = cls()\n",
    "        with open(path, 'r') as f:\n",
    "            config.__dict__.update(json.load(f))\n",
    "        return config\n",
    "\n",
    "# ==================== è¨­ç½® ====================\n",
    "\n",
    "def set_seed(seed=42, use_tf32=True):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "    if use_tf32 and torch.cuda.is_available() and torch.cuda.get_device_capability()[0] >= 8:\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "        torch.backends.cudnn.allow_tf32 = True\n",
    "        print(\"  TF32 enabled\")\n",
    "\n",
    "def worker_init_fn(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "def get_generator(seed):\n",
    "    g = torch.Generator()\n",
    "    g.manual_seed(seed)\n",
    "    return g\n",
    "\n",
    "def gcd(a, b):\n",
    "    while b:\n",
    "        a, b = b, a % b\n",
    "    return a\n",
    "\n",
    "MEMORY_FORMAT = getattr(torch, \"channels_last_3d\", torch.contiguous_format)\n",
    "\n",
    "# ==================== æ”¹é€²çš„æ•¸æ“šå¢å¼· ====================\n",
    "\n",
    "class ImprovedAugmentation3D:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "    \n",
    "    def random_flip(self, image, label):\n",
    "        if np.random.rand() < self.config.aug_flip_prob:\n",
    "            axis = np.random.choice([0, 1, 2])\n",
    "            image = torch.flip(image, dims=[axis + 1])\n",
    "            label = torch.flip(label, dims=[axis])\n",
    "        return image, label\n",
    "    \n",
    "    def random_rotate_90(self, image, label):\n",
    "        if np.random.rand() < self.config.aug_rotate_prob:\n",
    "            k = np.random.randint(1, 4)\n",
    "            image = torch.rot90(image, k, dims=[2, 3])\n",
    "            label = torch.rot90(label, k, dims=[1, 2])\n",
    "        return image, label\n",
    "    \n",
    "    def random_gamma(self, image):\n",
    "        if np.random.rand() < self.config.aug_gamma_prob:\n",
    "            gamma = np.random.uniform(*self.config.aug_gamma_range)\n",
    "            image_min = image.min()\n",
    "            image_max = image.max()\n",
    "            if image_max > image_min:\n",
    "                image_norm = (image - image_min) / (image_max - image_min)\n",
    "                image = torch.pow(image_norm, gamma)\n",
    "                image = image * (image_max - image_min) + image_min\n",
    "        return image\n",
    "    \n",
    "    def random_intensity_shift(self, image):\n",
    "        if np.random.rand() < self.config.aug_intensity_shift_prob:\n",
    "            shift = np.random.uniform(*self.config.aug_intensity_shift_range)\n",
    "            image = image + shift\n",
    "        return image\n",
    "    \n",
    "    def random_intensity_scale(self, image):\n",
    "        if np.random.rand() < self.config.aug_intensity_scale_prob:\n",
    "            scale = np.random.uniform(*self.config.aug_intensity_scale_range)\n",
    "            image = image * scale\n",
    "        return image\n",
    "    \n",
    "    def add_gaussian_noise(self, image):\n",
    "        \"\"\"æ–°å¢ï¼šé«˜æ–¯å™ªè²\"\"\"\n",
    "        if np.random.rand() < self.config.aug_noise_prob:\n",
    "            noise = torch.randn_like(image) * self.config.aug_noise_std\n",
    "            image = image + noise\n",
    "        return image\n",
    "    \n",
    "    def gaussian_blur(self, image):\n",
    "        \"\"\"æ–°å¢ï¼šé«˜æ–¯æ¨¡ç³Š\"\"\"\n",
    "        if np.random.rand() < self.config.aug_blur_prob:\n",
    "            # ä½¿ç”¨ avg pooling æ¨¡æ“¬æ¨¡ç³Š\n",
    "            image = F.avg_pool3d(image.unsqueeze(0), 3, stride=1, padding=1).squeeze(0)\n",
    "        return image\n",
    "    \n",
    "    def apply(self, image, label, is_training=True):\n",
    "        if not is_training:\n",
    "            return image, label\n",
    "        \n",
    "        image, label = self.random_flip(image, label)\n",
    "        image, label = self.random_rotate_90(image, label)\n",
    "        image = self.random_gamma(image)\n",
    "        image = self.random_intensity_shift(image)\n",
    "        image = self.random_intensity_scale(image)\n",
    "        image = self.add_gaussian_noise(image)\n",
    "        image = self.gaussian_blur(image)\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "# ==================== æ•¸æ“šé›† ====================\n",
    "\n",
    "class HippocampusDataset(Dataset):\n",
    "    def __init__(self, data_root, indices, config, is_training=True):\n",
    "        self.data_root = Path(data_root)\n",
    "        self.config = config\n",
    "        self.is_training = is_training\n",
    "        self.augmentation = ImprovedAugmentation3D(config)\n",
    "        \n",
    "        with open(self.data_root / 'dataset.json', 'r') as f:\n",
    "            metadata = json.load(f)\n",
    "        \n",
    "        training_data = metadata['training']\n",
    "        self.cases = [training_data[i] for i in indices]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.cases)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        case = self.cases[idx]\n",
    "        img_path = self.data_root / case['image'].lstrip('./')\n",
    "        lbl_path = self.data_root / case['label'].lstrip('./')\n",
    "        \n",
    "        image = nib.load(str(img_path)).get_fdata(dtype=np.float32)\n",
    "        label = nib.load(str(lbl_path)).get_fdata(dtype=np.float32)\n",
    "        \n",
    "        p1, p99 = np.percentile(image, [1, 99])\n",
    "        image = np.clip(image, p1, p99)\n",
    "        mean, std = image.mean(), image.std()\n",
    "        if std > 1e-8:\n",
    "            image = (image - mean) / std\n",
    "        \n",
    "        image_t = torch.from_numpy(image).unsqueeze(0).unsqueeze(0)\n",
    "        label_t = torch.from_numpy(label).unsqueeze(0).unsqueeze(0)\n",
    "        \n",
    "        image = F.interpolate(\n",
    "            image_t, size=(self.config.target_size,) * 3,\n",
    "            mode='trilinear', align_corners=False\n",
    "        ).squeeze(0)\n",
    "        \n",
    "        label = F.interpolate(\n",
    "            label_t, size=(self.config.target_size,) * 3,\n",
    "            mode='nearest'\n",
    "        ).squeeze(0).squeeze(0)\n",
    "        \n",
    "        label = torch.round(label).long()\n",
    "        label = torch.clamp(label, 0, 2)\n",
    "        \n",
    "        image, label = self.augmentation.apply(image, label, self.is_training)\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "def get_train_val_split(num_samples, train_ratio=0.8, seed=42):\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.permutation(num_samples)\n",
    "    split_idx = int(num_samples * train_ratio)\n",
    "    return indices[:split_idx], indices[split_idx:]\n",
    "\n",
    "# ==================== æ¨¡å‹ ====================\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, num_groups=8):\n",
    "        super().__init__()\n",
    "        actual_groups = gcd(num_groups, out_ch)\n",
    "        \n",
    "        self.conv1 = nn.Conv3d(in_ch, out_ch, 3, padding=1)\n",
    "        self.gn1 = nn.GroupNorm(actual_groups, out_ch)\n",
    "        self.conv2 = nn.Conv3d(out_ch, out_ch, 3, padding=1)\n",
    "        self.gn2 = nn.GroupNorm(actual_groups, out_ch)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.gn1(self.conv1(x)))\n",
    "        x = self.relu(self.gn2(self.conv2(x)))\n",
    "        return x\n",
    "\n",
    "class UNet3DDeepSupervision(nn.Module):\n",
    "    def __init__(self, in_ch=1, num_classes=3, base=16, deep_supervision=True):\n",
    "        super().__init__()\n",
    "        self.deep_supervision = deep_supervision\n",
    "        \n",
    "        self.enc1 = ConvBlock(in_ch, base)\n",
    "        self.enc2 = ConvBlock(base, base * 2)\n",
    "        self.enc3 = ConvBlock(base * 2, base * 4)\n",
    "        self.bottleneck = ConvBlock(base * 4, base * 8)\n",
    "        \n",
    "        self.up3 = nn.ConvTranspose3d(base * 8, base * 4, 2, stride=2)\n",
    "        self.dec3 = ConvBlock(base * 8, base * 4)\n",
    "        self.up2 = nn.ConvTranspose3d(base * 4, base * 2, 2, stride=2)\n",
    "        self.dec2 = ConvBlock(base * 4, base * 2)\n",
    "        self.up1 = nn.ConvTranspose3d(base * 2, base, 2, stride=2)\n",
    "        self.dec1 = ConvBlock(base * 2, base)\n",
    "        \n",
    "        self.out = nn.Conv3d(base, num_classes, 1)\n",
    "        self.pool = nn.MaxPool3d(2, 2)\n",
    "        \n",
    "        if deep_supervision:\n",
    "            self.ds_out3 = nn.Conv3d(base * 4, num_classes, 1)\n",
    "            self.ds_out2 = nn.Conv3d(base * 2, num_classes, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        e1 = self.enc1(x)\n",
    "        e2 = self.enc2(self.pool(e1))\n",
    "        e3 = self.enc3(self.pool(e2))\n",
    "        b = self.bottleneck(self.pool(e3))\n",
    "        \n",
    "        d3 = self.dec3(torch.cat([self.up3(b), e3], 1))\n",
    "        d2 = self.dec2(torch.cat([self.up2(d3), e2], 1))\n",
    "        d1 = self.dec1(torch.cat([self.up1(d2), e1], 1))\n",
    "        \n",
    "        out = self.out(d1)\n",
    "        \n",
    "        if self.deep_supervision and self.training:\n",
    "            ds3 = F.interpolate(self.ds_out3(d3), size=out.shape[2:], mode='trilinear', align_corners=False)\n",
    "            ds2 = F.interpolate(self.ds_out2(d2), size=out.shape[2:], mode='trilinear', align_corners=False)\n",
    "            return out, ds3, ds2\n",
    "        \n",
    "        return out\n",
    "\n",
    "# ==================== EMA ====================\n",
    "\n",
    "class ModelEMA:\n",
    "    def __init__(self, model, decay=0.9999):\n",
    "        self.model = copy.deepcopy(model).eval()\n",
    "        self.decay = decay\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    def update(self, model):\n",
    "        with torch.no_grad():\n",
    "            model_state = model.state_dict()\n",
    "            ema_state = self.model.state_dict()\n",
    "            assert model_state.keys() == ema_state.keys()\n",
    "            for key in ema_state.keys():\n",
    "                if ema_state[key].dtype.is_floating_point:\n",
    "                    ema_state[key].mul_(self.decay).add_(model_state[key], alpha=1 - self.decay)\n",
    "                else:\n",
    "                    ema_state[key].copy_(model_state[key])\n",
    "\n",
    "# ==================== æå¤±å‡½æ•¸ ====================\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=None, gamma=2.0):\n",
    "        super().__init__()\n",
    "        if alpha is not None:\n",
    "            self.register_buffer('alpha', alpha)\n",
    "        else:\n",
    "            self.alpha = None\n",
    "        self.gamma = gamma\n",
    "    \n",
    "    def forward(self, pred, target):\n",
    "        ce_loss = F.cross_entropy(pred, target, reduction='none')\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = ((1 - pt) ** self.gamma) * ce_loss\n",
    "        if self.alpha is not None:\n",
    "            alpha_t = self.alpha[target]\n",
    "            focal_loss = alpha_t * focal_loss\n",
    "        return focal_loss.mean()\n",
    "\n",
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, smooth=1.0, ignore_index=None):\n",
    "        super().__init__()\n",
    "        self.smooth = smooth\n",
    "        self.ignore_index = ignore_index\n",
    "    \n",
    "    def forward(self, pred, target):\n",
    "        pred = F.softmax(pred, dim=1)\n",
    "        target_1h = F.one_hot(target, pred.shape[1]).permute(0, 4, 1, 2, 3).float()\n",
    "        \n",
    "        if self.ignore_index is not None:\n",
    "            keep = [i for i in range(pred.shape[1]) if i != self.ignore_index]\n",
    "            pred = pred[:, keep]\n",
    "            target_1h = target_1h[:, keep]\n",
    "        \n",
    "        inter = (pred * target_1h).sum(dim=(2, 3, 4))\n",
    "        union = pred.sum(dim=(2, 3, 4)) + target_1h.sum(dim=(2, 3, 4))\n",
    "        dice = (2. * inter + self.smooth) / (union + self.smooth)\n",
    "        return 1 - dice.mean()\n",
    "\n",
    "class CombinedLossDeepSupervision(nn.Module):\n",
    "    def __init__(self, config, device):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.loss_weights = config.loss_weights\n",
    "        \n",
    "        if config.use_focal:\n",
    "            alpha = torch.tensor(config.focal_alpha, device=device)\n",
    "            self.ce = FocalLoss(alpha=alpha, gamma=config.focal_gamma)\n",
    "        else:\n",
    "            weight = torch.tensor(config.focal_alpha, device=device)\n",
    "            self.ce = nn.CrossEntropyLoss(weight=weight)\n",
    "        \n",
    "        ignore_idx = 0 if config.dice_ignore_background else None\n",
    "        self.dice = DiceLoss(ignore_index=ignore_idx)\n",
    "    \n",
    "    def forward(self, outputs, target, epoch=None):\n",
    "        ce_weight, dice_weight = self.loss_weights\n",
    "        \n",
    "        if epoch is not None and epoch <= self.config.ds_warmup_epochs:\n",
    "            ds_weights = self.config.ds_warmup_weights\n",
    "        else:\n",
    "            ds_weights = self.config.ds_weights\n",
    "        \n",
    "        if isinstance(outputs, tuple):\n",
    "            main_out, ds3, ds2 = outputs\n",
    "            loss = ds_weights[0] * (ce_weight * self.ce(main_out, target) + dice_weight * self.dice(main_out, target))\n",
    "            loss += ds_weights[1] * (ce_weight * self.ce(ds3, target) + dice_weight * self.dice(ds3, target))\n",
    "            loss += ds_weights[2] * (ce_weight * self.ce(ds2, target) + dice_weight * self.dice(ds2, target))\n",
    "            return loss\n",
    "        else:\n",
    "            return ce_weight * self.ce(outputs, target) + dice_weight * self.dice(outputs, target)\n",
    "\n",
    "# ==================== æŒ‡æ¨™ ====================\n",
    "\n",
    "class DatasetMetrics:\n",
    "    def __init__(self, num_classes, device):\n",
    "        self.num_classes = num_classes\n",
    "        self.device = device\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        self.tp = torch.zeros(self.num_classes, dtype=torch.float32, device=self.device)\n",
    "        self.fp = torch.zeros(self.num_classes, dtype=torch.float32, device=self.device)\n",
    "        self.fn = torch.zeros(self.num_classes, dtype=torch.float32, device=self.device)\n",
    "    \n",
    "    def update(self, pred, target):\n",
    "        for cls in range(1, self.num_classes):\n",
    "            pred_mask = (pred == cls)\n",
    "            target_mask = (target == cls)\n",
    "            self.tp[cls] += (pred_mask & target_mask).sum().to(self.tp.dtype)\n",
    "            self.fp[cls] += (pred_mask & ~target_mask).sum().to(self.fp.dtype)\n",
    "            self.fn[cls] += (~pred_mask & target_mask).sum().to(self.fn.dtype)\n",
    "    \n",
    "    def compute(self):\n",
    "        smooth = 1.0\n",
    "        tp = self.tp.detach().cpu()\n",
    "        fp = self.fp.detach().cpu()\n",
    "        fn = self.fn.detach().cpu()\n",
    "        \n",
    "        dice_scores = {}\n",
    "        iou_scores = {}\n",
    "        \n",
    "        for cls in range(1, self.num_classes):\n",
    "            dice = (2 * tp[cls] + smooth) / (2 * tp[cls] + fp[cls] + fn[cls] + smooth)\n",
    "            iou = (tp[cls] + smooth) / (tp[cls] + fp[cls] + fn[cls] + smooth)\n",
    "            dice_scores[cls] = dice.item()\n",
    "            iou_scores[cls] = iou.item()\n",
    "        \n",
    "        return dice_scores, iou_scores\n",
    "\n",
    "# ==================== Warmup Cosine Scheduler ====================\n",
    "\n",
    "class WarmupCosineAnnealingLR:\n",
    "    def __init__(self, optimizer, T_max, warmup_epochs, eta_min=0):\n",
    "        self.optimizer = optimizer\n",
    "        self.T_max = T_max\n",
    "        self.warmup_epochs = warmup_epochs\n",
    "        self.eta_min = eta_min\n",
    "        self.base_lr = optimizer.param_groups[0]['lr']\n",
    "        self.current_epoch = 0\n",
    "    \n",
    "    def step(self):\n",
    "        self.current_epoch += 1\n",
    "        \n",
    "        if self.current_epoch <= self.warmup_epochs:\n",
    "            # Warmup: ç·šæ€§å¢é•·\n",
    "            lr = self.base_lr * (self.current_epoch / self.warmup_epochs)\n",
    "        else:\n",
    "            # Cosine annealing\n",
    "            progress = (self.current_epoch - self.warmup_epochs) / (self.T_max - self.warmup_epochs)\n",
    "            lr = self.eta_min + (self.base_lr - self.eta_min) * (1 + math.cos(math.pi * progress)) / 2\n",
    "        \n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "    \n",
    "    def get_lr(self):\n",
    "        return self.optimizer.param_groups[0]['lr']\n",
    "\n",
    "# ==================== è¨“ç·´å™¨ ====================\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model, train_loader, val_loader, device, config):\n",
    "        self.model = model.to(device)\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.device = device\n",
    "        self.config = config\n",
    "        self.output_dir = Path(config.output_dir)\n",
    "        self.output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        self.optimizer = torch.optim.AdamW(\n",
    "            model.parameters(),\n",
    "            lr=config.lr,\n",
    "            weight_decay=config.weight_decay,\n",
    "            betas=(0.9, 0.999)\n",
    "        )\n",
    "        \n",
    "        self.scheduler = WarmupCosineAnnealingLR(\n",
    "            self.optimizer, \n",
    "            T_max=config.num_epochs, \n",
    "            warmup_epochs=config.warmup_epochs,\n",
    "            eta_min=config.lr_min\n",
    "        )\n",
    "        self.ema = ModelEMA(model, decay=config.ema_decay) if config.use_ema else None\n",
    "        self.criterion = CombinedLossDeepSupervision(config, device)\n",
    "        \n",
    "        self.use_amp = config.use_amp and torch.cuda.is_available()\n",
    "        self.scaler = GradScaler() if self.use_amp else None\n",
    "        \n",
    "        self.history = {\n",
    "            'train_loss': [], 'val_loss': [], 'val_dice': [],\n",
    "            'val_dice_class1': [], 'val_dice_class2': [],\n",
    "            'val_iou_class1': [], 'val_iou_class2': [], 'lr': []\n",
    "        }\n",
    "        self.best_dice = 0.0\n",
    "        self.current_epoch = 0\n",
    "        \n",
    "        print(f\"  AdamW (lr={config.lr}) + Warmup({config.warmup_epochs}) + Cosine\")\n",
    "        print(f\"  æ›´å¤§æ¨¡å‹ (base={config.model_base})\")\n",
    "        print(f\"  æ›´å¼·å¢å¼· (noise + blur)\")\n",
    "        print(f\"  æ›´é•·è¨“ç·´ ({config.num_epochs} epochs)\")\n",
    "        print(f\"  Dice æ¬Šé‡: {config.loss_weights[1]:.1f}\")\n",
    "    \n",
    "    def train_epoch(self):\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        for batch_idx, (images, labels) in enumerate(self.train_loader):\n",
    "            images = images.to(self.device, non_blocking=True)\n",
    "            labels = labels.to(self.device, non_blocking=True)\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "            if self.use_amp:\n",
    "                with autocast():\n",
    "                    outputs = self.model(images)\n",
    "                    loss = self.criterion(outputs, labels, epoch=self.current_epoch)\n",
    "                \n",
    "                self.scaler.scale(loss).backward()\n",
    "                self.scaler.unscale_(self.optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=self.config.grad_clip)\n",
    "                self.scaler.step(self.optimizer)\n",
    "                self.scaler.update()\n",
    "            else:\n",
    "                outputs = self.model(images)\n",
    "                loss = self.criterion(outputs, labels, epoch=self.current_epoch)\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=self.config.grad_clip)\n",
    "                self.optimizer.step()\n",
    "            \n",
    "            if self.ema:\n",
    "                self.ema.update(self.model)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            if batch_idx % 10 == 0:\n",
    "                print(f\"    Batch {batch_idx}/{len(self.train_loader)}, Loss: {loss.item():.4f}\")\n",
    "        \n",
    "        return total_loss / len(self.train_loader)\n",
    "    \n",
    "    def validate(self, use_ema=True):\n",
    "        model = self.ema.model if (use_ema and self.ema) else self.model\n",
    "        model.eval()\n",
    "        \n",
    "        total_loss = 0\n",
    "        metrics_tracker = DatasetMetrics(self.config.num_classes, self.device)\n",
    "        \n",
    "        with torch.inference_mode():\n",
    "            for images, labels in self.val_loader:\n",
    "                images = images.to(self.device, non_blocking=True)\n",
    "                labels = labels.to(self.device, non_blocking=True)\n",
    "                \n",
    "                if self.use_amp:\n",
    "                    with autocast():\n",
    "                        outputs = model(images)\n",
    "                        if isinstance(outputs, tuple):\n",
    "                            outputs = outputs[0]\n",
    "                        loss = self.criterion(outputs, labels)\n",
    "                else:\n",
    "                    outputs = model(images)\n",
    "                    if isinstance(outputs, tuple):\n",
    "                        outputs = outputs[0]\n",
    "                    loss = self.criterion(outputs, labels)\n",
    "                \n",
    "                preds = torch.argmax(outputs, dim=1)\n",
    "                \n",
    "                for i in range(preds.shape[0]):\n",
    "                    metrics_tracker.update(preds[i], labels[i])\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "        \n",
    "        dice_scores, iou_scores = metrics_tracker.compute()\n",
    "        avg_dice = np.mean([dice_scores[1], dice_scores[2]])\n",
    "        \n",
    "        avg_metrics = {\n",
    "            1: {'dice': dice_scores[1], 'iou': iou_scores[1]},\n",
    "            2: {'dice': dice_scores[2], 'iou': iou_scores[2]}\n",
    "        }\n",
    "        \n",
    "        return total_loss / len(self.val_loader), avg_dice, avg_metrics\n",
    "    \n",
    "    def train(self):\n",
    "        print(f\"\\né–‹å§‹è¨“ç·´ {self.config.num_epochs} epochs\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        for epoch in range(1, self.config.num_epochs + 1):\n",
    "            self.current_epoch = epoch\n",
    "            \n",
    "            print(f\"\\nEpoch {epoch}/{self.config.num_epochs}\")\n",
    "            print(\"-\" * 40)\n",
    "            \n",
    "            train_loss = self.train_epoch()\n",
    "            val_loss, val_dice, metrics = self.validate(use_ema=True)\n",
    "            \n",
    "            self.scheduler.step()\n",
    "            current_lr = self.scheduler.get_lr()\n",
    "            \n",
    "            self.history['train_loss'].append(train_loss)\n",
    "            self.history['val_loss'].append(val_loss)\n",
    "            self.history['val_dice'].append(val_dice)\n",
    "            self.history['val_dice_class1'].append(metrics[1]['dice'])\n",
    "            self.history['val_dice_class2'].append(metrics[2]['dice'])\n",
    "            self.history['val_iou_class1'].append(metrics[1]['iou'])\n",
    "            self.history['val_iou_class2'].append(metrics[2]['iou'])\n",
    "            self.history['lr'].append(current_lr)\n",
    "            \n",
    "            print(f\"\\n  è¨“ç·´æå¤±: {train_loss:.4f}\")\n",
    "            print(f\"  é©—è­‰æå¤±: {val_loss:.4f}\")\n",
    "            print(f\"  é©—è­‰ Dice: {val_dice:.4f} {'ğŸ¯ æ–°ç´€éŒ„!' if val_dice > self.best_dice else ''}\")\n",
    "            print(f\"    Class 1 - Dice: {metrics[1]['dice']:.4f}, IoU: {metrics[1]['iou']:.4f}\")\n",
    "            print(f\"    Class 2 - Dice: {metrics[2]['dice']:.4f}, IoU: {metrics[2]['iou']:.4f}\")\n",
    "            print(f\"  å­¸ç¿’ç‡: {current_lr:.6f}\")\n",
    "            \n",
    "            if val_dice > self.best_dice:\n",
    "                self.best_dice = val_dice\n",
    "                checkpoint = {\n",
    "                    'epoch': epoch,\n",
    "                    'model': self.model.state_dict(),\n",
    "                    'optimizer': self.optimizer.state_dict(),\n",
    "                    'dice': val_dice,\n",
    "                    'metrics': metrics,\n",
    "                    'history': self.history,\n",
    "                    'config': self.config.__dict__\n",
    "                }\n",
    "                if self.ema:\n",
    "                    checkpoint['ema'] = self.ema.model.state_dict()\n",
    "                \n",
    "                torch.save(checkpoint, self.output_dir / 'best_model.pth')\n",
    "                print(f\"  âœ“ å·²ä¿å­˜æœ€ä½³æ¨¡å‹\")\n",
    "        \n",
    "        with open(self.output_dir / 'history.json', 'w') as f:\n",
    "            json.dump(self.history, f, indent=2)\n",
    "        \n",
    "        self.config.save(self.output_dir / 'config.json')\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(f\"è¨“ç·´å®Œæˆ! æœ€ä½³ Dice: {self.best_dice:.4f}\")\n",
    "\n",
    "# ==================== ä¸»ç¨‹åº ====================\n",
    "\n",
    "def main():\n",
    "    config = ImprovedConfig()\n",
    "    \n",
    "    set_seed(config.seed, config.use_tf32)\n",
    "    \n",
    "    print(f\"éš¨æ©Ÿç¨®å­: {config.seed}\")\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"ä½¿ç”¨è¨­å‚™: {device}\")\n",
    "    \n",
    "    print(\"\\næº–å‚™è³‡æ–™...\")\n",
    "    with open(Path(config.data_root) / 'dataset.json') as f:\n",
    "        num_samples = len(json.load(f)['training'])\n",
    "    \n",
    "    train_indices, val_indices = get_train_val_split(num_samples, seed=config.seed)\n",
    "    print(f\"  è¨“ç·´é›†: {len(train_indices)}\")\n",
    "    print(f\"  é©—è­‰é›†: {len(val_indices)}\")\n",
    "    \n",
    "    train_dataset = HippocampusDataset(config.data_root, train_indices, config, is_training=True)\n",
    "    val_dataset = HippocampusDataset(config.data_root, val_indices, config, is_training=False)\n",
    "    \n",
    "    num_workers = min(config.num_workers, os.cpu_count() or 1) if torch.cuda.is_available() else 0\n",
    "    \n",
    "    train_generator = get_generator(config.seed)\n",
    "    val_generator = get_generator(config.seed)\n",
    "    \n",
    "    train_loader_kwargs = {\n",
    "        'batch_size': config.batch_size,\n",
    "        'shuffle': True,\n",
    "        'num_workers': num_workers,\n",
    "        'pin_memory': config.pin_memory,\n",
    "        'worker_init_fn': worker_init_fn,\n",
    "        'generator': train_generator\n",
    "    }\n",
    "    if num_workers > 0:\n",
    "        train_loader_kwargs.update({\n",
    "            'persistent_workers': config.persistent_workers,\n",
    "            'prefetch_factor': config.prefetch_factor\n",
    "        })\n",
    "    \n",
    "    val_loader_kwargs = {\n",
    "        'batch_size': config.batch_size,\n",
    "        'shuffle': False,\n",
    "        'num_workers': num_workers,\n",
    "        'pin_memory': config.pin_memory,\n",
    "        'worker_init_fn': worker_init_fn,\n",
    "        'generator': val_generator\n",
    "    }\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, **train_loader_kwargs)\n",
    "    val_loader = DataLoader(val_dataset, **val_loader_kwargs)\n",
    "    \n",
    "    print(\"\\nå»ºç«‹æ¨¡å‹...\")\n",
    "    model = UNet3DDeepSupervision(1, config.num_classes, config.model_base, config.deep_supervision)\n",
    "    print(f\"  åƒæ•¸: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    \n",
    "    trainer = Trainer(model, train_loader, val_loader, device, config)\n",
    "    trainer.train()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        main()\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nè¨“ç·´ä¸­æ–·\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\néŒ¯èª¤: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cefe37de-2ee1-40c8-ac79-2a4d32f51248",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ æ‰¾åˆ°æ•¸æ“šç›®éŒ„: ./Task04_Hippocampus\n",
      "  (260 å€‹æœ‰æ•ˆæ–‡ä»¶)\n",
      "\n",
      "============================================================\n",
      "ç´”ç²¹çš„ 3D U-Net è¨“ç·´\n",
      "============================================================\n",
      "é…ç½®:\n",
      "  data_dir: ./Task04_Hippocampus\n",
      "  batch_size: 2\n",
      "  num_epochs: 30\n",
      "  lr: 0.001\n",
      "  base_channels: 16\n",
      "  target_size: 64\n",
      "  device: cuda\n",
      "\n",
      "æº–å‚™æ•¸æ“šé›†...\n",
      "æ‰¾åˆ° 260 å€‹è¨“ç·´æ¨£æœ¬\n",
      "è¨“ç·´é›†: 208 æ¨£æœ¬\n",
      "é©—è­‰é›†: 52 æ¨£æœ¬\n",
      "\n",
      "å‰µå»ºæ¨¡å‹...\n",
      "æ¨¡å‹åƒæ•¸é‡: 1,402,003 (1.40M)\n",
      "\n",
      "é–‹å§‹è¨“ç·´...\n",
      "\n",
      "Epoch 1/30\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:03<00:00, 26.90it/s, loss=0.4429]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:02<00:00,  8.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6237\n",
      "Val Dice - Class 1: 0.6355, Class 2: 0.6403, Avg: 0.6379\n",
      "Learning Rate: 0.001000\n",
      "âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹ (Dice: 0.6379) -> /workspace/outputs/pure_unet_best.pth\n",
      "\n",
      "Epoch 2/30\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:03<00:00, 29.54it/s, loss=0.2678]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:02<00:00,  9.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3207\n",
      "Val Dice - Class 1: 0.6834, Class 2: 0.7307, Avg: 0.7070\n",
      "Learning Rate: 0.001000\n",
      "âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹ (Dice: 0.7070) -> /workspace/outputs/pure_unet_best.pth\n",
      "\n",
      "Epoch 3/30\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:04<00:00, 25.00it/s, loss=0.1526]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:02<00:00,  9.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1983\n",
      "Val Dice - Class 1: 0.7137, Class 2: 0.7626, Avg: 0.7381\n",
      "Learning Rate: 0.001000\n",
      "âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹ (Dice: 0.7381) -> /workspace/outputs/pure_unet_best.pth\n",
      "\n",
      "Epoch 4/30\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:03<00:00, 29.30it/s, loss=0.1304]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:02<00:00,  9.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1533\n",
      "Val Dice - Class 1: 0.6823, Class 2: 0.7495, Avg: 0.7159\n",
      "Learning Rate: 0.001000\n",
      "\n",
      "Epoch 5/30\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:03<00:00, 32.09it/s, loss=0.1059]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:02<00:00,  9.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1299\n",
      "Val Dice - Class 1: 0.7504, Class 2: 0.7680, Avg: 0.7592\n",
      "Learning Rate: 0.001000\n",
      "âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹ (Dice: 0.7592) -> /workspace/outputs/pure_unet_best.pth\n",
      "\n",
      "Epoch 6/30\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:03<00:00, 32.12it/s, loss=0.1206]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:02<00:00,  9.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1147\n",
      "Val Dice - Class 1: 0.7780, Class 2: 0.7877, Avg: 0.7829\n",
      "Learning Rate: 0.001000\n",
      "âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹ (Dice: 0.7829) -> /workspace/outputs/pure_unet_best.pth\n",
      "\n",
      "Epoch 7/30\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:03<00:00, 31.75it/s, loss=0.0998]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:02<00:00,  8.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1109\n",
      "Val Dice - Class 1: 0.7887, Class 2: 0.8045, Avg: 0.7966\n",
      "Learning Rate: 0.001000\n",
      "âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹ (Dice: 0.7966) -> /workspace/outputs/pure_unet_best.pth\n",
      "\n",
      "Epoch 8/30\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:03<00:00, 31.78it/s, loss=0.0930]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:02<00:00,  9.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1031\n",
      "Val Dice - Class 1: 0.7821, Class 2: 0.7951, Avg: 0.7886\n",
      "Learning Rate: 0.001000\n",
      "\n",
      "Epoch 9/30\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:03<00:00, 31.49it/s, loss=0.1116]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:02<00:00,  9.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1015\n",
      "Val Dice - Class 1: 0.7980, Class 2: 0.8020, Avg: 0.8000\n",
      "Learning Rate: 0.001000\n",
      "âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹ (Dice: 0.8000) -> /workspace/outputs/pure_unet_best.pth\n",
      "\n",
      "Epoch 10/30\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:03<00:00, 31.72it/s, loss=0.0916]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:02<00:00,  9.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1018\n",
      "Val Dice - Class 1: 0.8194, Class 2: 0.8148, Avg: 0.8171\n",
      "Learning Rate: 0.000500\n",
      "âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹ (Dice: 0.8171) -> /workspace/outputs/pure_unet_best.pth\n",
      "\n",
      "Epoch 11/30\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:03<00:00, 31.60it/s, loss=0.0827]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:02<00:00,  9.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0937\n",
      "Val Dice - Class 1: 0.8369, Class 2: 0.8204, Avg: 0.8287\n",
      "Learning Rate: 0.000500\n",
      "âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹ (Dice: 0.8287) -> /workspace/outputs/pure_unet_best.pth\n",
      "\n",
      "Epoch 12/30\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:03<00:00, 31.12it/s, loss=0.0698]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:02<00:00,  9.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0871\n",
      "Val Dice - Class 1: 0.8187, Class 2: 0.8157, Avg: 0.8172\n",
      "Learning Rate: 0.000500\n",
      "\n",
      "Epoch 13/30\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:03<00:00, 31.93it/s, loss=0.0780]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:02<00:00,  9.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0887\n",
      "Val Dice - Class 1: 0.8303, Class 2: 0.8223, Avg: 0.8263\n",
      "Learning Rate: 0.000500\n",
      "\n",
      "Epoch 14/30\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:03<00:00, 31.62it/s, loss=0.0742]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:02<00:00,  9.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0866\n",
      "Val Dice - Class 1: 0.8296, Class 2: 0.8007, Avg: 0.8151\n",
      "Learning Rate: 0.000500\n",
      "\n",
      "Epoch 15/30\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:03<00:00, 31.71it/s, loss=0.0687]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:02<00:00,  9.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0854\n",
      "Val Dice - Class 1: 0.8364, Class 2: 0.8212, Avg: 0.8288\n",
      "Learning Rate: 0.000500\n",
      "âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹ (Dice: 0.8288) -> /workspace/outputs/pure_unet_best.pth\n",
      "\n",
      "Epoch 16/30\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:03<00:00, 31.87it/s, loss=0.0768]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:02<00:00,  9.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0855\n",
      "Val Dice - Class 1: 0.8348, Class 2: 0.8211, Avg: 0.8280\n",
      "Learning Rate: 0.000500\n",
      "\n",
      "Epoch 17/30\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:03<00:00, 31.78it/s, loss=0.0676]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:02<00:00,  9.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0851\n",
      "Val Dice - Class 1: 0.8375, Class 2: 0.8297, Avg: 0.8336\n",
      "Learning Rate: 0.000500\n",
      "âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹ (Dice: 0.8336) -> /workspace/outputs/pure_unet_best.pth\n",
      "\n",
      "Epoch 18/30\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:03<00:00, 32.15it/s, loss=0.0954]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:02<00:00,  9.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0846\n",
      "Val Dice - Class 1: 0.8441, Class 2: 0.8247, Avg: 0.8344\n",
      "Learning Rate: 0.000500\n",
      "âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹ (Dice: 0.8344) -> /workspace/outputs/pure_unet_best.pth\n",
      "\n",
      "Epoch 19/30\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:03<00:00, 31.88it/s, loss=0.0842]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:02<00:00, 11.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0840\n",
      "Val Dice - Class 1: 0.8414, Class 2: 0.8183, Avg: 0.8299\n",
      "Learning Rate: 0.000500\n",
      "\n",
      "Epoch 20/30\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:03<00:00, 33.07it/s, loss=0.0814]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:02<00:00,  9.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0829\n",
      "Val Dice - Class 1: 0.8421, Class 2: 0.8304, Avg: 0.8363\n",
      "Learning Rate: 0.000250\n",
      "âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹ (Dice: 0.8363) -> /workspace/outputs/pure_unet_best.pth\n",
      "\n",
      "Epoch 21/30\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:03<00:00, 32.46it/s, loss=0.0545]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:02<00:00, 10.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0822\n",
      "Val Dice - Class 1: 0.8485, Class 2: 0.8302, Avg: 0.8394\n",
      "Learning Rate: 0.000250\n",
      "âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹ (Dice: 0.8394) -> /workspace/outputs/pure_unet_best.pth\n",
      "\n",
      "Epoch 22/30\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:03<00:00, 31.95it/s, loss=0.0890]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:02<00:00,  9.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0802\n",
      "Val Dice - Class 1: 0.8500, Class 2: 0.8312, Avg: 0.8406\n",
      "Learning Rate: 0.000250\n",
      "âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹ (Dice: 0.8406) -> /workspace/outputs/pure_unet_best.pth\n",
      "\n",
      "Epoch 23/30\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:03<00:00, 32.10it/s, loss=0.0727]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:02<00:00,  9.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0788\n",
      "Val Dice - Class 1: 0.8522, Class 2: 0.8319, Avg: 0.8421\n",
      "Learning Rate: 0.000250\n",
      "âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹ (Dice: 0.8421) -> /workspace/outputs/pure_unet_best.pth\n",
      "\n",
      "Epoch 24/30\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:03<00:00, 31.80it/s, loss=0.0785]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:02<00:00, 10.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0794\n",
      "Val Dice - Class 1: 0.8461, Class 2: 0.8333, Avg: 0.8397\n",
      "Learning Rate: 0.000250\n",
      "\n",
      "Epoch 25/30\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:03<00:00, 32.27it/s, loss=0.0814]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:02<00:00,  9.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0788\n",
      "Val Dice - Class 1: 0.8473, Class 2: 0.8378, Avg: 0.8425\n",
      "Learning Rate: 0.000250\n",
      "âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹ (Dice: 0.8425) -> /workspace/outputs/pure_unet_best.pth\n",
      "\n",
      "Epoch 26/30\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:03<00:00, 31.75it/s, loss=0.0944]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:02<00:00,  9.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0800\n",
      "Val Dice - Class 1: 0.8395, Class 2: 0.8173, Avg: 0.8284\n",
      "Learning Rate: 0.000250\n",
      "\n",
      "Epoch 27/30\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:03<00:00, 31.59it/s, loss=0.0681]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:02<00:00,  8.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0790\n",
      "Val Dice - Class 1: 0.8515, Class 2: 0.8339, Avg: 0.8427\n",
      "Learning Rate: 0.000250\n",
      "âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹ (Dice: 0.8427) -> /workspace/outputs/pure_unet_best.pth\n",
      "\n",
      "Epoch 28/30\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:03<00:00, 31.63it/s, loss=0.0790]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:02<00:00,  9.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0781\n",
      "Val Dice - Class 1: 0.8521, Class 2: 0.8295, Avg: 0.8408\n",
      "Learning Rate: 0.000250\n",
      "\n",
      "Epoch 29/30\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:03<00:00, 31.60it/s, loss=0.0619]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:02<00:00,  9.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0787\n",
      "Val Dice - Class 1: 0.8498, Class 2: 0.8351, Avg: 0.8424\n",
      "Learning Rate: 0.000250\n",
      "\n",
      "Epoch 30/30\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:03<00:00, 31.48it/s, loss=0.0616]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:02<00:00,  9.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0781\n",
      "Val Dice - Class 1: 0.8457, Class 2: 0.8304, Avg: 0.8380\n",
      "Learning Rate: 0.000125\n",
      "\n",
      "============================================================\n",
      "è¨“ç·´å®Œæˆï¼\n",
      "æœ€ä½³é©—è­‰ Dice: 0.8427\n",
      "æ¨¡å‹å·²ä¿å­˜è‡³: /workspace/outputs/pure_unet_best.pth\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "ç´”ç²¹çš„ 3D U-Net å¯¦ç¾ï¼ˆä¿®æ­£ç‰ˆï¼‰\n",
    "- è‡ªå‹•æª¢æ¸¬æ•¸æ“šè·¯å¾‘\n",
    "- ç§»é™¤æ·±åº¦ç›£ç£\n",
    "- ç§»é™¤ Focal Loss\n",
    "- ç§»é™¤ EMA\n",
    "- ä½¿ç”¨æœ€åŸºç¤çš„é…ç½®\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from pathlib import Path\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# ==================== è¨­ç½®éš¨æ©Ÿç¨®å­ ====================\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# ==================== ç´”ç²¹çš„ 3D U-Net æ¨¡å‹ ====================\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    \"\"\"åŸºç¤å·ç©å¡Š\"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv3d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm3d(out_channels)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        \n",
    "        self.conv2 = nn.Conv3d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm3d(out_channels)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu1(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class PureUNet3D(nn.Module):\n",
    "    \"\"\"ç´”ç²¹çš„ 3D U-Netï¼ˆæ²’æœ‰æ·±åº¦ç›£ç£ï¼‰\"\"\"\n",
    "    def __init__(self, in_channels=1, num_classes=3, base_channels=16):\n",
    "        super().__init__()\n",
    "        \n",
    "        # ç·¨ç¢¼å™¨\n",
    "        self.enc1 = ConvBlock(in_channels, base_channels)\n",
    "        self.pool1 = nn.MaxPool3d(2)\n",
    "        \n",
    "        self.enc2 = ConvBlock(base_channels, base_channels * 2)\n",
    "        self.pool2 = nn.MaxPool3d(2)\n",
    "        \n",
    "        self.enc3 = ConvBlock(base_channels * 2, base_channels * 4)\n",
    "        self.pool3 = nn.MaxPool3d(2)\n",
    "        \n",
    "        # ç“¶é ¸å±¤\n",
    "        self.bottleneck = ConvBlock(base_channels * 4, base_channels * 8)\n",
    "        \n",
    "        # è§£ç¢¼å™¨\n",
    "        self.upconv3 = nn.ConvTranspose3d(base_channels * 8, base_channels * 4, \n",
    "                                          kernel_size=2, stride=2)\n",
    "        self.dec3 = ConvBlock(base_channels * 8, base_channels * 4)\n",
    "        \n",
    "        self.upconv2 = nn.ConvTranspose3d(base_channels * 4, base_channels * 2, \n",
    "                                          kernel_size=2, stride=2)\n",
    "        self.dec2 = ConvBlock(base_channels * 4, base_channels * 2)\n",
    "        \n",
    "        self.upconv1 = nn.ConvTranspose3d(base_channels * 2, base_channels, \n",
    "                                          kernel_size=2, stride=2)\n",
    "        self.dec1 = ConvBlock(base_channels * 2, base_channels)\n",
    "        \n",
    "        # è¼¸å‡ºå±¤\n",
    "        self.out = nn.Conv3d(base_channels, num_classes, kernel_size=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # ç·¨ç¢¼è·¯å¾‘\n",
    "        e1 = self.enc1(x)\n",
    "        e2 = self.enc2(self.pool1(e1))\n",
    "        e3 = self.enc3(self.pool2(e2))\n",
    "        \n",
    "        # ç“¶é ¸\n",
    "        b = self.bottleneck(self.pool3(e3))\n",
    "        \n",
    "        # è§£ç¢¼è·¯å¾‘\n",
    "        d3 = self.upconv3(b)\n",
    "        d3 = torch.cat([d3, e3], dim=1)\n",
    "        d3 = self.dec3(d3)\n",
    "        \n",
    "        d2 = self.upconv2(d3)\n",
    "        d2 = torch.cat([d2, e2], dim=1)\n",
    "        d2 = self.dec2(d2)\n",
    "        \n",
    "        d1 = self.upconv1(d2)\n",
    "        d1 = torch.cat([d1, e1], dim=1)\n",
    "        d1 = self.dec1(d1)\n",
    "        \n",
    "        # è¼¸å‡º\n",
    "        out = self.out(d1)\n",
    "        return out\n",
    "\n",
    "\n",
    "# ==================== Dice Lossï¼ˆç°¡åŒ–ç‰ˆï¼‰====================\n",
    "\n",
    "class DiceLoss(nn.Module):\n",
    "    \"\"\"ç°¡å–®çš„ Dice Loss\"\"\"\n",
    "    def __init__(self, smooth=1.0):\n",
    "        super().__init__()\n",
    "        self.smooth = smooth\n",
    "    \n",
    "    def forward(self, pred, target):\n",
    "        pred = F.softmax(pred, dim=1)\n",
    "        \n",
    "        # è¨ˆç®—æ¯å€‹é¡åˆ¥çš„ Dice\n",
    "        dice_scores = []\n",
    "        for c in range(pred.shape[1]):\n",
    "            pred_c = pred[:, c]\n",
    "            target_c = (target == c).float()\n",
    "            \n",
    "            intersection = (pred_c * target_c).sum()\n",
    "            union = pred_c.sum() + target_c.sum()\n",
    "            \n",
    "            dice = (2.0 * intersection + self.smooth) / (union + self.smooth)\n",
    "            dice_scores.append(dice)\n",
    "        \n",
    "        return 1.0 - torch.stack(dice_scores).mean()\n",
    "\n",
    "\n",
    "# ==================== æ•¸æ“šé›† ====================\n",
    "\n",
    "class HippocampusDataset(Dataset):\n",
    "    \"\"\"æµ·é¦¬é«”æ•¸æ“šé›†\"\"\"\n",
    "    def __init__(self, data_dir, target_size=64, is_train=True):\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.target_size = target_size\n",
    "        self.is_train = is_train\n",
    "        \n",
    "        # ç²å–æ‰€æœ‰åœ–åƒæ–‡ä»¶ï¼ˆéæ¿¾æ‰ macOS éš±è—æ–‡ä»¶ï¼‰\n",
    "        all_image_files = list((self.data_dir / 'imagesTr').glob('*.nii.gz'))\n",
    "        self.image_files = sorted([f for f in all_image_files if not f.name.startswith('._')])\n",
    "        \n",
    "        all_label_files = list((self.data_dir / 'labelsTr').glob('*.nii.gz'))\n",
    "        self.label_files = sorted([f for f in all_label_files if not f.name.startswith('._')])\n",
    "        \n",
    "        if len(self.image_files) == 0:\n",
    "            raise ValueError(f\"åœ¨ {self.data_dir / 'imagesTr'} æ‰¾ä¸åˆ°ä»»ä½• .nii.gz æ–‡ä»¶ï¼\")\n",
    "        \n",
    "        print(f\"æ‰¾åˆ° {len(self.image_files)} å€‹è¨“ç·´æ¨£æœ¬\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "    \n",
    "    def preprocess(self, image):\n",
    "        \"\"\"é è™•ç†\"\"\"\n",
    "        # Percentile clipping\n",
    "        p1, p99 = np.percentile(image, [1, 99])\n",
    "        image = np.clip(image, p1, p99)\n",
    "        \n",
    "        # Z-score normalization\n",
    "        mean, std = image.mean(), image.std()\n",
    "        image = (image - mean) / (std + 1e-8)\n",
    "        return image\n",
    "    \n",
    "    def augment(self, image, label):\n",
    "        \"\"\"ç°¡å–®çš„æ•¸æ“šå¢å¼·\"\"\"\n",
    "        # éš¨æ©Ÿç¿»è½‰\n",
    "        if random.random() > 0.5:\n",
    "            axis = random.choice([0, 1, 2])\n",
    "            image = np.flip(image, axis).copy()\n",
    "            label = np.flip(label, axis).copy()\n",
    "        \n",
    "        # éš¨æ©Ÿæ—‹è½‰ 90 åº¦\n",
    "        if random.random() > 0.5:\n",
    "            k = random.randint(1, 3)\n",
    "            axes = random.choice([(0, 1), (0, 2), (1, 2)])\n",
    "            image = np.rot90(image, k, axes).copy()\n",
    "            label = np.rot90(label, k, axes).copy()\n",
    "        \n",
    "        return image, label\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # è®€å–æ–‡ä»¶\n",
    "        image = nib.load(self.image_files[idx]).get_fdata(dtype=np.float32)\n",
    "        label = nib.load(self.label_files[idx]).get_fdata(dtype=np.float32)\n",
    "        \n",
    "        # é è™•ç†\n",
    "        image = self.preprocess(image)\n",
    "        \n",
    "        # æ•¸æ“šå¢å¼·ï¼ˆåƒ…è¨“ç·´æ™‚ï¼‰\n",
    "        if self.is_train:\n",
    "            image, label = self.augment(image, label)\n",
    "        \n",
    "        # è½‰ç‚º tensor ä¸¦èª¿æ•´å¤§å°\n",
    "        image = torch.from_numpy(image).unsqueeze(0)  # [1, H, W, D]\n",
    "        label = torch.from_numpy(label).unsqueeze(0)  # [1, H, W, D]\n",
    "        \n",
    "        image = F.interpolate(\n",
    "            image.unsqueeze(0),\n",
    "            size=(self.target_size, self.target_size, self.target_size),\n",
    "            mode='trilinear',\n",
    "            align_corners=False\n",
    "        ).squeeze(0)\n",
    "        \n",
    "        label = F.interpolate(\n",
    "            label.unsqueeze(0),\n",
    "            size=(self.target_size, self.target_size, self.target_size),\n",
    "            mode='nearest'\n",
    "        ).squeeze(0)\n",
    "        \n",
    "        label = label.squeeze(0).long()\n",
    "        label = torch.clamp(label, 0, 2)\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "\n",
    "# ==================== è©•ä¼°æŒ‡æ¨™ ====================\n",
    "\n",
    "def compute_dice(pred, target, num_classes=3):\n",
    "    \"\"\"è¨ˆç®— Dice åˆ†æ•¸\"\"\"\n",
    "    dice_scores = []\n",
    "    \n",
    "    for c in range(1, num_classes):  # è·³éèƒŒæ™¯\n",
    "        pred_c = (pred == c)\n",
    "        target_c = (target == c)\n",
    "        \n",
    "        intersection = (pred_c & target_c).sum().float()\n",
    "        union = pred_c.sum().float() + target_c.sum().float()\n",
    "        \n",
    "        if union == 0:\n",
    "            dice = 1.0 if intersection == 0 else 0.0\n",
    "        else:\n",
    "            dice = (2.0 * intersection) / union\n",
    "        \n",
    "        dice_scores.append(dice.item())\n",
    "    \n",
    "    return dice_scores\n",
    "\n",
    "\n",
    "# ==================== è¨“ç·´å‡½æ•¸ ====================\n",
    "\n",
    "def train_epoch(model, loader, criterion_ce, criterion_dice, optimizer, device):\n",
    "    \"\"\"è¨“ç·´ä¸€å€‹ epoch\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    pbar = tqdm(loader, desc='Training')\n",
    "    for images, labels in pbar:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # å‰å‘å‚³æ’­\n",
    "        outputs = model(images)\n",
    "        \n",
    "        # è¨ˆç®—æå¤±ï¼ˆç°¡å–®çš„ CE + Diceï¼‰\n",
    "        loss_ce = criterion_ce(outputs, labels)\n",
    "        loss_dice = criterion_dice(outputs, labels)\n",
    "        loss = 0.5 * loss_ce + 0.5 * loss_dice\n",
    "        \n",
    "        # åå‘å‚³æ’­\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "    \n",
    "    return total_loss / len(loader)\n",
    "\n",
    "\n",
    "def validate(model, loader, device):\n",
    "    \"\"\"é©—è­‰\"\"\"\n",
    "    model.eval()\n",
    "    all_dice_scores = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(loader, desc='Validating'):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            \n",
    "            # è¨ˆç®— Dice\n",
    "            for pred, label in zip(preds, labels):\n",
    "                dice_scores = compute_dice(pred.cpu(), label.cpu())\n",
    "                all_dice_scores.append(dice_scores)\n",
    "    \n",
    "    # å¹³å‡ Dice\n",
    "    all_dice_scores = np.array(all_dice_scores)\n",
    "    mean_dice = all_dice_scores.mean(axis=0)\n",
    "    \n",
    "    return mean_dice\n",
    "\n",
    "\n",
    "# ==================== è‡ªå‹•æª¢æ¸¬æ•¸æ“šè·¯å¾‘ ====================\n",
    "\n",
    "def find_data_directory():\n",
    "    \"\"\"è‡ªå‹•æŸ¥æ‰¾æ•¸æ“šç›®éŒ„\"\"\"\n",
    "    possible_paths = [\n",
    "        '/home/claude/Task04_Hippocampus',\n",
    "        '/workspace/data/Task04_Hippocampus',\n",
    "        './Task04_Hippocampus',\n",
    "        './data/Task04_Hippocampus',\n",
    "        '../data/Task04_Hippocampus',\n",
    "        '/data/Task04_Hippocampus',\n",
    "    ]\n",
    "    \n",
    "    for path in possible_paths:\n",
    "        if Path(path).exists():\n",
    "            images_dir = Path(path) / 'imagesTr'\n",
    "            if images_dir.exists():\n",
    "                # éæ¿¾æ‰ macOS éš±è—æ–‡ä»¶\n",
    "                image_files = [f for f in images_dir.glob('*.nii.gz') if not f.name.startswith('._')]\n",
    "                if len(image_files) > 0:\n",
    "                    print(f\"âœ“ æ‰¾åˆ°æ•¸æ“šç›®éŒ„: {path}\")\n",
    "                    print(f\"  ({len(image_files)} å€‹æœ‰æ•ˆæ–‡ä»¶)\")\n",
    "                    return path\n",
    "    \n",
    "    # å¦‚æœéƒ½æ‰¾ä¸åˆ°ï¼Œåˆ—å‡ºç•¶å‰ç›®éŒ„\n",
    "    print(\"\\nâŒ æ‰¾ä¸åˆ°æ•¸æ“šç›®éŒ„ï¼\")\n",
    "    print(\"\\nç•¶å‰ç›®éŒ„å…§å®¹:\")\n",
    "    for item in Path('.').iterdir():\n",
    "        print(f\"  - {item}\")\n",
    "    \n",
    "    print(\"\\nè«‹ç¢ºä¿æ•¸æ“šåœ¨ä»¥ä¸‹ä½ç½®ä¹‹ä¸€:\")\n",
    "    for path in possible_paths:\n",
    "        print(f\"  - {path}\")\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "# ==================== ä¸»è¨“ç·´æµç¨‹ ====================\n",
    "\n",
    "def main():\n",
    "    # è‡ªå‹•æŸ¥æ‰¾æ•¸æ“šç›®éŒ„\n",
    "    data_dir = find_data_directory()\n",
    "    \n",
    "    if data_dir is None:\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"éŒ¯èª¤ï¼šæ‰¾ä¸åˆ°æ•¸æ“šé›†ï¼\")\n",
    "        print(\"=\" * 60)\n",
    "        print(\"\\nè§£æ±ºæ–¹æ¡ˆ:\")\n",
    "        print(\"1. ä¸‹è¼‰ Hippocampus æ•¸æ“šé›†\")\n",
    "        print(\"2. è§£å£“åˆ°ä»¥ä¸‹ä»»ä¸€ä½ç½®:\")\n",
    "        print(\"   - /workspace/data/Task04_Hippocampus\")\n",
    "        print(\"   - ./data/Task04_Hippocampus\")\n",
    "        print(\"   - ./Task04_Hippocampus\")\n",
    "        print(\"\\næ•¸æ“šé›†çµæ§‹æ‡‰è©²æ˜¯:\")\n",
    "        print(\"Task04_Hippocampus/\")\n",
    "        print(\"â”œâ”€â”€ imagesTr/\")\n",
    "        print(\"â”‚   â”œâ”€â”€ hippocampus_001.nii.gz\")\n",
    "        print(\"â”‚   â”œâ”€â”€ hippocampus_002.nii.gz\")\n",
    "        print(\"â”‚   â””â”€â”€ ...\")\n",
    "        print(\"â””â”€â”€ labelsTr/\")\n",
    "        print(\"    â”œâ”€â”€ hippocampus_001.nii.gz\")\n",
    "        print(\"    â”œâ”€â”€ hippocampus_002.nii.gz\")\n",
    "        print(\"    â””â”€â”€ ...\")\n",
    "        print(\"=\" * 60)\n",
    "        return\n",
    "    \n",
    "    # é…ç½®\n",
    "    config = {\n",
    "        'data_dir': data_dir,\n",
    "        'batch_size': 2,\n",
    "        'num_epochs': 30,\n",
    "        'lr': 1e-3,\n",
    "        'base_channels': 16,\n",
    "        'target_size': 64,\n",
    "        'device': 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    }\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"ç´”ç²¹çš„ 3D U-Net è¨“ç·´\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"é…ç½®:\")\n",
    "    for key, value in config.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    print()\n",
    "    \n",
    "    device = torch.device(config['device'])\n",
    "    \n",
    "    # å‰µå»ºæ•¸æ“šé›†\n",
    "    print(\"æº–å‚™æ•¸æ“šé›†...\")\n",
    "    try:\n",
    "        train_dataset = HippocampusDataset(\n",
    "            config['data_dir'],\n",
    "            target_size=config['target_size'],\n",
    "            is_train=True\n",
    "        )\n",
    "    except ValueError as e:\n",
    "        print(f\"\\néŒ¯èª¤: {e}\")\n",
    "        return\n",
    "    \n",
    "    # ç°¡å–®çš„è¨“ç·´/é©—è­‰åŠƒåˆ† (80/20)\n",
    "    train_size = int(0.8 * len(train_dataset))\n",
    "    val_size = len(train_dataset) - train_size\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "        train_dataset, \n",
    "        [train_size, val_size],\n",
    "        generator=torch.Generator().manual_seed(42)\n",
    "    )\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=config['batch_size'],\n",
    "        shuffle=True,\n",
    "        num_workers=4,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=config['batch_size'],\n",
    "        shuffle=False,\n",
    "        num_workers=4,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    print(f\"è¨“ç·´é›†: {len(train_dataset)} æ¨£æœ¬\")\n",
    "    print(f\"é©—è­‰é›†: {len(val_dataset)} æ¨£æœ¬\")\n",
    "    print()\n",
    "    \n",
    "    # å‰µå»ºæ¨¡å‹\n",
    "    print(\"å‰µå»ºæ¨¡å‹...\")\n",
    "    model = PureUNet3D(\n",
    "        in_channels=1,\n",
    "        num_classes=3,\n",
    "        base_channels=config['base_channels']\n",
    "    ).to(device)\n",
    "    \n",
    "    # è¨ˆç®—åƒæ•¸é‡\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"æ¨¡å‹åƒæ•¸é‡: {total_params:,} ({total_params/1e6:.2f}M)\")\n",
    "    print()\n",
    "    \n",
    "    # æå¤±å‡½æ•¸å’Œå„ªåŒ–å™¨\n",
    "    criterion_ce = nn.CrossEntropyLoss()\n",
    "    criterion_dice = DiceLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'])\n",
    "    \n",
    "    # ç°¡å–®çš„å­¸ç¿’ç‡è¡°æ¸›\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "        optimizer, \n",
    "        step_size=10, \n",
    "        gamma=0.5\n",
    "    )\n",
    "    \n",
    "    # è¨“ç·´å¾ªç’°\n",
    "    print(\"é–‹å§‹è¨“ç·´...\")\n",
    "    print()\n",
    "    \n",
    "    best_dice = 0.0\n",
    "    \n",
    "    for epoch in range(config['num_epochs']):\n",
    "        print(f\"Epoch {epoch+1}/{config['num_epochs']}\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        # è¨“ç·´\n",
    "        train_loss = train_epoch(\n",
    "            model, train_loader, criterion_ce, criterion_dice, \n",
    "            optimizer, device\n",
    "        )\n",
    "        \n",
    "        # é©—è­‰\n",
    "        val_dice = validate(model, val_loader, device)\n",
    "        \n",
    "        # å­¸ç¿’ç‡è¡°æ¸›\n",
    "        scheduler.step()\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        \n",
    "        # æ‰“å°çµæœ\n",
    "        print(f\"Train Loss: {train_loss:.4f}\")\n",
    "        print(f\"Val Dice - Class 1: {val_dice[0]:.4f}, Class 2: {val_dice[1]:.4f}, \"\n",
    "              f\"Avg: {val_dice.mean():.4f}\")\n",
    "        print(f\"Learning Rate: {current_lr:.6f}\")\n",
    "        \n",
    "        # ä¿å­˜æœ€ä½³æ¨¡å‹\n",
    "        if val_dice.mean() > best_dice:\n",
    "            best_dice = val_dice.mean()\n",
    "            \n",
    "            # ç¢ºä¿è¼¸å‡ºç›®éŒ„å­˜åœ¨\n",
    "            output_dir = Path('/workspace/outputs')\n",
    "            if not output_dir.exists():\n",
    "                output_dir = Path('./outputs')\n",
    "                output_dir.mkdir(exist_ok=True)\n",
    "            \n",
    "            save_path = output_dir / 'pure_unet_best.pth'\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'dice': val_dice,\n",
    "                'config': config\n",
    "            }, save_path)\n",
    "            print(f\"âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹ (Dice: {best_dice:.4f}) -> {save_path}\")\n",
    "        \n",
    "        print()\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"è¨“ç·´å®Œæˆï¼\")\n",
    "    print(f\"æœ€ä½³é©—è­‰ Dice: {best_dice:.4f}\")\n",
    "    print(f\"æ¨¡å‹å·²ä¿å­˜è‡³: {save_path}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "57181df4-e879-4de9-9f41-aa2ee58299d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ æ‰¾åˆ°æ•¸æ“šç›®éŒ„: ./Task04_Hippocampus\n",
      "  (260 å€‹æœ‰æ•ˆæ–‡ä»¶)\n",
      "\n",
      "============================================================\n",
      "å®Œæ•´çš„ nnU-Net è¨“ç·´\n",
      "============================================================\n",
      "é…ç½®:\n",
      "  data_dir: ./Task04_Hippocampus\n",
      "  batch_size: 2\n",
      "  num_epochs: 100\n",
      "  initial_lr: 0.01\n",
      "  base_channels: 32\n",
      "  num_pool: 3\n",
      "  target_size: 64\n",
      "  deep_supervision: True\n",
      "  use_augmentation: True\n",
      "  device: cuda\n",
      "\n",
      "SciPy å¯ç”¨: True\n",
      "\n",
      "æº–å‚™æ•¸æ“šé›†...\n",
      "æ‰¾åˆ° 260 å€‹è¨“ç·´æ¨£æœ¬\n",
      "è¨“ç·´é›†: 208 æ¨£æœ¬\n",
      "é©—è­‰é›†: 52 æ¨£æœ¬\n",
      "\n",
      "å‰µå»º nnU-Net æ¨¡å‹...\n",
      "æ¨¡å‹åƒæ•¸é‡: 6,271,980 (6.27M)\n",
      "\n",
      "é–‹å§‹è¨“ç·´...\n",
      "ä½¿ç”¨æ·±åº¦ç›£ç£: True\n",
      "ä½¿ç”¨æ•¸æ“šå¢å¼·: True\n",
      "\n",
      "Epoch 1/100\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:08<00:00, 12.32it/s, loss=0.4146]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:02<00:00,  8.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6857\n",
      "Val Dice - Class 1: 0.6209, Class 2: 0.6890, Avg: 0.6549\n",
      "Learning Rate: 0.009910\n",
      "âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹ (Dice: 0.6549) -> /workspace/outputs/nnunet_best.pth\n",
      "\n",
      "Epoch 2/100\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:08<00:00, 12.10it/s, loss=0.2751]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  8.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3103\n",
      "Val Dice - Class 1: 0.7304, Class 2: 0.7598, Avg: 0.7451\n",
      "Learning Rate: 0.009820\n",
      "âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹ (Dice: 0.7451) -> /workspace/outputs/nnunet_best.pth\n",
      "\n",
      "Epoch 3/100\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:08<00:00, 12.04it/s, loss=0.2592]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  8.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2503\n",
      "Val Dice - Class 1: 0.7698, Class 2: 0.7791, Avg: 0.7745\n",
      "Learning Rate: 0.009730\n",
      "âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹ (Dice: 0.7745) -> /workspace/outputs/nnunet_best.pth\n",
      "\n",
      "Epoch 4/100\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:08<00:00, 12.36it/s, loss=0.2159]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:02<00:00,  9.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2270\n",
      "Val Dice - Class 1: 0.7947, Class 2: 0.7938, Avg: 0.7942\n",
      "Learning Rate: 0.009639\n",
      "âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹ (Dice: 0.7942) -> /workspace/outputs/nnunet_best.pth\n",
      "\n",
      "Epoch 5/100\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:08<00:00, 12.80it/s, loss=0.1724]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:02<00:00, 10.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2143\n",
      "Val Dice - Class 1: 0.7947, Class 2: 0.7970, Avg: 0.7959\n",
      "Learning Rate: 0.009549\n",
      "âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹ (Dice: 0.7959) -> /workspace/outputs/nnunet_best.pth\n",
      "\n",
      "Epoch 6/100\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:08<00:00, 11.98it/s, loss=0.2008]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  8.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2040\n",
      "Val Dice - Class 1: 0.8088, Class 2: 0.7982, Avg: 0.8035\n",
      "Learning Rate: 0.009458\n",
      "âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹ (Dice: 0.8035) -> /workspace/outputs/nnunet_best.pth\n",
      "\n",
      "Epoch 7/100\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:08<00:00, 12.26it/s, loss=0.1725]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  8.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1973\n",
      "Val Dice - Class 1: 0.8149, Class 2: 0.8051, Avg: 0.8100\n",
      "Learning Rate: 0.009368\n",
      "âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹ (Dice: 0.8100) -> /workspace/outputs/nnunet_best.pth\n",
      "\n",
      "Epoch 8/100\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:08<00:00, 12.00it/s, loss=0.2426]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  7.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1978\n",
      "Val Dice - Class 1: 0.8212, Class 2: 0.8132, Avg: 0.8172\n",
      "Learning Rate: 0.009277\n",
      "âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹ (Dice: 0.8172) -> /workspace/outputs/nnunet_best.pth\n",
      "\n",
      "Epoch 9/100\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:08<00:00, 12.02it/s, loss=0.1516]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  8.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1893\n",
      "Val Dice - Class 1: 0.8189, Class 2: 0.8140, Avg: 0.8165\n",
      "Learning Rate: 0.009186\n",
      "\n",
      "Epoch 10/100\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:08<00:00, 12.05it/s, loss=0.1726]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  8.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1846\n",
      "Val Dice - Class 1: 0.8244, Class 2: 0.8150, Avg: 0.8197\n",
      "Learning Rate: 0.009095\n",
      "âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹ (Dice: 0.8197) -> /workspace/outputs/nnunet_best.pth\n",
      "\n",
      "Epoch 11/100\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:08<00:00, 11.96it/s, loss=0.1821]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  8.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1814\n",
      "Val Dice - Class 1: 0.8288, Class 2: 0.8192, Avg: 0.8240\n",
      "Learning Rate: 0.009004\n",
      "âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹ (Dice: 0.8240) -> /workspace/outputs/nnunet_best.pth\n",
      "\n",
      "Epoch 12/100\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:08<00:00, 12.16it/s, loss=0.2224]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  8.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1811\n",
      "Val Dice - Class 1: 0.8264, Class 2: 0.8224, Avg: 0.8244\n",
      "Learning Rate: 0.008913\n",
      "âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹ (Dice: 0.8244) -> /workspace/outputs/nnunet_best.pth\n",
      "\n",
      "Epoch 13/100\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:08<00:00, 12.04it/s, loss=0.1965]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  8.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1771\n",
      "Val Dice - Class 1: 0.8279, Class 2: 0.8143, Avg: 0.8211\n",
      "Learning Rate: 0.008822\n",
      "\n",
      "Epoch 14/100\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:08<00:00, 11.92it/s, loss=0.2030]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  8.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1742\n",
      "Val Dice - Class 1: 0.8345, Class 2: 0.8236, Avg: 0.8291\n",
      "Learning Rate: 0.008731\n",
      "âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹ (Dice: 0.8291) -> /workspace/outputs/nnunet_best.pth\n",
      "\n",
      "Epoch 15/100\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:08<00:00, 12.75it/s, loss=0.2407]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  8.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1748\n",
      "Val Dice - Class 1: 0.8323, Class 2: 0.8258, Avg: 0.8291\n",
      "Learning Rate: 0.008639\n",
      "âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹ (Dice: 0.8291) -> /workspace/outputs/nnunet_best.pth\n",
      "\n",
      "Epoch 16/100\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:08<00:00, 11.77it/s, loss=0.1472]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  8.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1735\n",
      "Val Dice - Class 1: 0.8361, Class 2: 0.8200, Avg: 0.8280\n",
      "Learning Rate: 0.008548\n",
      "\n",
      "Epoch 17/100\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:08<00:00, 12.26it/s, loss=0.1576]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  8.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1712\n",
      "Val Dice - Class 1: 0.8350, Class 2: 0.8208, Avg: 0.8279\n",
      "Learning Rate: 0.008456\n",
      "\n",
      "Epoch 18/100\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:08<00:00, 12.35it/s, loss=0.1474]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  8.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1716\n",
      "Val Dice - Class 1: 0.8368, Class 2: 0.8222, Avg: 0.8295\n",
      "Learning Rate: 0.008364\n",
      "âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹ (Dice: 0.8295) -> /workspace/outputs/nnunet_best.pth\n",
      "\n",
      "Epoch 19/100\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:08<00:00, 12.08it/s, loss=0.1765]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  8.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1670\n",
      "Val Dice - Class 1: 0.8444, Class 2: 0.8275, Avg: 0.8360\n",
      "Learning Rate: 0.008272\n",
      "âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹ (Dice: 0.8360) -> /workspace/outputs/nnunet_best.pth\n",
      "\n",
      "Epoch 20/100\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:08<00:00, 12.15it/s, loss=0.1579]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  8.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1681\n",
      "Val Dice - Class 1: 0.8478, Class 2: 0.8264, Avg: 0.8371\n",
      "Learning Rate: 0.008181\n",
      "âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹ (Dice: 0.8371) -> /workspace/outputs/nnunet_best.pth\n",
      "\n",
      "Epoch 21/100\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:08<00:00, 11.94it/s, loss=0.1647]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  8.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1652\n",
      "Val Dice - Class 1: 0.8454, Class 2: 0.8291, Avg: 0.8373\n",
      "Learning Rate: 0.008088\n",
      "âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹ (Dice: 0.8373) -> /workspace/outputs/nnunet_best.pth\n",
      "\n",
      "Epoch 22/100\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:08<00:00, 12.24it/s, loss=0.1386]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  8.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1638\n",
      "Val Dice - Class 1: 0.8425, Class 2: 0.8311, Avg: 0.8368\n",
      "Learning Rate: 0.007996\n",
      "\n",
      "Epoch 23/100\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:08<00:00, 12.26it/s, loss=0.1782]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  8.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1660\n",
      "Val Dice - Class 1: 0.8401, Class 2: 0.8116, Avg: 0.8258\n",
      "Learning Rate: 0.007904\n",
      "\n",
      "Epoch 24/100\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:08<00:00, 12.07it/s, loss=0.1542]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:02<00:00, 10.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1645\n",
      "Val Dice - Class 1: 0.8443, Class 2: 0.8306, Avg: 0.8375\n",
      "Learning Rate: 0.007811\n",
      "âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹ (Dice: 0.8375) -> /workspace/outputs/nnunet_best.pth\n",
      "\n",
      "Epoch 25/100\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:08<00:00, 12.00it/s, loss=0.1252]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  8.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1614\n",
      "Val Dice - Class 1: 0.8459, Class 2: 0.8310, Avg: 0.8385\n",
      "Learning Rate: 0.007719\n",
      "âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹ (Dice: 0.8385) -> /workspace/outputs/nnunet_best.pth\n",
      "\n",
      "Epoch 26/100\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:08<00:00, 11.94it/s, loss=0.1834]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  8.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1612\n",
      "Val Dice - Class 1: 0.8481, Class 2: 0.8314, Avg: 0.8398\n",
      "Learning Rate: 0.007626\n",
      "âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹ (Dice: 0.8398) -> /workspace/outputs/nnunet_best.pth\n",
      "\n",
      "Epoch 27/100\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:08<00:00, 12.14it/s, loss=0.1442]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:02<00:00,  8.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1603\n",
      "Val Dice - Class 1: 0.8492, Class 2: 0.8307, Avg: 0.8400\n",
      "Learning Rate: 0.007533\n",
      "âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹ (Dice: 0.8400) -> /workspace/outputs/nnunet_best.pth\n",
      "\n",
      "Epoch 28/100\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:08<00:00, 12.29it/s, loss=0.1483]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  8.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1601\n",
      "Val Dice - Class 1: 0.8448, Class 2: 0.8294, Avg: 0.8371\n",
      "Learning Rate: 0.007440\n",
      "\n",
      "Epoch 29/100\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:08<00:00, 12.79it/s, loss=0.2072]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  8.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1596\n",
      "Val Dice - Class 1: 0.8522, Class 2: 0.8333, Avg: 0.8428\n",
      "Learning Rate: 0.007347\n",
      "âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹ (Dice: 0.8428) -> /workspace/outputs/nnunet_best.pth\n",
      "\n",
      "Epoch 30/100\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:08<00:00, 12.06it/s, loss=0.1859]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:02<00:00,  8.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1580\n",
      "Val Dice - Class 1: 0.8488, Class 2: 0.8340, Avg: 0.8414\n",
      "Learning Rate: 0.007254\n",
      "\n",
      "Epoch 31/100\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:08<00:00, 12.00it/s, loss=0.1590]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  8.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1580\n",
      "Val Dice - Class 1: 0.8451, Class 2: 0.8334, Avg: 0.8392\n",
      "Learning Rate: 0.007161\n",
      "\n",
      "Epoch 32/100\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:08<00:00, 11.96it/s, loss=0.1449]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  8.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1546\n",
      "Val Dice - Class 1: 0.8488, Class 2: 0.8361, Avg: 0.8425\n",
      "Learning Rate: 0.007067\n",
      "\n",
      "Epoch 33/100\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:08<00:00, 12.09it/s, loss=0.1603]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  8.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1556\n",
      "Val Dice - Class 1: 0.8513, Class 2: 0.8245, Avg: 0.8379\n",
      "Learning Rate: 0.006974\n",
      "\n",
      "Epoch 34/100\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:08<00:00, 12.87it/s, loss=0.1641]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  8.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1559\n",
      "Val Dice - Class 1: 0.8461, Class 2: 0.8340, Avg: 0.8401\n",
      "Learning Rate: 0.006880\n",
      "\n",
      "Epoch 35/100\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:08<00:00, 12.07it/s, loss=0.1296]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  8.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1549\n",
      "Val Dice - Class 1: 0.8479, Class 2: 0.8298, Avg: 0.8389\n",
      "Learning Rate: 0.006786\n",
      "\n",
      "Epoch 36/100\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:08<00:00, 12.03it/s, loss=0.2202]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  7.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1539\n",
      "Val Dice - Class 1: 0.8507, Class 2: 0.8348, Avg: 0.8428\n",
      "Learning Rate: 0.006692\n",
      "\n",
      "Epoch 37/100\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:08<00:00, 12.25it/s, loss=0.1766]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  8.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1522\n",
      "Val Dice - Class 1: 0.8551, Class 2: 0.8332, Avg: 0.8442\n",
      "Learning Rate: 0.006598\n",
      "âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹ (Dice: 0.8442) -> /workspace/outputs/nnunet_best.pth\n",
      "\n",
      "Epoch 38/100\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:08<00:00, 11.98it/s, loss=0.1441]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  8.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1537\n",
      "Val Dice - Class 1: 0.8523, Class 2: 0.8354, Avg: 0.8438\n",
      "Learning Rate: 0.006504\n",
      "\n",
      "Epoch 39/100\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:08<00:00, 12.20it/s, loss=0.1755]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  8.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1539\n",
      "Val Dice - Class 1: 0.8567, Class 2: 0.8417, Avg: 0.8492\n",
      "Learning Rate: 0.006409\n",
      "âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹ (Dice: 0.8492) -> /workspace/outputs/nnunet_best.pth\n",
      "\n",
      "Epoch 40/100\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:08<00:00, 12.18it/s, loss=0.1352]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  8.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1536\n",
      "Val Dice - Class 1: 0.8543, Class 2: 0.8372, Avg: 0.8457\n",
      "Learning Rate: 0.006314\n",
      "\n",
      "Epoch 41/100\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:08<00:00, 12.82it/s, loss=0.1327]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  8.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1513\n",
      "Val Dice - Class 1: 0.8543, Class 2: 0.8347, Avg: 0.8445\n",
      "Learning Rate: 0.006220\n",
      "\n",
      "Epoch 42/100\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:08<00:00, 12.32it/s, loss=0.1674]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:02<00:00,  8.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1514\n",
      "Val Dice - Class 1: 0.8550, Class 2: 0.8339, Avg: 0.8444\n",
      "Learning Rate: 0.006125\n",
      "\n",
      "Epoch 43/100\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:08<00:00, 12.43it/s, loss=0.1328]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:02<00:00, 11.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1482\n",
      "Val Dice - Class 1: 0.8548, Class 2: 0.8403, Avg: 0.8475\n",
      "Learning Rate: 0.006030\n",
      "\n",
      "Epoch 44/100\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:08<00:00, 12.85it/s, loss=0.1863]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:02<00:00,  8.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1483\n",
      "Val Dice - Class 1: 0.8525, Class 2: 0.8332, Avg: 0.8429\n",
      "Learning Rate: 0.005934\n",
      "\n",
      "Epoch 45/100\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:08<00:00, 12.22it/s, loss=0.1644]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  8.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1530\n",
      "Val Dice - Class 1: 0.8588, Class 2: 0.8406, Avg: 0.8497\n",
      "Learning Rate: 0.005839\n",
      "âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹ (Dice: 0.8497) -> /workspace/outputs/nnunet_best.pth\n",
      "\n",
      "Epoch 46/100\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:08<00:00, 12.24it/s, loss=0.1548]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  8.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1492\n",
      "Val Dice - Class 1: 0.8523, Class 2: 0.8349, Avg: 0.8436\n",
      "Learning Rate: 0.005743\n",
      "\n",
      "Epoch 47/100\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:08<00:00, 12.38it/s, loss=0.1297]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  8.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1490\n",
      "Val Dice - Class 1: 0.8564, Class 2: 0.8418, Avg: 0.8491\n",
      "Learning Rate: 0.005647\n",
      "\n",
      "Epoch 48/100\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:08<00:00, 12.35it/s, loss=0.1354]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:02<00:00,  8.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1483\n",
      "Val Dice - Class 1: 0.8607, Class 2: 0.8427, Avg: 0.8517\n",
      "Learning Rate: 0.005551\n",
      "âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹ (Dice: 0.8517) -> /workspace/outputs/nnunet_best.pth\n",
      "\n",
      "Epoch 49/100\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:08<00:00, 12.44it/s, loss=0.1453]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  8.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1478\n",
      "Val Dice - Class 1: 0.8569, Class 2: 0.8390, Avg: 0.8480\n",
      "Learning Rate: 0.005455\n",
      "\n",
      "Epoch 50/100\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:08<00:00, 12.57it/s, loss=0.1586]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:02<00:00,  8.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1470\n",
      "Val Dice - Class 1: 0.8583, Class 2: 0.8379, Avg: 0.8481\n",
      "Learning Rate: 0.005359\n",
      "\n",
      "Epoch 51/100\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:08<00:00, 12.53it/s, loss=0.1603]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  8.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1463\n",
      "Val Dice - Class 1: 0.8585, Class 2: 0.8399, Avg: 0.8492\n",
      "Learning Rate: 0.005262\n",
      "\n",
      "Epoch 52/100\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:08<00:00, 12.33it/s, loss=0.1127]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  8.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1447\n",
      "Val Dice - Class 1: 0.8538, Class 2: 0.8376, Avg: 0.8457\n",
      "Learning Rate: 0.005166\n",
      "\n",
      "Epoch 53/100\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:08<00:00, 12.99it/s, loss=0.1734]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:02<00:00,  9.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1477\n",
      "Val Dice - Class 1: 0.8584, Class 2: 0.8367, Avg: 0.8476\n",
      "Learning Rate: 0.005069\n",
      "\n",
      "Epoch 54/100\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:08<00:00, 12.32it/s, loss=0.1253]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:02<00:00,  8.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1459\n",
      "Val Dice - Class 1: 0.8600, Class 2: 0.8421, Avg: 0.8511\n",
      "Learning Rate: 0.004971\n",
      "\n",
      "Epoch 55/100\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:08<00:00, 12.35it/s, loss=0.1218]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  8.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1436\n",
      "Val Dice - Class 1: 0.8606, Class 2: 0.8416, Avg: 0.8511\n",
      "Learning Rate: 0.004874\n",
      "\n",
      "Epoch 56/100\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:08<00:00, 12.42it/s, loss=0.1137]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  8.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1451\n",
      "Val Dice - Class 1: 0.8624, Class 2: 0.8420, Avg: 0.8522\n",
      "Learning Rate: 0.004776\n",
      "âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹ (Dice: 0.8522) -> /workspace/outputs/nnunet_best.pth\n",
      "\n",
      "Epoch 57/100\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:08<00:00, 12.56it/s, loss=0.1239]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  8.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1435\n",
      "Val Dice - Class 1: 0.8620, Class 2: 0.8462, Avg: 0.8541\n",
      "Learning Rate: 0.004679\n",
      "âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹ (Dice: 0.8541) -> /workspace/outputs/nnunet_best.pth\n",
      "\n",
      "Epoch 58/100\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:08<00:00, 12.44it/s, loss=0.1559]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:02<00:00,  9.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1428\n",
      "Val Dice - Class 1: 0.8613, Class 2: 0.8456, Avg: 0.8534\n",
      "Learning Rate: 0.004581\n",
      "\n",
      "Epoch 59/100\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:08<00:00, 12.52it/s, loss=0.1973]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  8.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1412\n",
      "Val Dice - Class 1: 0.8644, Class 2: 0.8429, Avg: 0.8537\n",
      "Learning Rate: 0.004482\n",
      "\n",
      "Epoch 60/100\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:08<00:00, 12.39it/s, loss=0.1234]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  8.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1415\n",
      "Val Dice - Class 1: 0.8626, Class 2: 0.8445, Avg: 0.8536\n",
      "Learning Rate: 0.004384\n",
      "\n",
      "Epoch 61/100\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:08<00:00, 12.23it/s, loss=0.1289]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  8.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1424\n",
      "Val Dice - Class 1: 0.8638, Class 2: 0.8433, Avg: 0.8536\n",
      "Learning Rate: 0.004285\n",
      "\n",
      "Epoch 62/100\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:08<00:00, 12.59it/s, loss=0.1293]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  8.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1417\n",
      "Val Dice - Class 1: 0.8601, Class 2: 0.8406, Avg: 0.8503\n",
      "Learning Rate: 0.004186\n",
      "\n",
      "Epoch 63/100\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:07<00:00, 13.11it/s, loss=0.1834]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  8.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1413\n",
      "Val Dice - Class 1: 0.8624, Class 2: 0.8444, Avg: 0.8534\n",
      "Learning Rate: 0.004087\n",
      "\n",
      "Epoch 64/100\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:08<00:00, 12.51it/s, loss=0.2080]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  8.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1429\n",
      "Val Dice - Class 1: 0.8612, Class 2: 0.8396, Avg: 0.8504\n",
      "Learning Rate: 0.003987\n",
      "\n",
      "Epoch 65/100\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:08<00:00, 12.43it/s, loss=0.1526]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  8.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1417\n",
      "Val Dice - Class 1: 0.8628, Class 2: 0.8398, Avg: 0.8513\n",
      "Learning Rate: 0.003887\n",
      "\n",
      "Epoch 66/100\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:08<00:00, 12.21it/s, loss=0.1415]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  8.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1399\n",
      "Val Dice - Class 1: 0.8626, Class 2: 0.8468, Avg: 0.8547\n",
      "Learning Rate: 0.003787\n",
      "âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹ (Dice: 0.8547) -> /workspace/outputs/nnunet_best.pth\n",
      "\n",
      "Epoch 67/100\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:08<00:00, 12.23it/s, loss=0.1230]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  8.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1408\n",
      "Val Dice - Class 1: 0.8664, Class 2: 0.8456, Avg: 0.8560\n",
      "Learning Rate: 0.003687\n",
      "âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹ (Dice: 0.8560) -> /workspace/outputs/nnunet_best.pth\n",
      "\n",
      "Epoch 68/100\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:08<00:00, 12.07it/s, loss=0.1630]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  8.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1390\n",
      "Val Dice - Class 1: 0.8612, Class 2: 0.8405, Avg: 0.8509\n",
      "Learning Rate: 0.003586\n",
      "\n",
      "Epoch 69/100\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:08<00:00, 12.03it/s, loss=0.1194]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:02<00:00,  8.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1395\n",
      "Val Dice - Class 1: 0.8620, Class 2: 0.8405, Avg: 0.8513\n",
      "Learning Rate: 0.003485\n",
      "\n",
      "Epoch 70/100\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:08<00:00, 12.48it/s, loss=0.1557]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  8.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1381\n",
      "Val Dice - Class 1: 0.8627, Class 2: 0.8463, Avg: 0.8545\n",
      "Learning Rate: 0.003384\n",
      "\n",
      "Epoch 71/100\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:08<00:00, 12.42it/s, loss=0.1368]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  8.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1368\n",
      "Val Dice - Class 1: 0.8638, Class 2: 0.8444, Avg: 0.8541\n",
      "Learning Rate: 0.003282\n",
      "\n",
      "Epoch 72/100\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:08<00:00, 12.18it/s, loss=0.1579]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:02<00:00, 10.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1357\n",
      "Val Dice - Class 1: 0.8651, Class 2: 0.8460, Avg: 0.8555\n",
      "Learning Rate: 0.003180\n",
      "\n",
      "Epoch 73/100\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:07<00:00, 13.03it/s, loss=0.1581]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:02<00:00,  8.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1381\n",
      "Val Dice - Class 1: 0.8636, Class 2: 0.8458, Avg: 0.8547\n",
      "Learning Rate: 0.003078\n",
      "\n",
      "Epoch 74/100\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:08<00:00, 12.19it/s, loss=0.1365]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  8.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1360\n",
      "Val Dice - Class 1: 0.8617, Class 2: 0.8452, Avg: 0.8534\n",
      "Learning Rate: 0.002975\n",
      "\n",
      "Epoch 75/100\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:08<00:00, 12.46it/s, loss=0.1247]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  8.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1350\n",
      "Val Dice - Class 1: 0.8646, Class 2: 0.8478, Avg: 0.8562\n",
      "Learning Rate: 0.002872\n",
      "âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹ (Dice: 0.8562) -> /workspace/outputs/nnunet_best.pth\n",
      "\n",
      "Epoch 76/100\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:08<00:00, 12.42it/s, loss=0.1273]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  8.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1361\n",
      "Val Dice - Class 1: 0.8615, Class 2: 0.8451, Avg: 0.8533\n",
      "Learning Rate: 0.002768\n",
      "\n",
      "Epoch 77/100\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:08<00:00, 12.17it/s, loss=0.1342]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  8.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1347\n",
      "Val Dice - Class 1: 0.8652, Class 2: 0.8408, Avg: 0.8530\n",
      "Learning Rate: 0.002664\n",
      "\n",
      "Epoch 78/100\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:08<00:00, 12.46it/s, loss=0.1157]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:02<00:00,  9.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1357\n",
      "Val Dice - Class 1: 0.8638, Class 2: 0.8475, Avg: 0.8556\n",
      "Learning Rate: 0.002560\n",
      "\n",
      "Epoch 79/100\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:08<00:00, 12.75it/s, loss=0.1491]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:02<00:00,  8.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1337\n",
      "Val Dice - Class 1: 0.8668, Class 2: 0.8490, Avg: 0.8579\n",
      "Learning Rate: 0.002455\n",
      "âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹ (Dice: 0.8579) -> /workspace/outputs/nnunet_best.pth\n",
      "\n",
      "Epoch 80/100\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:08<00:00, 12.32it/s, loss=0.1278]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  8.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1342\n",
      "Val Dice - Class 1: 0.8679, Class 2: 0.8488, Avg: 0.8584\n",
      "Learning Rate: 0.002349\n",
      "âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹ (Dice: 0.8584) -> /workspace/outputs/nnunet_best.pth\n",
      "\n",
      "Epoch 81/100\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:08<00:00, 12.13it/s, loss=0.1451]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  8.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1341\n",
      "Val Dice - Class 1: 0.8659, Class 2: 0.8472, Avg: 0.8565\n",
      "Learning Rate: 0.002243\n",
      "\n",
      "Epoch 82/100\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:08<00:00, 12.79it/s, loss=0.1194]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  8.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1325\n",
      "Val Dice - Class 1: 0.8651, Class 2: 0.8432, Avg: 0.8541\n",
      "Learning Rate: 0.002137\n",
      "\n",
      "Epoch 83/100\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:08<00:00, 11.99it/s, loss=0.1404]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  8.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1324\n",
      "Val Dice - Class 1: 0.8659, Class 2: 0.8489, Avg: 0.8574\n",
      "Learning Rate: 0.002030\n",
      "\n",
      "Epoch 84/100\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:08<00:00, 12.73it/s, loss=0.1334]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  8.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1311\n",
      "Val Dice - Class 1: 0.8688, Class 2: 0.8500, Avg: 0.8594\n",
      "Learning Rate: 0.001922\n",
      "âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹ (Dice: 0.8594) -> /workspace/outputs/nnunet_best.pth\n",
      "\n",
      "Epoch 85/100\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:08<00:00, 12.40it/s, loss=0.1167]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  8.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1309\n",
      "Val Dice - Class 1: 0.8692, Class 2: 0.8497, Avg: 0.8595\n",
      "Learning Rate: 0.001813\n",
      "âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹ (Dice: 0.8595) -> /workspace/outputs/nnunet_best.pth\n",
      "\n",
      "Epoch 86/100\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:08<00:00, 12.28it/s, loss=0.1140]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  8.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1288\n",
      "Val Dice - Class 1: 0.8654, Class 2: 0.8506, Avg: 0.8580\n",
      "Learning Rate: 0.001704\n",
      "\n",
      "Epoch 87/100\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:08<00:00, 12.41it/s, loss=0.1443]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  8.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1308\n",
      "Val Dice - Class 1: 0.8685, Class 2: 0.8506, Avg: 0.8595\n",
      "Learning Rate: 0.001594\n",
      "âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹ (Dice: 0.8595) -> /workspace/outputs/nnunet_best.pth\n",
      "\n",
      "Epoch 88/100\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:08<00:00, 12.09it/s, loss=0.1125]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  8.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1293\n",
      "Val Dice - Class 1: 0.8678, Class 2: 0.8486, Avg: 0.8582\n",
      "Learning Rate: 0.001483\n",
      "\n",
      "Epoch 89/100\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:08<00:00, 12.38it/s, loss=0.1265]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  8.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1303\n",
      "Val Dice - Class 1: 0.8683, Class 2: 0.8482, Avg: 0.8583\n",
      "Learning Rate: 0.001372\n",
      "\n",
      "Epoch 90/100\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:08<00:00, 12.46it/s, loss=0.1469]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  8.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1293\n",
      "Val Dice - Class 1: 0.8707, Class 2: 0.8496, Avg: 0.8601\n",
      "Learning Rate: 0.001259\n",
      "âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹ (Dice: 0.8601) -> /workspace/outputs/nnunet_best.pth\n",
      "\n",
      "Epoch 91/100\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:08<00:00, 12.15it/s, loss=0.1358]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:02<00:00, 11.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1276\n",
      "Val Dice - Class 1: 0.8684, Class 2: 0.8505, Avg: 0.8594\n",
      "Learning Rate: 0.001145\n",
      "\n",
      "Epoch 92/100\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:08<00:00, 12.81it/s, loss=0.1364]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  8.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1280\n",
      "Val Dice - Class 1: 0.8722, Class 2: 0.8537, Avg: 0.8629\n",
      "Learning Rate: 0.001030\n",
      "âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹ (Dice: 0.8629) -> /workspace/outputs/nnunet_best.pth\n",
      "\n",
      "Epoch 93/100\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:08<00:00, 12.07it/s, loss=0.1418]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  8.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1254\n",
      "Val Dice - Class 1: 0.8697, Class 2: 0.8510, Avg: 0.8603\n",
      "Learning Rate: 0.000913\n",
      "\n",
      "Epoch 94/100\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:08<00:00, 12.11it/s, loss=0.1217]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  8.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1270\n",
      "Val Dice - Class 1: 0.8708, Class 2: 0.8527, Avg: 0.8617\n",
      "Learning Rate: 0.000795\n",
      "\n",
      "Epoch 95/100\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:08<00:00, 12.17it/s, loss=0.1147]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  8.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1270\n",
      "Val Dice - Class 1: 0.8697, Class 2: 0.8514, Avg: 0.8606\n",
      "Learning Rate: 0.000675\n",
      "\n",
      "Epoch 96/100\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:08<00:00, 11.97it/s, loss=0.1824]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  8.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1283\n",
      "Val Dice - Class 1: 0.8695, Class 2: 0.8525, Avg: 0.8610\n",
      "Learning Rate: 0.000552\n",
      "\n",
      "Epoch 97/100\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:08<00:00, 12.23it/s, loss=0.1218]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  8.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1248\n",
      "Val Dice - Class 1: 0.8723, Class 2: 0.8544, Avg: 0.8634\n",
      "Learning Rate: 0.000426\n",
      "âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹ (Dice: 0.8634) -> /workspace/outputs/nnunet_best.pth\n",
      "\n",
      "Epoch 98/100\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:08<00:00, 11.96it/s, loss=0.1215]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  8.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1254\n",
      "Val Dice - Class 1: 0.8722, Class 2: 0.8529, Avg: 0.8626\n",
      "Learning Rate: 0.000296\n",
      "\n",
      "Epoch 99/100\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:08<00:00, 12.22it/s, loss=0.1237]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  8.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1267\n",
      "Val Dice - Class 1: 0.8719, Class 2: 0.8530, Avg: 0.8625\n",
      "Learning Rate: 0.000158\n",
      "\n",
      "Epoch 100/100\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:08<00:00, 12.13it/s, loss=0.1153]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  8.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1253\n",
      "Val Dice - Class 1: 0.8691, Class 2: 0.8517, Avg: 0.8604\n",
      "Learning Rate: 0.000000\n",
      "\n",
      "============================================================\n",
      "è¨“ç·´å®Œæˆï¼\n",
      "æœ€ä½³é©—è­‰ Dice: 0.8634\n",
      "æ¨¡å‹å·²ä¿å­˜è‡³: /workspace/outputs/nnunet_best.pth\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "å®Œæ•´çš„ nnU-Net å¯¦ç¾\n",
    "åŸºæ–¼è«–æ–‡: nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation\n",
    "\n",
    "æ ¸å¿ƒç‰¹æ€§:\n",
    "- nnU-Net æ¶æ§‹ï¼ˆLeaky ReLU + Instance Normï¼‰\n",
    "- æ·±åº¦ç›£ç£ï¼ˆDeep Supervisionï¼‰\n",
    "- æ•¸æ“šå¢å¼·ï¼ˆæ—‹è½‰ã€ç¸®æ”¾ã€å½ˆæ€§è®Šå½¢ç­‰ï¼‰\n",
    "- Dice + CE çµ„åˆæå¤±\n",
    "- Poly å­¸ç¿’ç‡èª¿åº¦\n",
    "- 5-fold äº¤å‰é©—è­‰ï¼ˆç°¡åŒ–ç‰ˆï¼‰\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from pathlib import Path\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "# æª¢æŸ¥ scipy æ˜¯å¦å¯ç”¨ï¼ˆä¸åœ¨é€™è£¡å°å…¥ä»¥é¿å…ç‰ˆæœ¬è¡çªï¼‰\n",
    "SCIPY_AVAILABLE = False\n",
    "try:\n",
    "    import scipy\n",
    "    SCIPY_AVAILABLE = True\n",
    "except:\n",
    "    pass\n",
    "\n",
    "if not SCIPY_AVAILABLE:\n",
    "    print(\"è­¦å‘Š: scipy ä¸å¯ç”¨ï¼Œå°‡ä½¿ç”¨ç°¡åŒ–çš„æ•¸æ“šå¢å¼·\")\n",
    "\n",
    "# ==================== è¨­ç½®éš¨æ©Ÿç¨®å­ ====================\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# ==================== nnU-Net æ¶æ§‹çµ„ä»¶ ====================\n",
    "\n",
    "class nnUNetConvBlock(nn.Module):\n",
    "    \"\"\"nnU-Net å·ç©å¡Š: Conv -> InstanceNorm -> LeakyReLU\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "        self.norm = nn.InstanceNorm3d(out_channels, affine=True)\n",
    "        self.activation = nn.LeakyReLU(negative_slope=0.01, inplace=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.activation(self.norm(self.conv(x)))\n",
    "\n",
    "\n",
    "class nnUNetResidualBlock(nn.Module):\n",
    "    \"\"\"nnU-Net çš„é›™å·ç©å¡Šï¼ˆé¡æ®˜å·®çµæ§‹ï¼‰\"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = nnUNetConvBlock(in_channels, out_channels)\n",
    "        self.conv2 = nnUNetConvBlock(out_channels, out_channels)\n",
    "        \n",
    "        # å¦‚æœé€šé“æ•¸æ”¹è®Šï¼Œéœ€è¦ 1x1 å·ç©èª¿æ•´\n",
    "        self.skip = None\n",
    "        if in_channels != out_channels:\n",
    "            self.skip = nn.Conv3d(in_channels, out_channels, kernel_size=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.conv2(out)\n",
    "        \n",
    "        if self.skip is not None:\n",
    "            residual = self.skip(residual)\n",
    "        \n",
    "        return out + residual\n",
    "\n",
    "\n",
    "class nnUNetDownsample(nn.Module):\n",
    "    \"\"\"nnU-Net ä¸‹æ¡æ¨£: Strided Convolution\"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv = nnUNetConvBlock(in_channels, out_channels, kernel_size=3, stride=2, padding=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class nnUNetUpsample(nn.Module):\n",
    "    \"\"\"nnU-Net ä¸Šæ¡æ¨£: Transposed Convolution\"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.upconv = nn.ConvTranspose3d(in_channels, out_channels, kernel_size=2, stride=2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.upconv(x)\n",
    "\n",
    "\n",
    "class nnUNet(nn.Module):\n",
    "    \"\"\"\n",
    "    å®Œæ•´çš„ nnU-Net æ¶æ§‹ï¼ˆ3Dï¼‰\n",
    "    - ä½¿ç”¨ Instance Normalization\n",
    "    - ä½¿ç”¨ Leaky ReLU\n",
    "    - ä½¿ç”¨ Strided Convolution ä¸‹æ¡æ¨£\n",
    "    - æ”¯æŒæ·±åº¦ç›£ç£\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels=1, num_classes=3, base_channels=32, num_pool=3, deep_supervision=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_pool = num_pool\n",
    "        self.deep_supervision = deep_supervision\n",
    "        \n",
    "        # ç·¨ç¢¼å™¨\n",
    "        self.encoders = nn.ModuleList()\n",
    "        self.downsamplers = nn.ModuleList()\n",
    "        \n",
    "        current_channels = in_channels\n",
    "        for i in range(num_pool + 1):\n",
    "            out_channels = base_channels * (2 ** i)\n",
    "            self.encoders.append(nnUNetResidualBlock(current_channels, out_channels))\n",
    "            \n",
    "            if i < num_pool:\n",
    "                self.downsamplers.append(nnUNetDownsample(out_channels, out_channels))\n",
    "            \n",
    "            current_channels = out_channels\n",
    "        \n",
    "        # è§£ç¢¼å™¨\n",
    "        self.upsamplers = nn.ModuleList()\n",
    "        self.decoders = nn.ModuleList()\n",
    "        \n",
    "        for i in range(num_pool):\n",
    "            in_ch = base_channels * (2 ** (num_pool - i))\n",
    "            out_ch = base_channels * (2 ** (num_pool - i - 1))\n",
    "            \n",
    "            self.upsamplers.append(nnUNetUpsample(in_ch, out_ch))\n",
    "            self.decoders.append(nnUNetResidualBlock(in_ch, out_ch))  # in_ch å› ç‚ºæœ‰ skip connection\n",
    "        \n",
    "        # è¼¸å‡ºé ­ï¼ˆå¤šå€‹ç”¨æ–¼æ·±åº¦ç›£ç£ï¼‰\n",
    "        self.seg_outputs = nn.ModuleList()\n",
    "        for i in range(num_pool + 1):\n",
    "            out_ch = base_channels * (2 ** i) if i == num_pool else base_channels * (2 ** i)\n",
    "            self.seg_outputs.append(nn.Conv3d(out_ch if i == 0 else base_channels * (2 ** i), num_classes, kernel_size=1))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # ç·¨ç¢¼è·¯å¾‘\n",
    "        encoder_outputs = []\n",
    "        current = x\n",
    "        \n",
    "        for i, encoder in enumerate(self.encoders):\n",
    "            current = encoder(current)\n",
    "            encoder_outputs.append(current)\n",
    "            \n",
    "            if i < self.num_pool:\n",
    "                current = self.downsamplers[i](current)\n",
    "        \n",
    "        # è§£ç¢¼è·¯å¾‘\n",
    "        seg_outputs = []\n",
    "        \n",
    "        # æœ€æ·±å±¤çš„è¼¸å‡ºï¼ˆç”¨æ–¼æ·±åº¦ç›£ç£ï¼‰\n",
    "        if self.deep_supervision:\n",
    "            seg_outputs.append(self.seg_outputs[-1](encoder_outputs[-1]))\n",
    "        \n",
    "        current = encoder_outputs[-1]\n",
    "        \n",
    "        for i in range(self.num_pool):\n",
    "            # ä¸Šæ¡æ¨£\n",
    "            current = self.upsamplers[i](current)\n",
    "            \n",
    "            # Skip connection\n",
    "            skip = encoder_outputs[-(i + 2)]\n",
    "            current = torch.cat([current, skip], dim=1)\n",
    "            \n",
    "            # è§£ç¢¼å¡Š\n",
    "            current = self.decoders[i](current)\n",
    "            \n",
    "            # æ·±åº¦ç›£ç£è¼¸å‡º\n",
    "            if self.deep_supervision:\n",
    "                seg_outputs.append(self.seg_outputs[-(i + 2)](current))\n",
    "        \n",
    "        # æœ€çµ‚è¼¸å‡º\n",
    "        final_output = self.seg_outputs[0](current) if not self.deep_supervision else seg_outputs[-1]\n",
    "        \n",
    "        if self.deep_supervision and self.training:\n",
    "            # åè½‰é †åºï¼Œå¾æ·ºåˆ°æ·±\n",
    "            return list(reversed(seg_outputs))\n",
    "        else:\n",
    "            return final_output\n",
    "\n",
    "\n",
    "# ==================== nnU-Net æå¤±å‡½æ•¸ ====================\n",
    "\n",
    "class nnUNetLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    nnU-Net æå¤±: Dice + CE\n",
    "    æ”¯æŒæ·±åº¦ç›£ç£\n",
    "    \"\"\"\n",
    "    def __init__(self, deep_supervision_weights=None, dice_weight=1.0, ce_weight=1.0):\n",
    "        super().__init__()\n",
    "        self.deep_supervision_weights = deep_supervision_weights\n",
    "        self.dice_weight = dice_weight\n",
    "        self.ce_weight = ce_weight\n",
    "        self.ce_loss = nn.CrossEntropyLoss()\n",
    "    \n",
    "    def dice_loss(self, pred, target, smooth=1.0):\n",
    "        \"\"\"Soft Dice Loss\"\"\"\n",
    "        pred = F.softmax(pred, dim=1)\n",
    "        \n",
    "        # è¨ˆç®—æ¯å€‹é¡åˆ¥çš„ Dice\n",
    "        dice_scores = []\n",
    "        for c in range(pred.shape[1]):\n",
    "            pred_c = pred[:, c]\n",
    "            target_c = (target == c).float()\n",
    "            \n",
    "            intersection = (pred_c * target_c).sum()\n",
    "            union = pred_c.sum() + target_c.sum()\n",
    "            \n",
    "            dice = (2.0 * intersection + smooth) / (union + smooth)\n",
    "            dice_scores.append(dice)\n",
    "        \n",
    "        return 1.0 - torch.stack(dice_scores).mean()\n",
    "    \n",
    "    def forward(self, outputs, target):\n",
    "        if isinstance(outputs, (list, tuple)):\n",
    "            # æ·±åº¦ç›£ç£\n",
    "            if self.deep_supervision_weights is None:\n",
    "                # é»˜èªæ¬Šé‡ï¼šè¶Šæ·±å±¤æ¬Šé‡è¶Šå°\n",
    "                weights = [1.0 / (2 ** i) for i in range(len(outputs))]\n",
    "                weights = [w / sum(weights) for w in weights]\n",
    "            else:\n",
    "                weights = self.deep_supervision_weights\n",
    "            \n",
    "            total_loss = 0\n",
    "            for i, output in enumerate(outputs):\n",
    "                # éœ€è¦èª¿æ•´ target å¤§å°ä»¥åŒ¹é…è¼¸å‡º\n",
    "                if output.shape[2:] != target.shape[1:]:\n",
    "                    target_resized = F.interpolate(\n",
    "                        target.unsqueeze(1).float(),\n",
    "                        size=output.shape[2:],\n",
    "                        mode='nearest'\n",
    "                    ).squeeze(1).long()\n",
    "                else:\n",
    "                    target_resized = target\n",
    "                \n",
    "                ce = self.ce_loss(output, target_resized)\n",
    "                dice = self.dice_loss(output, target_resized)\n",
    "                total_loss += weights[i] * (self.ce_weight * ce + self.dice_weight * dice)\n",
    "            \n",
    "            return total_loss\n",
    "        else:\n",
    "            # å–®ä¸€è¼¸å‡º\n",
    "            ce = self.ce_loss(outputs, target)\n",
    "            dice = self.dice_loss(outputs, target)\n",
    "            return self.ce_weight * ce + self.dice_weight * dice\n",
    "\n",
    "\n",
    "# ==================== nnU-Net æ•¸æ“šå¢å¼· ====================\n",
    "\n",
    "class nnUNetAugmentation:\n",
    "    \"\"\"nnU-Net é¢¨æ ¼çš„æ•¸æ“šå¢å¼·\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def random_rotation(image, label, angle_range=(-15, 15)):\n",
    "        \"\"\"éš¨æ©Ÿæ—‹è½‰\"\"\"\n",
    "        if not SCIPY_AVAILABLE:\n",
    "            # ç°¡åŒ–ç‰ˆï¼šåªåš 90 åº¦æ—‹è½‰\n",
    "            if random.random() > 0.5:\n",
    "                k = random.randint(1, 3)\n",
    "                axes = random.choice([(0, 1), (0, 2), (1, 2)])\n",
    "                image = np.rot90(image, k, axes).copy()\n",
    "                label = np.rot90(label, k, axes).copy()\n",
    "            return image, label\n",
    "        \n",
    "        if random.random() > 0.5:\n",
    "            try:\n",
    "                from scipy.ndimage import rotate\n",
    "                angle = random.uniform(*angle_range)\n",
    "                axes = random.choice([(0, 1), (0, 2), (1, 2)])\n",
    "                image = rotate(image, angle, axes=axes, reshape=False, order=3, mode='constant')\n",
    "                label = rotate(label, angle, axes=axes, reshape=False, order=0, mode='constant')\n",
    "            except:\n",
    "                # å¦‚æœå°å…¥å¤±æ•—ï¼Œä½¿ç”¨ 90 åº¦æ—‹è½‰\n",
    "                k = random.randint(1, 3)\n",
    "                axes = random.choice([(0, 1), (0, 2), (1, 2)])\n",
    "                image = np.rot90(image, k, axes).copy()\n",
    "                label = np.rot90(label, k, axes).copy()\n",
    "        return image, label\n",
    "    \n",
    "    @staticmethod\n",
    "    def random_scaling(image, label, scale_range=(0.85, 1.25)):\n",
    "        \"\"\"éš¨æ©Ÿç¸®æ”¾\"\"\"\n",
    "        if not SCIPY_AVAILABLE:\n",
    "            return image, label\n",
    "        \n",
    "        if random.random() > 0.5:\n",
    "            try:\n",
    "                from scipy.ndimage import zoom\n",
    "                scale = random.uniform(*scale_range)\n",
    "                scales = [scale] * 3\n",
    "                image = zoom(image, scales, order=3, mode='constant')\n",
    "                label = zoom(label, scales, order=0, mode='constant')\n",
    "            except:\n",
    "                pass  # å¦‚æœå¤±æ•—å°±è·³éç¸®æ”¾\n",
    "        return image, label\n",
    "    \n",
    "    @staticmethod\n",
    "    def random_elastic_deformation(image, label, alpha=100, sigma=10):\n",
    "        \"\"\"å½ˆæ€§è®Šå½¢\"\"\"\n",
    "        if not SCIPY_AVAILABLE:\n",
    "            return image, label\n",
    "        \n",
    "        if random.random() > 0.3:  # 30% æ©Ÿç‡\n",
    "            try:\n",
    "                from scipy.ndimage import gaussian_filter, map_coordinates\n",
    "                shape = image.shape\n",
    "                \n",
    "                # ç”Ÿæˆéš¨æ©Ÿä½ç§»å ´\n",
    "                dx = gaussian_filter((np.random.rand(*shape) * 2 - 1), sigma) * alpha\n",
    "                dy = gaussian_filter((np.random.rand(*shape) * 2 - 1), sigma) * alpha\n",
    "                dz = gaussian_filter((np.random.rand(*shape) * 2 - 1), sigma) * alpha\n",
    "                \n",
    "                x, y, z = np.meshgrid(np.arange(shape[0]), np.arange(shape[1]), np.arange(shape[2]), indexing='ij')\n",
    "                indices = np.reshape(x + dx, (-1, 1)), np.reshape(y + dy, (-1, 1)), np.reshape(z + dz, (-1, 1))\n",
    "                \n",
    "                image = map_coordinates(image, indices, order=3, mode='reflect').reshape(shape)\n",
    "                label = map_coordinates(label, indices, order=0, mode='reflect').reshape(shape)\n",
    "            except:\n",
    "                pass  # å¦‚æœå¤±æ•—å°±è·³éå½ˆæ€§è®Šå½¢\n",
    "        \n",
    "        return image, label\n",
    "    \n",
    "    @staticmethod\n",
    "    def random_gamma(image, gamma_range=(0.7, 1.5)):\n",
    "        \"\"\"éš¨æ©Ÿ Gamma æ ¡æ­£\"\"\"\n",
    "        if random.random() > 0.5:\n",
    "            gamma = random.uniform(*gamma_range)\n",
    "            image_min = image.min()\n",
    "            image_range = image.max() - image_min\n",
    "            if image_range > 0:\n",
    "                image = ((image - image_min) / image_range) ** gamma * image_range + image_min\n",
    "        return image\n",
    "    \n",
    "    @staticmethod\n",
    "    def random_brightness(image, brightness_range=(-0.2, 0.2)):\n",
    "        \"\"\"éš¨æ©Ÿäº®åº¦èª¿æ•´\"\"\"\n",
    "        if random.random() > 0.5:\n",
    "            brightness = random.uniform(*brightness_range)\n",
    "            image = image + brightness * image.std()\n",
    "        return image\n",
    "    \n",
    "    @staticmethod\n",
    "    def random_contrast(image, contrast_range=(0.75, 1.25)):\n",
    "        \"\"\"éš¨æ©Ÿå°æ¯”åº¦èª¿æ•´\"\"\"\n",
    "        if random.random() > 0.5:\n",
    "            contrast = random.uniform(*contrast_range)\n",
    "            mean = image.mean()\n",
    "            image = (image - mean) * contrast + mean\n",
    "        return image\n",
    "    \n",
    "    @staticmethod\n",
    "    def random_flip(image, label):\n",
    "        \"\"\"éš¨æ©Ÿç¿»è½‰\"\"\"\n",
    "        for axis in range(3):\n",
    "            if random.random() > 0.5:\n",
    "                image = np.flip(image, axis=axis).copy()\n",
    "                label = np.flip(label, axis=axis).copy()\n",
    "        return image, label\n",
    "\n",
    "\n",
    "# ==================== nnU-Net æ•¸æ“šé›† ====================\n",
    "\n",
    "class nnUNetDataset(Dataset):\n",
    "    \"\"\"nnU-Net é¢¨æ ¼çš„æ•¸æ“šé›†\"\"\"\n",
    "    def __init__(self, data_dir, target_size=64, is_train=True, use_augmentation=True):\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.target_size = target_size\n",
    "        self.is_train = is_train\n",
    "        self.use_augmentation = use_augmentation and is_train\n",
    "        \n",
    "        # ç²å–æ‰€æœ‰åœ–åƒæ–‡ä»¶ï¼ˆéæ¿¾æ‰ macOS éš±è—æ–‡ä»¶ï¼‰\n",
    "        all_image_files = list((self.data_dir / 'imagesTr').glob('*.nii.gz'))\n",
    "        self.image_files = sorted([f for f in all_image_files if not f.name.startswith('._')])\n",
    "        \n",
    "        all_label_files = list((self.data_dir / 'labelsTr').glob('*.nii.gz'))\n",
    "        self.label_files = sorted([f for f in all_label_files if not f.name.startswith('._')])\n",
    "        \n",
    "        if len(self.image_files) == 0:\n",
    "            raise ValueError(f\"åœ¨ {self.data_dir / 'imagesTr'} æ‰¾ä¸åˆ°ä»»ä½• .nii.gz æ–‡ä»¶ï¼\")\n",
    "        \n",
    "        print(f\"æ‰¾åˆ° {len(self.image_files)} å€‹è¨“ç·´æ¨£æœ¬\")\n",
    "        \n",
    "        self.aug = nnUNetAugmentation()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "    \n",
    "    def preprocess(self, image):\n",
    "        \"\"\"nnU-Net é¢¨æ ¼é è™•ç†\"\"\"\n",
    "        # Clip to percentiles\n",
    "        p1, p99 = np.percentile(image[image > 0], [0.5, 99.5]) if (image > 0).any() else (0, 1)\n",
    "        image = np.clip(image, p1, p99)\n",
    "        \n",
    "        # Z-score normalization (per image)\n",
    "        mean = image[image > 0].mean() if (image > 0).any() else 0\n",
    "        std = image[image > 0].std() if (image > 0).any() else 1\n",
    "        image = (image - mean) / (std + 1e-8)\n",
    "        \n",
    "        return image\n",
    "    \n",
    "    def apply_augmentation(self, image, label):\n",
    "        \"\"\"æ‡‰ç”¨ nnU-Net æ•¸æ“šå¢å¼·\"\"\"\n",
    "        # å¹¾ä½•è®Šæ›\n",
    "        image, label = self.aug.random_rotation(image, label)\n",
    "        image, label = self.aug.random_scaling(image, label)\n",
    "        image, label = self.aug.random_flip(image, label)\n",
    "        \n",
    "        # å¼·åº¦è®Šæ›ï¼ˆåƒ…å°åœ–åƒï¼‰\n",
    "        image = self.aug.random_gamma(image)\n",
    "        image = self.aug.random_brightness(image)\n",
    "        image = self.aug.random_contrast(image)\n",
    "        \n",
    "        return image, label\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # è®€å–æ–‡ä»¶\n",
    "        image = nib.load(self.image_files[idx]).get_fdata(dtype=np.float32)\n",
    "        label = nib.load(self.label_files[idx]).get_fdata(dtype=np.float32)\n",
    "        \n",
    "        # é è™•ç†\n",
    "        image = self.preprocess(image)\n",
    "        \n",
    "        # æ•¸æ“šå¢å¼·\n",
    "        if self.use_augmentation:\n",
    "            image, label = self.apply_augmentation(image, label)\n",
    "        \n",
    "        # è½‰ç‚º tensor ä¸¦èª¿æ•´å¤§å°\n",
    "        image = torch.from_numpy(image).unsqueeze(0).unsqueeze(0)  # [1, 1, H, W, D]\n",
    "        label = torch.from_numpy(label).unsqueeze(0).unsqueeze(0)  # [1, 1, H, W, D]\n",
    "        \n",
    "        image = F.interpolate(\n",
    "            image,\n",
    "            size=(self.target_size, self.target_size, self.target_size),\n",
    "            mode='trilinear',\n",
    "            align_corners=False\n",
    "        ).squeeze(0)\n",
    "        \n",
    "        label = F.interpolate(\n",
    "            label,\n",
    "            size=(self.target_size, self.target_size, self.target_size),\n",
    "            mode='nearest'\n",
    "        ).squeeze(0)\n",
    "        \n",
    "        label = label.squeeze(0).long()\n",
    "        label = torch.clamp(label, 0, 2)\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "\n",
    "# ==================== è©•ä¼°æŒ‡æ¨™ ====================\n",
    "\n",
    "def compute_dice(pred, target, num_classes=3):\n",
    "    \"\"\"è¨ˆç®— Dice åˆ†æ•¸\"\"\"\n",
    "    dice_scores = []\n",
    "    \n",
    "    for c in range(1, num_classes):  # è·³éèƒŒæ™¯\n",
    "        pred_c = (pred == c)\n",
    "        target_c = (target == c)\n",
    "        \n",
    "        intersection = (pred_c & target_c).sum().float()\n",
    "        union = pred_c.sum().float() + target_c.sum().float()\n",
    "        \n",
    "        if union == 0:\n",
    "            dice = 1.0 if intersection == 0 else 0.0\n",
    "        else:\n",
    "            dice = (2.0 * intersection) / union\n",
    "        \n",
    "        dice_scores.append(dice.item())\n",
    "    \n",
    "    return dice_scores\n",
    "\n",
    "\n",
    "# ==================== Poly å­¸ç¿’ç‡èª¿åº¦å™¨ ====================\n",
    "\n",
    "class PolynomialLRScheduler:\n",
    "    \"\"\"nnU-Net ä½¿ç”¨çš„ Polynomial å­¸ç¿’ç‡èª¿åº¦\"\"\"\n",
    "    def __init__(self, optimizer, initial_lr, max_epochs, power=0.9):\n",
    "        self.optimizer = optimizer\n",
    "        self.initial_lr = initial_lr\n",
    "        self.max_epochs = max_epochs\n",
    "        self.power = power\n",
    "        self.current_epoch = 0\n",
    "    \n",
    "    def step(self):\n",
    "        self.current_epoch += 1\n",
    "        lr = self.initial_lr * (1 - self.current_epoch / self.max_epochs) ** self.power\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "        return lr\n",
    "\n",
    "\n",
    "# ==================== è¨“ç·´å‡½æ•¸ ====================\n",
    "\n",
    "def train_epoch(model, loader, criterion, optimizer, device):\n",
    "    \"\"\"è¨“ç·´ä¸€å€‹ epoch\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    pbar = tqdm(loader, desc='Training')\n",
    "    for images, labels in pbar:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # å‰å‘å‚³æ’­\n",
    "        outputs = model(images)\n",
    "        \n",
    "        # è¨ˆç®—æå¤±ï¼ˆæ”¯æŒæ·±åº¦ç›£ç£ï¼‰\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # åå‘å‚³æ’­\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # æ¢¯åº¦è£å‰ªï¼ˆnnU-Net ä½¿ç”¨ï¼‰\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 12)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "    \n",
    "    return total_loss / len(loader)\n",
    "\n",
    "\n",
    "def validate(model, loader, device):\n",
    "    \"\"\"é©—è­‰\"\"\"\n",
    "    model.eval()\n",
    "    all_dice_scores = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(loader, desc='Validating'):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            \n",
    "            # å¦‚æœæ˜¯æ·±åº¦ç›£ç£ï¼Œåªå–æœ€çµ‚è¼¸å‡º\n",
    "            if isinstance(outputs, (list, tuple)):\n",
    "                outputs = outputs[-1]\n",
    "            \n",
    "            preds = outputs.argmax(dim=1)\n",
    "            \n",
    "            # è¨ˆç®— Dice\n",
    "            for pred, label in zip(preds, labels):\n",
    "                dice_scores = compute_dice(pred.cpu(), label.cpu())\n",
    "                all_dice_scores.append(dice_scores)\n",
    "    \n",
    "    # å¹³å‡ Dice\n",
    "    all_dice_scores = np.array(all_dice_scores)\n",
    "    mean_dice = all_dice_scores.mean(axis=0)\n",
    "    \n",
    "    return mean_dice\n",
    "\n",
    "\n",
    "# ==================== è‡ªå‹•æª¢æ¸¬æ•¸æ“šè·¯å¾‘ ====================\n",
    "\n",
    "def find_data_directory():\n",
    "    \"\"\"è‡ªå‹•æŸ¥æ‰¾æ•¸æ“šç›®éŒ„\"\"\"\n",
    "    possible_paths = [\n",
    "        '/home/claude/Task04_Hippocampus',\n",
    "        '/workspace/data/Task04_Hippocampus',\n",
    "        './Task04_Hippocampus',\n",
    "        './data/Task04_Hippocampus',\n",
    "        '../data/Task04_Hippocampus',\n",
    "        '/data/Task04_Hippocampus',\n",
    "    ]\n",
    "    \n",
    "    for path in possible_paths:\n",
    "        if Path(path).exists():\n",
    "            images_dir = Path(path) / 'imagesTr'\n",
    "            if images_dir.exists():\n",
    "                # éæ¿¾æ‰ macOS éš±è—æ–‡ä»¶\n",
    "                image_files = [f for f in images_dir.glob('*.nii.gz') if not f.name.startswith('._')]\n",
    "                if len(image_files) > 0:\n",
    "                    print(f\"âœ“ æ‰¾åˆ°æ•¸æ“šç›®éŒ„: {path}\")\n",
    "                    print(f\"  ({len(image_files)} å€‹æœ‰æ•ˆæ–‡ä»¶)\")\n",
    "                    return path\n",
    "    \n",
    "    print(\"\\nâŒ æ‰¾ä¸åˆ°æ•¸æ“šç›®éŒ„ï¼\")\n",
    "    return None\n",
    "\n",
    "\n",
    "# ==================== ä¸»è¨“ç·´æµç¨‹ ====================\n",
    "\n",
    "def main():\n",
    "    # è‡ªå‹•æŸ¥æ‰¾æ•¸æ“šç›®éŒ„\n",
    "    data_dir = find_data_directory()\n",
    "    \n",
    "    if data_dir is None:\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"éŒ¯èª¤ï¼šæ‰¾ä¸åˆ°æ•¸æ“šé›†ï¼\")\n",
    "        print(\"=\" * 60)\n",
    "        return\n",
    "    \n",
    "    # nnU-Net é…ç½®\n",
    "    config = {\n",
    "        'data_dir': data_dir,\n",
    "        'batch_size': 2,\n",
    "        'num_epochs': 100,  # nnU-Net é€šå¸¸è¨“ç·´æ›´é•·æ™‚é–“\n",
    "        'initial_lr': 1e-2,  # nnU-Net ä½¿ç”¨è¼ƒå¤§çš„åˆå§‹å­¸ç¿’ç‡\n",
    "        'base_channels': 32,  # nnU-Net ä½¿ç”¨æ›´å¤šé€šé“\n",
    "        'num_pool': 3,  # ä¸‹æ¡æ¨£å±¤æ•¸\n",
    "        'target_size': 64,\n",
    "        'deep_supervision': True,\n",
    "        'use_augmentation': True,\n",
    "        'device': 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    }\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"å®Œæ•´çš„ nnU-Net è¨“ç·´\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"é…ç½®:\")\n",
    "    for key, value in config.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    print()\n",
    "    print(f\"SciPy å¯ç”¨: {SCIPY_AVAILABLE}\")\n",
    "    if not SCIPY_AVAILABLE:\n",
    "        print(\"  âš ï¸  å°‡ä½¿ç”¨ç°¡åŒ–çš„æ•¸æ“šå¢å¼·ï¼ˆåƒ…ç¿»è½‰å’Œ 90Â° æ—‹è½‰ï¼‰\")\n",
    "    print()\n",
    "    \n",
    "    device = torch.device(config['device'])\n",
    "    \n",
    "    # å‰µå»ºæ•¸æ“šé›†\n",
    "    print(\"æº–å‚™æ•¸æ“šé›†...\")\n",
    "    try:\n",
    "        full_dataset = nnUNetDataset(\n",
    "            config['data_dir'],\n",
    "            target_size=config['target_size'],\n",
    "            is_train=True,\n",
    "            use_augmentation=config['use_augmentation']\n",
    "        )\n",
    "    except ValueError as e:\n",
    "        print(f\"\\néŒ¯èª¤: {e}\")\n",
    "        return\n",
    "    \n",
    "    # è¨“ç·´/é©—è­‰åŠƒåˆ† (80/20)\n",
    "    train_size = int(0.8 * len(full_dataset))\n",
    "    val_size = len(full_dataset) - train_size\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "        full_dataset, \n",
    "        [train_size, val_size],\n",
    "        generator=torch.Generator().manual_seed(42)\n",
    "    )\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=config['batch_size'],\n",
    "        shuffle=True,\n",
    "        num_workers=4,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=config['batch_size'],\n",
    "        shuffle=False,\n",
    "        num_workers=4,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    print(f\"è¨“ç·´é›†: {len(train_dataset)} æ¨£æœ¬\")\n",
    "    print(f\"é©—è­‰é›†: {len(val_dataset)} æ¨£æœ¬\")\n",
    "    print()\n",
    "    \n",
    "    # å‰µå»º nnU-Net æ¨¡å‹\n",
    "    print(\"å‰µå»º nnU-Net æ¨¡å‹...\")\n",
    "    model = nnUNet(\n",
    "        in_channels=1,\n",
    "        num_classes=3,\n",
    "        base_channels=config['base_channels'],\n",
    "        num_pool=config['num_pool'],\n",
    "        deep_supervision=config['deep_supervision']\n",
    "    ).to(device)\n",
    "    \n",
    "    # è¨ˆç®—åƒæ•¸é‡\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"æ¨¡å‹åƒæ•¸é‡: {total_params:,} ({total_params/1e6:.2f}M)\")\n",
    "    print()\n",
    "    \n",
    "    # æå¤±å‡½æ•¸å’Œå„ªåŒ–å™¨\n",
    "    criterion = nnUNetLoss(dice_weight=1.0, ce_weight=1.0)\n",
    "    \n",
    "    # nnU-Net ä½¿ç”¨ SGD with momentum å’Œ Nesterov\n",
    "    optimizer = torch.optim.SGD(\n",
    "        model.parameters(),\n",
    "        lr=config['initial_lr'],\n",
    "        momentum=0.99,\n",
    "        weight_decay=3e-5,\n",
    "        nesterov=True\n",
    "    )\n",
    "    \n",
    "    # Polynomial å­¸ç¿’ç‡èª¿åº¦\n",
    "    scheduler = PolynomialLRScheduler(\n",
    "        optimizer,\n",
    "        initial_lr=config['initial_lr'],\n",
    "        max_epochs=config['num_epochs'],\n",
    "        power=0.9\n",
    "    )\n",
    "    \n",
    "    # è¨“ç·´å¾ªç’°\n",
    "    print(\"é–‹å§‹è¨“ç·´...\")\n",
    "    print(f\"ä½¿ç”¨æ·±åº¦ç›£ç£: {config['deep_supervision']}\")\n",
    "    print(f\"ä½¿ç”¨æ•¸æ“šå¢å¼·: {config['use_augmentation']}\")\n",
    "    print()\n",
    "    \n",
    "    best_dice = 0.0\n",
    "    \n",
    "    for epoch in range(config['num_epochs']):\n",
    "        print(f\"Epoch {epoch+1}/{config['num_epochs']}\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        # è¨“ç·´\n",
    "        train_loss = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        \n",
    "        # é©—è­‰\n",
    "        val_dice = validate(model, val_loader, device)\n",
    "        \n",
    "        # å­¸ç¿’ç‡èª¿åº¦\n",
    "        current_lr = scheduler.step()\n",
    "        \n",
    "        # æ‰“å°çµæœ\n",
    "        print(f\"Train Loss: {train_loss:.4f}\")\n",
    "        print(f\"Val Dice - Class 1: {val_dice[0]:.4f}, Class 2: {val_dice[1]:.4f}, \"\n",
    "              f\"Avg: {val_dice.mean():.4f}\")\n",
    "        print(f\"Learning Rate: {current_lr:.6f}\")\n",
    "        \n",
    "        # ä¿å­˜æœ€ä½³æ¨¡å‹\n",
    "        if val_dice.mean() > best_dice:\n",
    "            best_dice = val_dice.mean()\n",
    "            \n",
    "            # ç¢ºä¿è¼¸å‡ºç›®éŒ„å­˜åœ¨\n",
    "            output_dir = Path('/workspace/outputs')\n",
    "            if not output_dir.exists():\n",
    "                output_dir = Path('./outputs')\n",
    "                output_dir.mkdir(exist_ok=True)\n",
    "            \n",
    "            save_path = output_dir / 'nnunet_best.pth'\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'dice': val_dice,\n",
    "                'config': config\n",
    "            }, save_path)\n",
    "            print(f\"âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹ (Dice: {best_dice:.4f}) -> {save_path}\")\n",
    "        \n",
    "        print()\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"è¨“ç·´å®Œæˆï¼\")\n",
    "    print(f\"æœ€ä½³é©—è­‰ Dice: {best_dice:.4f}\")\n",
    "    print(f\"æ¨¡å‹å·²ä¿å­˜è‡³: {save_path}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "950ddbea-82b5-430f-8759-0c8c7e81fc31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æª¢æ¸¬åˆ° Jupyter ç’°å¢ƒï¼Œä½¿ç”¨é»˜èªç¨®å­ 42\n",
      "\n",
      "============================================================\n",
      "å¢å¼·ç‰ˆ nnU-Net è¨“ç·´ (Seed: 42)\n",
      "============================================================\n",
      "é…ç½®:\n",
      "  data_dir: ./Task04_Hippocampus\n",
      "  batch_size: 2\n",
      "  num_epochs: 200\n",
      "  initial_lr: 0.01\n",
      "  base_channels: 48\n",
      "  num_pool: 3\n",
      "  target_size: 64\n",
      "  deep_supervision: True\n",
      "  use_augmentation: True\n",
      "  device: cuda\n",
      "  seed: 42\n",
      "\n",
      "è¨“ç·´é›†: 208 æ¨£æœ¬\n",
      "é©—è­‰é›†: 52 æ¨£æœ¬\n",
      "\n",
      "æ¨¡å‹åƒæ•¸é‡: 14,105,820 (14.11M)\n",
      "\n",
      "Epoch 1/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.22it/s, loss=0.4846]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  6.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6262\n",
      "Val Dice - Class 1: 0.6574, Class 2: 0.7210, Avg: 0.6892\n",
      "Learning Rate: 0.009955\n",
      "âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹ (Dice: 0.6892)\n",
      "\n",
      "Epoch 2/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.22it/s, loss=0.2565]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  8.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3083\n",
      "Val Dice - Class 1: 0.7604, Class 2: 0.7597, Avg: 0.7601\n",
      "Learning Rate: 0.009910\n",
      "âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹ (Dice: 0.7601)\n",
      "\n",
      "Epoch 3/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.25it/s, loss=0.3606]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  8.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2527\n",
      "Val Dice - Class 1: 0.7627, Class 2: 0.7887, Avg: 0.7757\n",
      "Learning Rate: 0.009865\n",
      "âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹ (Dice: 0.7757)\n",
      "\n",
      "Epoch 4/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.22it/s, loss=0.3312]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  8.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2279\n",
      "Val Dice - Class 1: 0.7816, Class 2: 0.7886, Avg: 0.7851\n",
      "Learning Rate: 0.009820\n",
      "âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹ (Dice: 0.7851)\n",
      "\n",
      "Epoch 5/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.22it/s, loss=0.2139]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  8.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2142\n",
      "Val Dice - Class 1: 0.8059, Class 2: 0.8074, Avg: 0.8067\n",
      "Learning Rate: 0.009775\n",
      "âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹ (Dice: 0.8067)\n",
      "\n",
      "Epoch 6/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.17it/s, loss=0.1980]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  8.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2056\n",
      "Val Dice - Class 1: 0.8091, Class 2: 0.8054, Avg: 0.8072\n",
      "Learning Rate: 0.009730\n",
      "âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹ (Dice: 0.8072)\n",
      "\n",
      "Epoch 7/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.18it/s, loss=0.3041]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  8.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1974\n",
      "Val Dice - Class 1: 0.8150, Class 2: 0.8128, Avg: 0.8139\n",
      "Learning Rate: 0.009684\n",
      "âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹ (Dice: 0.8139)\n",
      "\n",
      "Epoch 8/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.18it/s, loss=0.1863]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  7.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1951\n",
      "Val Dice - Class 1: 0.8179, Class 2: 0.8091, Avg: 0.8135\n",
      "Learning Rate: 0.009639\n",
      "\n",
      "Epoch 9/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.17it/s, loss=0.2450]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  8.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1924\n",
      "Val Dice - Class 1: 0.8231, Class 2: 0.8178, Avg: 0.8205\n",
      "Learning Rate: 0.009594\n",
      "âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹ (Dice: 0.8205)\n",
      "\n",
      "Epoch 10/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.16it/s, loss=0.1478]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  8.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1828\n",
      "Val Dice - Class 1: 0.8331, Class 2: 0.8205, Avg: 0.8268\n",
      "Learning Rate: 0.009549\n",
      "âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹ (Dice: 0.8268)\n",
      "\n",
      "Epoch 11/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.17it/s, loss=0.1918]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  8.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1806\n",
      "Val Dice - Class 1: 0.8321, Class 2: 0.8194, Avg: 0.8258\n",
      "Learning Rate: 0.009504\n",
      "\n",
      "Epoch 12/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.18it/s, loss=0.1885]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  8.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1776\n",
      "Val Dice - Class 1: 0.8354, Class 2: 0.8243, Avg: 0.8299\n",
      "Learning Rate: 0.009458\n",
      "âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹ (Dice: 0.8299)\n",
      "\n",
      "Epoch 13/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.16it/s, loss=0.1763]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  8.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1769\n",
      "Val Dice - Class 1: 0.8329, Class 2: 0.8222, Avg: 0.8276\n",
      "Learning Rate: 0.009413\n",
      "\n",
      "Epoch 14/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.21it/s, loss=0.1331]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  8.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1730\n",
      "Val Dice - Class 1: 0.8334, Class 2: 0.8175, Avg: 0.8255\n",
      "Learning Rate: 0.009368\n",
      "\n",
      "Epoch 15/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.18it/s, loss=0.1415]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  8.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1730\n",
      "Val Dice - Class 1: 0.8389, Class 2: 0.8259, Avg: 0.8324\n",
      "Learning Rate: 0.009322\n",
      "âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹ (Dice: 0.8324)\n",
      "\n",
      "Epoch 16/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.16it/s, loss=0.1551]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  8.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1717\n",
      "Val Dice - Class 1: 0.8380, Class 2: 0.8231, Avg: 0.8306\n",
      "Learning Rate: 0.009277\n",
      "\n",
      "Epoch 17/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.18it/s, loss=0.2227]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  8.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1711\n",
      "Val Dice - Class 1: 0.8451, Class 2: 0.8293, Avg: 0.8372\n",
      "Learning Rate: 0.009232\n",
      "âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹ (Dice: 0.8372)\n",
      "\n",
      "Epoch 18/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.15it/s, loss=0.2356]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  8.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1674\n",
      "Val Dice - Class 1: 0.8401, Class 2: 0.8271, Avg: 0.8336\n",
      "Learning Rate: 0.009186\n",
      "\n",
      "Epoch 19/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.21it/s, loss=0.1827]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  8.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1665\n",
      "Val Dice - Class 1: 0.8454, Class 2: 0.8338, Avg: 0.8396\n",
      "Learning Rate: 0.009141\n",
      "âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹ (Dice: 0.8396)\n",
      "\n",
      "Epoch 20/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.17it/s, loss=0.1469]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  7.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1664\n",
      "Val Dice - Class 1: 0.8422, Class 2: 0.8304, Avg: 0.8363\n",
      "Learning Rate: 0.009095\n",
      "\n",
      "Epoch 21/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.14it/s, loss=0.1294]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  8.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1649\n",
      "Val Dice - Class 1: 0.8469, Class 2: 0.8308, Avg: 0.8389\n",
      "Learning Rate: 0.009050\n",
      "\n",
      "Epoch 22/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.17it/s, loss=0.1923]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  8.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1633\n",
      "Val Dice - Class 1: 0.8405, Class 2: 0.8321, Avg: 0.8363\n",
      "Learning Rate: 0.009004\n",
      "\n",
      "Epoch 23/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.19it/s, loss=0.1628]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  7.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1653\n",
      "Val Dice - Class 1: 0.8462, Class 2: 0.8294, Avg: 0.8378\n",
      "Learning Rate: 0.008959\n",
      "\n",
      "Epoch 24/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.19it/s, loss=0.1754]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  8.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1642\n",
      "Val Dice - Class 1: 0.8367, Class 2: 0.8283, Avg: 0.8325\n",
      "Learning Rate: 0.008913\n",
      "\n",
      "Epoch 25/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.21it/s, loss=0.1230]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:02<00:00, 12.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1618\n",
      "Val Dice - Class 1: 0.8488, Class 2: 0.8312, Avg: 0.8400\n",
      "Learning Rate: 0.008868\n",
      "âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹ (Dice: 0.8400)\n",
      "\n",
      "Epoch 26/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.24it/s, loss=0.1456]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:02<00:00, 12.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1600\n",
      "Val Dice - Class 1: 0.8513, Class 2: 0.8328, Avg: 0.8421\n",
      "Learning Rate: 0.008822\n",
      "âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹ (Dice: 0.8421)\n",
      "\n",
      "Epoch 27/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.28it/s, loss=0.1508]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:02<00:00, 10.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1594\n",
      "Val Dice - Class 1: 0.8542, Class 2: 0.8347, Avg: 0.8445\n",
      "Learning Rate: 0.008776\n",
      "âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹ (Dice: 0.8445)\n",
      "\n",
      "Epoch 28/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.23it/s, loss=0.2022]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:02<00:00,  9.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1578\n",
      "Val Dice - Class 1: 0.8475, Class 2: 0.8272, Avg: 0.8374\n",
      "Learning Rate: 0.008731\n",
      "\n",
      "Epoch 29/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.29it/s, loss=0.1569]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:02<00:00, 10.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1556\n",
      "Val Dice - Class 1: 0.8500, Class 2: 0.8312, Avg: 0.8406\n",
      "Learning Rate: 0.008685\n",
      "\n",
      "Epoch 30/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.24it/s, loss=0.1726]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:02<00:00, 11.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1543\n",
      "Val Dice - Class 1: 0.8483, Class 2: 0.8344, Avg: 0.8414\n",
      "Learning Rate: 0.008639\n",
      "\n",
      "Epoch 31/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.28it/s, loss=0.1446]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:02<00:00, 11.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1556\n",
      "Val Dice - Class 1: 0.8515, Class 2: 0.8350, Avg: 0.8433\n",
      "Learning Rate: 0.008594\n",
      "\n",
      "Epoch 32/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.30it/s, loss=0.1331]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:02<00:00, 11.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1583\n",
      "Val Dice - Class 1: 0.8508, Class 2: 0.8349, Avg: 0.8429\n",
      "Learning Rate: 0.008548\n",
      "\n",
      "Epoch 33/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.28it/s, loss=0.1141]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:02<00:00, 11.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1539\n",
      "Val Dice - Class 1: 0.8539, Class 2: 0.8327, Avg: 0.8433\n",
      "Learning Rate: 0.008502\n",
      "\n",
      "Epoch 34/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.27it/s, loss=0.1787]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:02<00:00, 11.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1557\n",
      "Val Dice - Class 1: 0.8561, Class 2: 0.8370, Avg: 0.8466\n",
      "Learning Rate: 0.008456\n",
      "âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹ (Dice: 0.8466)\n",
      "\n",
      "Epoch 35/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.28it/s, loss=0.1466]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:02<00:00, 11.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1508\n",
      "Val Dice - Class 1: 0.8527, Class 2: 0.8380, Avg: 0.8453\n",
      "Learning Rate: 0.008410\n",
      "\n",
      "Epoch 36/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.29it/s, loss=0.1990]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:02<00:00, 10.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1515\n",
      "Val Dice - Class 1: 0.8558, Class 2: 0.8397, Avg: 0.8478\n",
      "Learning Rate: 0.008364\n",
      "âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹ (Dice: 0.8478)\n",
      "\n",
      "Epoch 37/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.27it/s, loss=0.1467]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:02<00:00, 12.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1500\n",
      "Val Dice - Class 1: 0.8558, Class 2: 0.8359, Avg: 0.8459\n",
      "Learning Rate: 0.008318\n",
      "\n",
      "Epoch 38/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.30it/s, loss=0.1297]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:02<00:00, 11.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1491\n",
      "Val Dice - Class 1: 0.8567, Class 2: 0.8392, Avg: 0.8480\n",
      "Learning Rate: 0.008272\n",
      "âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹ (Dice: 0.8480)\n",
      "\n",
      "Epoch 39/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.27it/s, loss=0.1883]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:02<00:00, 12.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1498\n",
      "Val Dice - Class 1: 0.8581, Class 2: 0.8368, Avg: 0.8474\n",
      "Learning Rate: 0.008227\n",
      "\n",
      "Epoch 40/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.25it/s, loss=0.1431]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:02<00:00, 11.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1477\n",
      "Val Dice - Class 1: 0.8547, Class 2: 0.8424, Avg: 0.8486\n",
      "Learning Rate: 0.008181\n",
      "âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹ (Dice: 0.8486)\n",
      "\n",
      "Epoch 41/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.20it/s, loss=0.1373]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:02<00:00, 11.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1480\n",
      "Val Dice - Class 1: 0.8566, Class 2: 0.8386, Avg: 0.8476\n",
      "Learning Rate: 0.008134\n",
      "\n",
      "Epoch 42/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.25it/s, loss=0.1336]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:02<00:00, 11.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1470\n",
      "Val Dice - Class 1: 0.8570, Class 2: 0.8415, Avg: 0.8492\n",
      "Learning Rate: 0.008088\n",
      "âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹ (Dice: 0.8492)\n",
      "\n",
      "Epoch 43/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.26it/s, loss=0.1398]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:02<00:00, 11.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1468\n",
      "Val Dice - Class 1: 0.8590, Class 2: 0.8407, Avg: 0.8499\n",
      "Learning Rate: 0.008042\n",
      "âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹ (Dice: 0.8499)\n",
      "\n",
      "Epoch 44/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.25it/s, loss=0.1486]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:02<00:00, 11.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1468\n",
      "Val Dice - Class 1: 0.8617, Class 2: 0.8435, Avg: 0.8526\n",
      "Learning Rate: 0.007996\n",
      "âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹ (Dice: 0.8526)\n",
      "\n",
      "Epoch 45/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.27it/s, loss=0.1521]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  8.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1430\n",
      "Val Dice - Class 1: 0.8628, Class 2: 0.8451, Avg: 0.8539\n",
      "Learning Rate: 0.007950\n",
      "âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹ (Dice: 0.8539)\n",
      "\n",
      "Epoch 46/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.07it/s, loss=0.1757]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  7.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1429\n",
      "Val Dice - Class 1: 0.8624, Class 2: 0.8409, Avg: 0.8516\n",
      "Learning Rate: 0.007904\n",
      "\n",
      "Epoch 47/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.07it/s, loss=0.1523]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  7.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1459\n",
      "Val Dice - Class 1: 0.8592, Class 2: 0.8364, Avg: 0.8478\n",
      "Learning Rate: 0.007858\n",
      "\n",
      "Epoch 48/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.08it/s, loss=0.1713]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:02<00:00, 11.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1431\n",
      "Val Dice - Class 1: 0.8591, Class 2: 0.8406, Avg: 0.8499\n",
      "Learning Rate: 0.007811\n",
      "\n",
      "Epoch 49/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.14it/s, loss=0.1219]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  7.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1456\n",
      "Val Dice - Class 1: 0.8569, Class 2: 0.8419, Avg: 0.8494\n",
      "Learning Rate: 0.007765\n",
      "\n",
      "Epoch 50/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.10it/s, loss=0.1422]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:02<00:00,  8.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1457\n",
      "Val Dice - Class 1: 0.8571, Class 2: 0.8392, Avg: 0.8482\n",
      "Learning Rate: 0.007719\n",
      "\n",
      "Epoch 51/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.14it/s, loss=0.1373]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  7.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1444\n",
      "Val Dice - Class 1: 0.8617, Class 2: 0.8454, Avg: 0.8535\n",
      "Learning Rate: 0.007673\n",
      "\n",
      "Epoch 52/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.10it/s, loss=0.1531]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  7.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1454\n",
      "Val Dice - Class 1: 0.8556, Class 2: 0.8452, Avg: 0.8504\n",
      "Learning Rate: 0.007626\n",
      "\n",
      "Epoch 53/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.03it/s, loss=0.1287]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  8.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1433\n",
      "Val Dice - Class 1: 0.8651, Class 2: 0.8458, Avg: 0.8554\n",
      "Learning Rate: 0.007580\n",
      "âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹ (Dice: 0.8554)\n",
      "\n",
      "Epoch 54/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.05it/s, loss=0.1564]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  8.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1452\n",
      "Val Dice - Class 1: 0.8554, Class 2: 0.8411, Avg: 0.8483\n",
      "Learning Rate: 0.007533\n",
      "\n",
      "Epoch 55/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.12it/s, loss=0.1264]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  7.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1444\n",
      "Val Dice - Class 1: 0.8602, Class 2: 0.8391, Avg: 0.8496\n",
      "Learning Rate: 0.007487\n",
      "\n",
      "Epoch 56/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.05it/s, loss=0.1651]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  7.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1420\n",
      "Val Dice - Class 1: 0.8673, Class 2: 0.8454, Avg: 0.8564\n",
      "Learning Rate: 0.007440\n",
      "âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹ (Dice: 0.8564)\n",
      "\n",
      "Epoch 57/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.06it/s, loss=0.1449]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  8.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1386\n",
      "Val Dice - Class 1: 0.8636, Class 2: 0.8446, Avg: 0.8541\n",
      "Learning Rate: 0.007394\n",
      "\n",
      "Epoch 58/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.08it/s, loss=0.1269]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  7.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1404\n",
      "Val Dice - Class 1: 0.8632, Class 2: 0.8466, Avg: 0.8549\n",
      "Learning Rate: 0.007347\n",
      "\n",
      "Epoch 59/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.05it/s, loss=0.1062]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  7.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1407\n",
      "Val Dice - Class 1: 0.8626, Class 2: 0.8436, Avg: 0.8531\n",
      "Learning Rate: 0.007301\n",
      "\n",
      "Epoch 60/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.09it/s, loss=0.1392]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  7.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1394\n",
      "Val Dice - Class 1: 0.8690, Class 2: 0.8506, Avg: 0.8598\n",
      "Learning Rate: 0.007254\n",
      "âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹ (Dice: 0.8598)\n",
      "\n",
      "Epoch 61/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.09it/s, loss=0.1357]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  7.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1376\n",
      "Val Dice - Class 1: 0.8580, Class 2: 0.8442, Avg: 0.8511\n",
      "Learning Rate: 0.007208\n",
      "\n",
      "Epoch 62/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.13it/s, loss=0.1666]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:02<00:00,  9.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1363\n",
      "Val Dice - Class 1: 0.8647, Class 2: 0.8435, Avg: 0.8541\n",
      "Learning Rate: 0.007161\n",
      "\n",
      "Epoch 63/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.16it/s, loss=0.1191]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  7.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1396\n",
      "Val Dice - Class 1: 0.8645, Class 2: 0.8483, Avg: 0.8564\n",
      "Learning Rate: 0.007114\n",
      "\n",
      "Epoch 64/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.09it/s, loss=0.1484]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  8.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1368\n",
      "Val Dice - Class 1: 0.8690, Class 2: 0.8511, Avg: 0.8600\n",
      "Learning Rate: 0.007067\n",
      "âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹ (Dice: 0.8600)\n",
      "\n",
      "Epoch 65/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.05it/s, loss=0.1635]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  7.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1378\n",
      "Val Dice - Class 1: 0.8629, Class 2: 0.8469, Avg: 0.8549\n",
      "Learning Rate: 0.007021\n",
      "\n",
      "Epoch 66/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.03it/s, loss=0.1366]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  7.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1376\n",
      "Val Dice - Class 1: 0.8622, Class 2: 0.8469, Avg: 0.8545\n",
      "Learning Rate: 0.006974\n",
      "\n",
      "Epoch 67/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.03it/s, loss=0.1303]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  8.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1391\n",
      "Val Dice - Class 1: 0.8643, Class 2: 0.8434, Avg: 0.8538\n",
      "Learning Rate: 0.006927\n",
      "\n",
      "Epoch 68/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.12it/s, loss=0.1269]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  7.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1368\n",
      "Val Dice - Class 1: 0.8603, Class 2: 0.8399, Avg: 0.8501\n",
      "Learning Rate: 0.006880\n",
      "\n",
      "Epoch 69/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.12it/s, loss=0.1553]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  8.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1363\n",
      "Val Dice - Class 1: 0.8612, Class 2: 0.8445, Avg: 0.8529\n",
      "Learning Rate: 0.006833\n",
      "\n",
      "Epoch 70/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.11it/s, loss=0.1530]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  7.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1370\n",
      "Val Dice - Class 1: 0.8645, Class 2: 0.8457, Avg: 0.8551\n",
      "Learning Rate: 0.006786\n",
      "\n",
      "Epoch 71/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.11it/s, loss=0.1496]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  7.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1366\n",
      "Val Dice - Class 1: 0.8605, Class 2: 0.8470, Avg: 0.8537\n",
      "Learning Rate: 0.006739\n",
      "\n",
      "Epoch 72/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.13it/s, loss=0.1183]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  7.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1349\n",
      "Val Dice - Class 1: 0.8626, Class 2: 0.8458, Avg: 0.8542\n",
      "Learning Rate: 0.006692\n",
      "\n",
      "Epoch 73/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.08it/s, loss=0.1842]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  7.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1346\n",
      "Val Dice - Class 1: 0.8658, Class 2: 0.8451, Avg: 0.8555\n",
      "Learning Rate: 0.006645\n",
      "\n",
      "Epoch 74/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.11it/s, loss=0.1530]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  7.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1339\n",
      "Val Dice - Class 1: 0.8634, Class 2: 0.8480, Avg: 0.8557\n",
      "Learning Rate: 0.006598\n",
      "\n",
      "Epoch 75/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.09it/s, loss=0.1179]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  7.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1345\n",
      "Val Dice - Class 1: 0.8647, Class 2: 0.8472, Avg: 0.8559\n",
      "Learning Rate: 0.006551\n",
      "\n",
      "Epoch 76/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.12it/s, loss=0.1299]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  7.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1346\n",
      "Val Dice - Class 1: 0.8647, Class 2: 0.8492, Avg: 0.8569\n",
      "Learning Rate: 0.006504\n",
      "\n",
      "Epoch 77/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.10it/s, loss=0.1140]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  8.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1334\n",
      "Val Dice - Class 1: 0.8641, Class 2: 0.8459, Avg: 0.8550\n",
      "Learning Rate: 0.006456\n",
      "\n",
      "Epoch 78/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.10it/s, loss=0.1619]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  8.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1346\n",
      "Val Dice - Class 1: 0.8643, Class 2: 0.8474, Avg: 0.8558\n",
      "Learning Rate: 0.006409\n",
      "\n",
      "Epoch 79/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.06it/s, loss=0.1274]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  7.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1319\n",
      "Val Dice - Class 1: 0.8656, Class 2: 0.8491, Avg: 0.8573\n",
      "Learning Rate: 0.006362\n",
      "\n",
      "Epoch 80/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.09it/s, loss=0.1472]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  7.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1328\n",
      "Val Dice - Class 1: 0.8671, Class 2: 0.8490, Avg: 0.8580\n",
      "Learning Rate: 0.006314\n",
      "\n",
      "Epoch 81/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.09it/s, loss=0.1184]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  7.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1303\n",
      "Val Dice - Class 1: 0.8660, Class 2: 0.8526, Avg: 0.8593\n",
      "Learning Rate: 0.006267\n",
      "\n",
      "Epoch 82/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.15it/s, loss=0.1048]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:02<00:00,  9.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1324\n",
      "Val Dice - Class 1: 0.8661, Class 2: 0.8478, Avg: 0.8569\n",
      "Learning Rate: 0.006220\n",
      "\n",
      "Epoch 83/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.07it/s, loss=0.1463]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  7.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1309\n",
      "Val Dice - Class 1: 0.8690, Class 2: 0.8495, Avg: 0.8592\n",
      "Learning Rate: 0.006172\n",
      "\n",
      "Epoch 84/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.09it/s, loss=0.1390]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  7.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1318\n",
      "Val Dice - Class 1: 0.8616, Class 2: 0.8452, Avg: 0.8534\n",
      "Learning Rate: 0.006125\n",
      "\n",
      "Epoch 85/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.08it/s, loss=0.1432]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  7.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1296\n",
      "Val Dice - Class 1: 0.8668, Class 2: 0.8504, Avg: 0.8586\n",
      "Learning Rate: 0.006077\n",
      "\n",
      "Epoch 86/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.14it/s, loss=0.1193]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  7.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1308\n",
      "Val Dice - Class 1: 0.8646, Class 2: 0.8512, Avg: 0.8579\n",
      "Learning Rate: 0.006030\n",
      "\n",
      "Epoch 87/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.08it/s, loss=0.1507]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  7.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1273\n",
      "Val Dice - Class 1: 0.8680, Class 2: 0.8494, Avg: 0.8587\n",
      "Learning Rate: 0.005982\n",
      "\n",
      "Epoch 88/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.08it/s, loss=0.1139]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  7.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1265\n",
      "Val Dice - Class 1: 0.8730, Class 2: 0.8556, Avg: 0.8643\n",
      "Learning Rate: 0.005934\n",
      "âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹ (Dice: 0.8643)\n",
      "\n",
      "Epoch 89/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.07it/s, loss=0.1208]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  7.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1247\n",
      "Val Dice - Class 1: 0.8690, Class 2: 0.8531, Avg: 0.8611\n",
      "Learning Rate: 0.005887\n",
      "\n",
      "Epoch 90/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.11it/s, loss=0.1211]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  7.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1278\n",
      "Val Dice - Class 1: 0.8655, Class 2: 0.8502, Avg: 0.8579\n",
      "Learning Rate: 0.005839\n",
      "\n",
      "Epoch 91/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.12it/s, loss=0.1100]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  7.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1289\n",
      "Val Dice - Class 1: 0.8683, Class 2: 0.8498, Avg: 0.8590\n",
      "Learning Rate: 0.005791\n",
      "\n",
      "Epoch 92/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.09it/s, loss=0.0964]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  7.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1259\n",
      "Val Dice - Class 1: 0.8680, Class 2: 0.8518, Avg: 0.8599\n",
      "Learning Rate: 0.005743\n",
      "\n",
      "Epoch 93/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.08it/s, loss=0.1336]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  8.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1282\n",
      "Val Dice - Class 1: 0.8700, Class 2: 0.8509, Avg: 0.8605\n",
      "Learning Rate: 0.005695\n",
      "\n",
      "Epoch 94/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.11it/s, loss=0.1106]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  7.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1270\n",
      "Val Dice - Class 1: 0.8673, Class 2: 0.8489, Avg: 0.8581\n",
      "Learning Rate: 0.005647\n",
      "\n",
      "Epoch 95/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.12it/s, loss=0.1544]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  8.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1262\n",
      "Val Dice - Class 1: 0.8684, Class 2: 0.8553, Avg: 0.8618\n",
      "Learning Rate: 0.005599\n",
      "\n",
      "Epoch 96/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.08it/s, loss=0.1359]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  8.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1255\n",
      "Val Dice - Class 1: 0.8691, Class 2: 0.8508, Avg: 0.8599\n",
      "Learning Rate: 0.005551\n",
      "\n",
      "Epoch 97/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.10it/s, loss=0.1368]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  8.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1278\n",
      "Val Dice - Class 1: 0.8691, Class 2: 0.8504, Avg: 0.8597\n",
      "Learning Rate: 0.005503\n",
      "\n",
      "Epoch 98/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.10it/s, loss=0.1017]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  7.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1265\n",
      "Val Dice - Class 1: 0.8699, Class 2: 0.8515, Avg: 0.8607\n",
      "Learning Rate: 0.005455\n",
      "\n",
      "Epoch 99/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.06it/s, loss=0.1257]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  7.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1256\n",
      "Val Dice - Class 1: 0.8711, Class 2: 0.8521, Avg: 0.8616\n",
      "Learning Rate: 0.005407\n",
      "\n",
      "Epoch 100/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.11it/s, loss=0.1186]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  7.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1252\n",
      "Val Dice - Class 1: 0.8675, Class 2: 0.8532, Avg: 0.8604\n",
      "Learning Rate: 0.005359\n",
      "\n",
      "Epoch 101/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.08it/s, loss=0.1254]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  8.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1234\n",
      "Val Dice - Class 1: 0.8692, Class 2: 0.8534, Avg: 0.8613\n",
      "Learning Rate: 0.005311\n",
      "\n",
      "Epoch 102/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.10it/s, loss=0.1344]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  7.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1237\n",
      "Val Dice - Class 1: 0.8653, Class 2: 0.8484, Avg: 0.8568\n",
      "Learning Rate: 0.005262\n",
      "\n",
      "Epoch 103/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.06it/s, loss=0.1340]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  7.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1245\n",
      "Val Dice - Class 1: 0.8698, Class 2: 0.8510, Avg: 0.8604\n",
      "Learning Rate: 0.005214\n",
      "\n",
      "Epoch 104/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.04it/s, loss=0.1169]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  8.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1228\n",
      "Val Dice - Class 1: 0.8705, Class 2: 0.8545, Avg: 0.8625\n",
      "Learning Rate: 0.005166\n",
      "\n",
      "Epoch 105/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.07it/s, loss=0.1184]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  8.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1215\n",
      "Val Dice - Class 1: 0.8679, Class 2: 0.8502, Avg: 0.8590\n",
      "Learning Rate: 0.005117\n",
      "\n",
      "Epoch 106/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.14it/s, loss=0.1149]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  8.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1230\n",
      "Val Dice - Class 1: 0.8682, Class 2: 0.8544, Avg: 0.8613\n",
      "Learning Rate: 0.005069\n",
      "\n",
      "Epoch 107/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.13it/s, loss=0.1239]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  7.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1229\n",
      "Val Dice - Class 1: 0.8690, Class 2: 0.8552, Avg: 0.8621\n",
      "Learning Rate: 0.005020\n",
      "\n",
      "Epoch 108/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.14it/s, loss=0.0851]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  8.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1225\n",
      "Val Dice - Class 1: 0.8720, Class 2: 0.8552, Avg: 0.8636\n",
      "Learning Rate: 0.004971\n",
      "\n",
      "Epoch 109/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.12it/s, loss=0.1081]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  7.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1217\n",
      "Val Dice - Class 1: 0.8680, Class 2: 0.8545, Avg: 0.8612\n",
      "Learning Rate: 0.004923\n",
      "\n",
      "Epoch 110/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.12it/s, loss=0.1188]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  7.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1214\n",
      "Val Dice - Class 1: 0.8698, Class 2: 0.8547, Avg: 0.8623\n",
      "Learning Rate: 0.004874\n",
      "\n",
      "Epoch 111/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.12it/s, loss=0.1768]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  7.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1234\n",
      "Val Dice - Class 1: 0.8667, Class 2: 0.8521, Avg: 0.8594\n",
      "Learning Rate: 0.004825\n",
      "\n",
      "Epoch 112/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.06it/s, loss=0.1087]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  8.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1215\n",
      "Val Dice - Class 1: 0.8656, Class 2: 0.8512, Avg: 0.8584\n",
      "Learning Rate: 0.004776\n",
      "\n",
      "Epoch 113/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.11it/s, loss=0.0962]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  8.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1195\n",
      "Val Dice - Class 1: 0.8724, Class 2: 0.8549, Avg: 0.8636\n",
      "Learning Rate: 0.004728\n",
      "\n",
      "Epoch 114/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.14it/s, loss=0.1147]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  7.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1195\n",
      "Val Dice - Class 1: 0.8710, Class 2: 0.8539, Avg: 0.8625\n",
      "Learning Rate: 0.004679\n",
      "\n",
      "Epoch 115/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.16it/s, loss=0.1530]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:02<00:00,  8.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1199\n",
      "Val Dice - Class 1: 0.8710, Class 2: 0.8539, Avg: 0.8625\n",
      "Learning Rate: 0.004630\n",
      "\n",
      "Epoch 116/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.10it/s, loss=0.1308]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  7.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1223\n",
      "Val Dice - Class 1: 0.8701, Class 2: 0.8540, Avg: 0.8621\n",
      "Learning Rate: 0.004581\n",
      "\n",
      "Epoch 117/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.12it/s, loss=0.1105]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  7.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1204\n",
      "Val Dice - Class 1: 0.8706, Class 2: 0.8554, Avg: 0.8630\n",
      "Learning Rate: 0.004532\n",
      "\n",
      "Epoch 118/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.12it/s, loss=0.1169]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  8.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1201\n",
      "Val Dice - Class 1: 0.8722, Class 2: 0.8552, Avg: 0.8637\n",
      "Learning Rate: 0.004482\n",
      "\n",
      "Epoch 119/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.12it/s, loss=0.1187]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  7.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1187\n",
      "Val Dice - Class 1: 0.8719, Class 2: 0.8515, Avg: 0.8617\n",
      "Learning Rate: 0.004433\n",
      "\n",
      "Epoch 120/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.11it/s, loss=0.0828]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  7.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1187\n",
      "Val Dice - Class 1: 0.8678, Class 2: 0.8521, Avg: 0.8599\n",
      "Learning Rate: 0.004384\n",
      "\n",
      "Epoch 121/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.08it/s, loss=0.1338]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  8.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1187\n",
      "Val Dice - Class 1: 0.8712, Class 2: 0.8550, Avg: 0.8631\n",
      "Learning Rate: 0.004334\n",
      "\n",
      "Epoch 122/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.18it/s, loss=0.0896]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  8.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1183\n",
      "Val Dice - Class 1: 0.8714, Class 2: 0.8539, Avg: 0.8626\n",
      "Learning Rate: 0.004285\n",
      "\n",
      "Epoch 123/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.07it/s, loss=0.1094]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  7.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1173\n",
      "Val Dice - Class 1: 0.8695, Class 2: 0.8516, Avg: 0.8605\n",
      "Learning Rate: 0.004236\n",
      "\n",
      "Epoch 124/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.12it/s, loss=0.1348]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  7.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1174\n",
      "Val Dice - Class 1: 0.8719, Class 2: 0.8531, Avg: 0.8625\n",
      "Learning Rate: 0.004186\n",
      "\n",
      "Epoch 125/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.13it/s, loss=0.1013]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:02<00:00,  9.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1162\n",
      "Val Dice - Class 1: 0.8744, Class 2: 0.8570, Avg: 0.8657\n",
      "Learning Rate: 0.004136\n",
      "âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹ (Dice: 0.8657)\n",
      "\n",
      "Epoch 126/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.04it/s, loss=0.1492]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  7.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1138\n",
      "Val Dice - Class 1: 0.8703, Class 2: 0.8558, Avg: 0.8630\n",
      "Learning Rate: 0.004087\n",
      "\n",
      "Epoch 127/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.11it/s, loss=0.1116]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  7.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1170\n",
      "Val Dice - Class 1: 0.8708, Class 2: 0.8504, Avg: 0.8606\n",
      "Learning Rate: 0.004037\n",
      "\n",
      "Epoch 128/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.10it/s, loss=0.1601]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  7.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1149\n",
      "Val Dice - Class 1: 0.8737, Class 2: 0.8561, Avg: 0.8649\n",
      "Learning Rate: 0.003987\n",
      "\n",
      "Epoch 129/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.12it/s, loss=0.1134]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  7.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1130\n",
      "Val Dice - Class 1: 0.8706, Class 2: 0.8533, Avg: 0.8620\n",
      "Learning Rate: 0.003937\n",
      "\n",
      "Epoch 130/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.11it/s, loss=0.0934]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  7.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1139\n",
      "Val Dice - Class 1: 0.8708, Class 2: 0.8537, Avg: 0.8623\n",
      "Learning Rate: 0.003887\n",
      "\n",
      "Epoch 131/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.11it/s, loss=0.0882]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  7.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1127\n",
      "Val Dice - Class 1: 0.8711, Class 2: 0.8538, Avg: 0.8624\n",
      "Learning Rate: 0.003837\n",
      "\n",
      "Epoch 132/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.11it/s, loss=0.1148]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  8.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1128\n",
      "Val Dice - Class 1: 0.8703, Class 2: 0.8540, Avg: 0.8622\n",
      "Learning Rate: 0.003787\n",
      "\n",
      "Epoch 133/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.10it/s, loss=0.0918]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  7.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1142\n",
      "Val Dice - Class 1: 0.8691, Class 2: 0.8542, Avg: 0.8617\n",
      "Learning Rate: 0.003737\n",
      "\n",
      "Epoch 134/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.09it/s, loss=0.0923]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  8.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1160\n",
      "Val Dice - Class 1: 0.8741, Class 2: 0.8563, Avg: 0.8652\n",
      "Learning Rate: 0.003687\n",
      "\n",
      "Epoch 135/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.12it/s, loss=0.1090]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  7.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1140\n",
      "Val Dice - Class 1: 0.8716, Class 2: 0.8530, Avg: 0.8623\n",
      "Learning Rate: 0.003637\n",
      "\n",
      "Epoch 136/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.07it/s, loss=0.1227]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  7.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1132\n",
      "Val Dice - Class 1: 0.8711, Class 2: 0.8535, Avg: 0.8623\n",
      "Learning Rate: 0.003586\n",
      "\n",
      "Epoch 137/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.10it/s, loss=0.0880]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  8.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1133\n",
      "Val Dice - Class 1: 0.8708, Class 2: 0.8530, Avg: 0.8619\n",
      "Learning Rate: 0.003536\n",
      "\n",
      "Epoch 138/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.10it/s, loss=0.1264]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  7.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1123\n",
      "Val Dice - Class 1: 0.8728, Class 2: 0.8561, Avg: 0.8644\n",
      "Learning Rate: 0.003485\n",
      "\n",
      "Epoch 139/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.12it/s, loss=0.1183]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  7.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1114\n",
      "Val Dice - Class 1: 0.8712, Class 2: 0.8533, Avg: 0.8623\n",
      "Learning Rate: 0.003435\n",
      "\n",
      "Epoch 140/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.11it/s, loss=0.0842]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  7.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1129\n",
      "Val Dice - Class 1: 0.8706, Class 2: 0.8550, Avg: 0.8628\n",
      "Learning Rate: 0.003384\n",
      "\n",
      "Epoch 141/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.11it/s, loss=0.1013]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  7.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1127\n",
      "Val Dice - Class 1: 0.8726, Class 2: 0.8552, Avg: 0.8639\n",
      "Learning Rate: 0.003333\n",
      "\n",
      "Epoch 142/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.10it/s, loss=0.0904]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  7.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1128\n",
      "Val Dice - Class 1: 0.8714, Class 2: 0.8537, Avg: 0.8626\n",
      "Learning Rate: 0.003282\n",
      "\n",
      "Epoch 143/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.11it/s, loss=0.0913]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  7.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1107\n",
      "Val Dice - Class 1: 0.8680, Class 2: 0.8535, Avg: 0.8607\n",
      "Learning Rate: 0.003231\n",
      "\n",
      "Epoch 144/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.08it/s, loss=0.1234]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  7.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1137\n",
      "Val Dice - Class 1: 0.8707, Class 2: 0.8527, Avg: 0.8617\n",
      "Learning Rate: 0.003180\n",
      "\n",
      "Epoch 145/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.13it/s, loss=0.1017]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  8.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1111\n",
      "Val Dice - Class 1: 0.8704, Class 2: 0.8540, Avg: 0.8622\n",
      "Learning Rate: 0.003129\n",
      "\n",
      "Epoch 146/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.12it/s, loss=0.0895]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  8.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1105\n",
      "Val Dice - Class 1: 0.8727, Class 2: 0.8555, Avg: 0.8641\n",
      "Learning Rate: 0.003078\n",
      "\n",
      "Epoch 147/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.14it/s, loss=0.1099]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  8.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1099\n",
      "Val Dice - Class 1: 0.8722, Class 2: 0.8506, Avg: 0.8614\n",
      "Learning Rate: 0.003026\n",
      "\n",
      "Epoch 148/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.07it/s, loss=0.1258]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  7.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1115\n",
      "Val Dice - Class 1: 0.8704, Class 2: 0.8548, Avg: 0.8626\n",
      "Learning Rate: 0.002975\n",
      "\n",
      "Epoch 149/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.14it/s, loss=0.1280]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  8.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1094\n",
      "Val Dice - Class 1: 0.8707, Class 2: 0.8553, Avg: 0.8630\n",
      "Learning Rate: 0.002923\n",
      "\n",
      "Epoch 150/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.10it/s, loss=0.1071]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  7.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1088\n",
      "Val Dice - Class 1: 0.8697, Class 2: 0.8546, Avg: 0.8621\n",
      "Learning Rate: 0.002872\n",
      "\n",
      "Epoch 151/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.10it/s, loss=0.1097]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  7.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1091\n",
      "Val Dice - Class 1: 0.8675, Class 2: 0.8533, Avg: 0.8604\n",
      "Learning Rate: 0.002820\n",
      "\n",
      "Epoch 152/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.09it/s, loss=0.0896]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  7.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1082\n",
      "Val Dice - Class 1: 0.8686, Class 2: 0.8533, Avg: 0.8609\n",
      "Learning Rate: 0.002768\n",
      "\n",
      "Epoch 153/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.10it/s, loss=0.1056]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  7.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1092\n",
      "Val Dice - Class 1: 0.8692, Class 2: 0.8549, Avg: 0.8621\n",
      "Learning Rate: 0.002716\n",
      "\n",
      "Epoch 154/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.12it/s, loss=0.1184]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  6.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1091\n",
      "Val Dice - Class 1: 0.8728, Class 2: 0.8549, Avg: 0.8639\n",
      "Learning Rate: 0.002664\n",
      "\n",
      "Epoch 155/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.06it/s, loss=0.0943]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  8.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1091\n",
      "Val Dice - Class 1: 0.8738, Class 2: 0.8563, Avg: 0.8650\n",
      "Learning Rate: 0.002612\n",
      "\n",
      "Epoch 156/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.11it/s, loss=0.0910]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  7.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1069\n",
      "Val Dice - Class 1: 0.8696, Class 2: 0.8549, Avg: 0.8623\n",
      "Learning Rate: 0.002560\n",
      "\n",
      "Epoch 157/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.10it/s, loss=0.0865]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  8.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1070\n",
      "Val Dice - Class 1: 0.8721, Class 2: 0.8567, Avg: 0.8644\n",
      "Learning Rate: 0.002507\n",
      "\n",
      "Epoch 158/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.08it/s, loss=0.1415]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  7.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1066\n",
      "Val Dice - Class 1: 0.8732, Class 2: 0.8566, Avg: 0.8649\n",
      "Learning Rate: 0.002455\n",
      "\n",
      "Epoch 159/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.04it/s, loss=0.0990]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  7.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1076\n",
      "Val Dice - Class 1: 0.8712, Class 2: 0.8559, Avg: 0.8635\n",
      "Learning Rate: 0.002402\n",
      "\n",
      "Epoch 160/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.09it/s, loss=0.1069]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  7.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1081\n",
      "Val Dice - Class 1: 0.8702, Class 2: 0.8555, Avg: 0.8629\n",
      "Learning Rate: 0.002349\n",
      "\n",
      "Epoch 161/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.04it/s, loss=0.0908]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  8.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1073\n",
      "Val Dice - Class 1: 0.8717, Class 2: 0.8559, Avg: 0.8638\n",
      "Learning Rate: 0.002296\n",
      "\n",
      "Epoch 162/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.09it/s, loss=0.1179]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  8.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1061\n",
      "Val Dice - Class 1: 0.8733, Class 2: 0.8578, Avg: 0.8655\n",
      "Learning Rate: 0.002243\n",
      "\n",
      "Epoch 163/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.10it/s, loss=0.1223]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  7.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1049\n",
      "Val Dice - Class 1: 0.8735, Class 2: 0.8575, Avg: 0.8655\n",
      "Learning Rate: 0.002190\n",
      "\n",
      "Epoch 164/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.12it/s, loss=0.0941]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  7.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1034\n",
      "Val Dice - Class 1: 0.8731, Class 2: 0.8555, Avg: 0.8643\n",
      "Learning Rate: 0.002137\n",
      "\n",
      "Epoch 165/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.09it/s, loss=0.1207]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  7.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1037\n",
      "Val Dice - Class 1: 0.8736, Class 2: 0.8571, Avg: 0.8654\n",
      "Learning Rate: 0.002083\n",
      "\n",
      "Epoch 166/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.09it/s, loss=0.1165]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  8.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1045\n",
      "Val Dice - Class 1: 0.8708, Class 2: 0.8531, Avg: 0.8620\n",
      "Learning Rate: 0.002030\n",
      "\n",
      "Epoch 167/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.13it/s, loss=0.1113]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  7.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1052\n",
      "Val Dice - Class 1: 0.8738, Class 2: 0.8558, Avg: 0.8648\n",
      "Learning Rate: 0.001976\n",
      "\n",
      "Epoch 168/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.09it/s, loss=0.1165]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  7.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1038\n",
      "Val Dice - Class 1: 0.8723, Class 2: 0.8578, Avg: 0.8650\n",
      "Learning Rate: 0.001922\n",
      "\n",
      "Epoch 169/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.13it/s, loss=0.1266]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:02<00:00,  9.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1035\n",
      "Val Dice - Class 1: 0.8705, Class 2: 0.8541, Avg: 0.8623\n",
      "Learning Rate: 0.001868\n",
      "\n",
      "Epoch 170/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.13it/s, loss=0.0827]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  7.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1057\n",
      "Val Dice - Class 1: 0.8739, Class 2: 0.8574, Avg: 0.8657\n",
      "Learning Rate: 0.001813\n",
      "\n",
      "Epoch 171/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.09it/s, loss=0.0935]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  7.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1020\n",
      "Val Dice - Class 1: 0.8723, Class 2: 0.8568, Avg: 0.8645\n",
      "Learning Rate: 0.001759\n",
      "\n",
      "Epoch 172/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.07it/s, loss=0.1306]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  8.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1029\n",
      "Val Dice - Class 1: 0.8710, Class 2: 0.8550, Avg: 0.8630\n",
      "Learning Rate: 0.001704\n",
      "\n",
      "Epoch 173/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.08it/s, loss=0.0914]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  8.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1000\n",
      "Val Dice - Class 1: 0.8707, Class 2: 0.8543, Avg: 0.8625\n",
      "Learning Rate: 0.001649\n",
      "\n",
      "Epoch 174/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.11it/s, loss=0.1040]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  8.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1018\n",
      "Val Dice - Class 1: 0.8738, Class 2: 0.8583, Avg: 0.8661\n",
      "Learning Rate: 0.001594\n",
      "âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹ (Dice: 0.8661)\n",
      "\n",
      "Epoch 175/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.10it/s, loss=0.1037]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  8.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1008\n",
      "Val Dice - Class 1: 0.8720, Class 2: 0.8521, Avg: 0.8620\n",
      "Learning Rate: 0.001539\n",
      "\n",
      "Epoch 176/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.15it/s, loss=0.0996]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  7.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1013\n",
      "Val Dice - Class 1: 0.8707, Class 2: 0.8517, Avg: 0.8612\n",
      "Learning Rate: 0.001483\n",
      "\n",
      "Epoch 177/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.08it/s, loss=0.1324]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  8.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1009\n",
      "Val Dice - Class 1: 0.8709, Class 2: 0.8573, Avg: 0.8641\n",
      "Learning Rate: 0.001428\n",
      "\n",
      "Epoch 178/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.09it/s, loss=0.1049]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  8.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1017\n",
      "Val Dice - Class 1: 0.8729, Class 2: 0.8579, Avg: 0.8654\n",
      "Learning Rate: 0.001372\n",
      "\n",
      "Epoch 179/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.12it/s, loss=0.0880]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  8.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1006\n",
      "Val Dice - Class 1: 0.8707, Class 2: 0.8550, Avg: 0.8628\n",
      "Learning Rate: 0.001315\n",
      "\n",
      "Epoch 180/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.10it/s, loss=0.0989]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  7.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1003\n",
      "Val Dice - Class 1: 0.8708, Class 2: 0.8558, Avg: 0.8633\n",
      "Learning Rate: 0.001259\n",
      "\n",
      "Epoch 181/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.14it/s, loss=0.1182]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  7.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1011\n",
      "Val Dice - Class 1: 0.8715, Class 2: 0.8555, Avg: 0.8635\n",
      "Learning Rate: 0.001202\n",
      "\n",
      "Epoch 182/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.12it/s, loss=0.0932]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  7.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1003\n",
      "Val Dice - Class 1: 0.8758, Class 2: 0.8585, Avg: 0.8672\n",
      "Learning Rate: 0.001145\n",
      "âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹ (Dice: 0.8672)\n",
      "\n",
      "Epoch 183/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.19it/s, loss=0.1180]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  8.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0998\n",
      "Val Dice - Class 1: 0.8724, Class 2: 0.8568, Avg: 0.8646\n",
      "Learning Rate: 0.001088\n",
      "\n",
      "Epoch 184/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.14it/s, loss=0.1154]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  8.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0994\n",
      "Val Dice - Class 1: 0.8741, Class 2: 0.8580, Avg: 0.8661\n",
      "Learning Rate: 0.001030\n",
      "\n",
      "Epoch 185/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.08it/s, loss=0.0849]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  7.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0982\n",
      "Val Dice - Class 1: 0.8743, Class 2: 0.8579, Avg: 0.8661\n",
      "Learning Rate: 0.000972\n",
      "\n",
      "Epoch 186/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.08it/s, loss=0.1393]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  7.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0993\n",
      "Val Dice - Class 1: 0.8757, Class 2: 0.8592, Avg: 0.8674\n",
      "Learning Rate: 0.000913\n",
      "âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹ (Dice: 0.8674)\n",
      "\n",
      "Epoch 187/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.14it/s, loss=0.0844]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  7.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0972\n",
      "Val Dice - Class 1: 0.8727, Class 2: 0.8574, Avg: 0.8651\n",
      "Learning Rate: 0.000854\n",
      "\n",
      "Epoch 188/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.10it/s, loss=0.1131]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  8.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0991\n",
      "Val Dice - Class 1: 0.8732, Class 2: 0.8571, Avg: 0.8651\n",
      "Learning Rate: 0.000795\n",
      "\n",
      "Epoch 189/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.14it/s, loss=0.0898]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  8.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0979\n",
      "Val Dice - Class 1: 0.8726, Class 2: 0.8572, Avg: 0.8649\n",
      "Learning Rate: 0.000735\n",
      "\n",
      "Epoch 190/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.10it/s, loss=0.1014]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  7.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0972\n",
      "Val Dice - Class 1: 0.8735, Class 2: 0.8583, Avg: 0.8659\n",
      "Learning Rate: 0.000675\n",
      "\n",
      "Epoch 191/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.10it/s, loss=0.1026]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  7.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0971\n",
      "Val Dice - Class 1: 0.8704, Class 2: 0.8546, Avg: 0.8625\n",
      "Learning Rate: 0.000614\n",
      "\n",
      "Epoch 192/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.12it/s, loss=0.1096]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  7.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0964\n",
      "Val Dice - Class 1: 0.8729, Class 2: 0.8572, Avg: 0.8650\n",
      "Learning Rate: 0.000552\n",
      "\n",
      "Epoch 193/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.10it/s, loss=0.0868]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  7.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0988\n",
      "Val Dice - Class 1: 0.8732, Class 2: 0.8598, Avg: 0.8665\n",
      "Learning Rate: 0.000489\n",
      "\n",
      "Epoch 194/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.07it/s, loss=0.0918]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  7.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0958\n",
      "Val Dice - Class 1: 0.8742, Class 2: 0.8563, Avg: 0.8653\n",
      "Learning Rate: 0.000426\n",
      "\n",
      "Epoch 195/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.13it/s, loss=0.0958]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  8.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0979\n",
      "Val Dice - Class 1: 0.8733, Class 2: 0.8578, Avg: 0.8656\n",
      "Learning Rate: 0.000362\n",
      "\n",
      "Epoch 196/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.15it/s, loss=0.0712]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  7.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0970\n",
      "Val Dice - Class 1: 0.8732, Class 2: 0.8570, Avg: 0.8651\n",
      "Learning Rate: 0.000296\n",
      "\n",
      "Epoch 197/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.12it/s, loss=0.1040]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  7.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0963\n",
      "Val Dice - Class 1: 0.8748, Class 2: 0.8591, Avg: 0.8670\n",
      "Learning Rate: 0.000228\n",
      "\n",
      "Epoch 198/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.13it/s, loss=0.0980]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  7.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0963\n",
      "Val Dice - Class 1: 0.8745, Class 2: 0.8578, Avg: 0.8661\n",
      "Learning Rate: 0.000158\n",
      "\n",
      "Epoch 199/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.12it/s, loss=0.1221]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  8.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0959\n",
      "Val Dice - Class 1: 0.8751, Class 2: 0.8594, Avg: 0.8673\n",
      "Learning Rate: 0.000085\n",
      "\n",
      "Epoch 200/200\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:12<00:00,  8.11it/s, loss=0.0976]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  7.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0958\n",
      "Val Dice - Class 1: 0.8713, Class 2: 0.8548, Avg: 0.8630\n",
      "Learning Rate: 0.000000\n",
      "\n",
      "============================================================\n",
      "è¨“ç·´å®Œæˆï¼Seed: 42\n",
      "æœ€ä½³é©—è­‰ Dice: 0.8674\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "å¢å¼·ç‰ˆ nnU-Net - ç”¨æ–¼ Ensemble\n",
    "- 200 epochsï¼ˆæ›´é•·è¨“ç·´ï¼‰\n",
    "- 48 base channelsï¼ˆæ›´å¤§æ¨¡å‹ï¼‰\n",
    "- æ”¯æŒå¤šå€‹éš¨æ©Ÿç¨®å­è¨“ç·´\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from pathlib import Path\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "\n",
    "# æª¢æŸ¥ scipy æ˜¯å¦å¯ç”¨\n",
    "SCIPY_AVAILABLE = False\n",
    "try:\n",
    "    import scipy\n",
    "    SCIPY_AVAILABLE = True\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# ==================== è¨­ç½®éš¨æ©Ÿç¨®å­ ====================\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# ==================== nnU-Net æ¶æ§‹ï¼ˆèˆ‡ä¹‹å‰ç›¸åŒï¼‰====================\n",
    "\n",
    "class nnUNetConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "        self.norm = nn.InstanceNorm3d(out_channels, affine=True)\n",
    "        self.activation = nn.LeakyReLU(negative_slope=0.01, inplace=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.activation(self.norm(self.conv(x)))\n",
    "\n",
    "class nnUNetResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = nnUNetConvBlock(in_channels, out_channels)\n",
    "        self.conv2 = nnUNetConvBlock(out_channels, out_channels)\n",
    "        self.skip = None\n",
    "        if in_channels != out_channels:\n",
    "            self.skip = nn.Conv3d(in_channels, out_channels, kernel_size=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.conv2(out)\n",
    "        if self.skip is not None:\n",
    "            residual = self.skip(residual)\n",
    "        return out + residual\n",
    "\n",
    "class nnUNetDownsample(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv = nnUNetConvBlock(in_channels, out_channels, kernel_size=3, stride=2, padding=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class nnUNetUpsample(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.upconv = nn.ConvTranspose3d(in_channels, out_channels, kernel_size=2, stride=2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.upconv(x)\n",
    "\n",
    "class nnUNet(nn.Module):\n",
    "    def __init__(self, in_channels=1, num_classes=3, base_channels=48, num_pool=3, deep_supervision=True):\n",
    "        super().__init__()\n",
    "        self.num_pool = num_pool\n",
    "        self.deep_supervision = deep_supervision\n",
    "        \n",
    "        # ç·¨ç¢¼å™¨\n",
    "        self.encoders = nn.ModuleList()\n",
    "        self.downsamplers = nn.ModuleList()\n",
    "        \n",
    "        current_channels = in_channels\n",
    "        for i in range(num_pool + 1):\n",
    "            out_channels = base_channels * (2 ** i)\n",
    "            self.encoders.append(nnUNetResidualBlock(current_channels, out_channels))\n",
    "            if i < num_pool:\n",
    "                self.downsamplers.append(nnUNetDownsample(out_channels, out_channels))\n",
    "            current_channels = out_channels\n",
    "        \n",
    "        # è§£ç¢¼å™¨\n",
    "        self.upsamplers = nn.ModuleList()\n",
    "        self.decoders = nn.ModuleList()\n",
    "        \n",
    "        for i in range(num_pool):\n",
    "            in_ch = base_channels * (2 ** (num_pool - i))\n",
    "            out_ch = base_channels * (2 ** (num_pool - i - 1))\n",
    "            self.upsamplers.append(nnUNetUpsample(in_ch, out_ch))\n",
    "            self.decoders.append(nnUNetResidualBlock(in_ch, out_ch))\n",
    "        \n",
    "        # è¼¸å‡ºé ­\n",
    "        self.seg_outputs = nn.ModuleList()\n",
    "        for i in range(num_pool + 1):\n",
    "            out_ch = base_channels * (2 ** i)\n",
    "            self.seg_outputs.append(nn.Conv3d(out_ch, num_classes, kernel_size=1))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        encoder_outputs = []\n",
    "        current = x\n",
    "        \n",
    "        for i, encoder in enumerate(self.encoders):\n",
    "            current = encoder(current)\n",
    "            encoder_outputs.append(current)\n",
    "            if i < self.num_pool:\n",
    "                current = self.downsamplers[i](current)\n",
    "        \n",
    "        seg_outputs = []\n",
    "        if self.deep_supervision:\n",
    "            seg_outputs.append(self.seg_outputs[-1](encoder_outputs[-1]))\n",
    "        \n",
    "        current = encoder_outputs[-1]\n",
    "        for i in range(self.num_pool):\n",
    "            current = self.upsamplers[i](current)\n",
    "            skip = encoder_outputs[-(i + 2)]\n",
    "            current = torch.cat([current, skip], dim=1)\n",
    "            current = self.decoders[i](current)\n",
    "            if self.deep_supervision:\n",
    "                seg_outputs.append(self.seg_outputs[-(i + 2)](current))\n",
    "        \n",
    "        final_output = self.seg_outputs[0](current) if not self.deep_supervision else seg_outputs[-1]\n",
    "        \n",
    "        if self.deep_supervision and self.training:\n",
    "            return list(reversed(seg_outputs))\n",
    "        else:\n",
    "            return final_output\n",
    "\n",
    "# ==================== æå¤±å‡½æ•¸ ====================\n",
    "\n",
    "class nnUNetLoss(nn.Module):\n",
    "    def __init__(self, deep_supervision_weights=None, dice_weight=1.0, ce_weight=1.0):\n",
    "        super().__init__()\n",
    "        self.deep_supervision_weights = deep_supervision_weights\n",
    "        self.dice_weight = dice_weight\n",
    "        self.ce_weight = ce_weight\n",
    "        self.ce_loss = nn.CrossEntropyLoss()\n",
    "    \n",
    "    def dice_loss(self, pred, target, smooth=1.0):\n",
    "        pred = F.softmax(pred, dim=1)\n",
    "        dice_scores = []\n",
    "        for c in range(pred.shape[1]):\n",
    "            pred_c = pred[:, c]\n",
    "            target_c = (target == c).float()\n",
    "            intersection = (pred_c * target_c).sum()\n",
    "            union = pred_c.sum() + target_c.sum()\n",
    "            dice = (2.0 * intersection + smooth) / (union + smooth)\n",
    "            dice_scores.append(dice)\n",
    "        return 1.0 - torch.stack(dice_scores).mean()\n",
    "    \n",
    "    def forward(self, outputs, target):\n",
    "        if isinstance(outputs, (list, tuple)):\n",
    "            if self.deep_supervision_weights is None:\n",
    "                weights = [1.0 / (2 ** i) for i in range(len(outputs))]\n",
    "                weights = [w / sum(weights) for w in weights]\n",
    "            else:\n",
    "                weights = self.deep_supervision_weights\n",
    "            \n",
    "            total_loss = 0\n",
    "            for i, output in enumerate(outputs):\n",
    "                if output.shape[2:] != target.shape[1:]:\n",
    "                    target_resized = F.interpolate(\n",
    "                        target.unsqueeze(1).float(),\n",
    "                        size=output.shape[2:],\n",
    "                        mode='nearest'\n",
    "                    ).squeeze(1).long()\n",
    "                else:\n",
    "                    target_resized = target\n",
    "                \n",
    "                ce = self.ce_loss(output, target_resized)\n",
    "                dice = self.dice_loss(output, target_resized)\n",
    "                total_loss += weights[i] * (self.ce_weight * ce + self.dice_weight * dice)\n",
    "            return total_loss\n",
    "        else:\n",
    "            ce = self.ce_loss(outputs, target)\n",
    "            dice = self.dice_loss(outputs, target)\n",
    "            return self.ce_weight * ce + self.dice_weight * dice\n",
    "\n",
    "# ==================== æ•¸æ“šå¢å¼· ====================\n",
    "\n",
    "class nnUNetAugmentation:\n",
    "    @staticmethod\n",
    "    def random_rotation(image, label, angle_range=(-20, 20)):  # å¢å¼·åˆ° Â±20Â°\n",
    "        if not SCIPY_AVAILABLE:\n",
    "            if random.random() > 0.5:\n",
    "                k = random.randint(1, 3)\n",
    "                axes = random.choice([(0, 1), (0, 2), (1, 2)])\n",
    "                image = np.rot90(image, k, axes).copy()\n",
    "                label = np.rot90(label, k, axes).copy()\n",
    "            return image, label\n",
    "        \n",
    "        if random.random() > 0.5:\n",
    "            try:\n",
    "                from scipy.ndimage import rotate\n",
    "                angle = random.uniform(*angle_range)\n",
    "                axes = random.choice([(0, 1), (0, 2), (1, 2)])\n",
    "                image = rotate(image, angle, axes=axes, reshape=False, order=3, mode='constant')\n",
    "                label = rotate(label, angle, axes=axes, reshape=False, order=0, mode='constant')\n",
    "            except:\n",
    "                k = random.randint(1, 3)\n",
    "                axes = random.choice([(0, 1), (0, 2), (1, 2)])\n",
    "                image = np.rot90(image, k, axes).copy()\n",
    "                label = np.rot90(label, k, axes).copy()\n",
    "        return image, label\n",
    "    \n",
    "    @staticmethod\n",
    "    def random_scaling(image, label, scale_range=(0.8, 1.3)):  # å¢å¼·ç¸®æ”¾ç¯„åœ\n",
    "        if not SCIPY_AVAILABLE:\n",
    "            return image, label\n",
    "        \n",
    "        if random.random() > 0.5:\n",
    "            try:\n",
    "                from scipy.ndimage import zoom\n",
    "                scale = random.uniform(*scale_range)\n",
    "                scales = [scale] * 3\n",
    "                image = zoom(image, scales, order=3, mode='constant')\n",
    "                label = zoom(label, scales, order=0, mode='constant')\n",
    "            except:\n",
    "                pass\n",
    "        return image, label\n",
    "    \n",
    "    @staticmethod\n",
    "    def random_elastic_deformation(image, label, alpha=100, sigma=10):\n",
    "        if not SCIPY_AVAILABLE:\n",
    "            return image, label\n",
    "        \n",
    "        if random.random() > 0.3:\n",
    "            try:\n",
    "                from scipy.ndimage import gaussian_filter, map_coordinates\n",
    "                shape = image.shape\n",
    "                dx = gaussian_filter((np.random.rand(*shape) * 2 - 1), sigma) * alpha\n",
    "                dy = gaussian_filter((np.random.rand(*shape) * 2 - 1), sigma) * alpha\n",
    "                dz = gaussian_filter((np.random.rand(*shape) * 2 - 1), sigma) * alpha\n",
    "                x, y, z = np.meshgrid(np.arange(shape[0]), np.arange(shape[1]), np.arange(shape[2]), indexing='ij')\n",
    "                indices = np.reshape(x + dx, (-1, 1)), np.reshape(y + dy, (-1, 1)), np.reshape(z + dz, (-1, 1))\n",
    "                image = map_coordinates(image, indices, order=3, mode='reflect').reshape(shape)\n",
    "                label = map_coordinates(label, indices, order=0, mode='reflect').reshape(shape)\n",
    "            except:\n",
    "                pass\n",
    "        return image, label\n",
    "    \n",
    "    @staticmethod\n",
    "    def random_gamma(image, gamma_range=(0.7, 1.5)):\n",
    "        if random.random() > 0.5:\n",
    "            gamma = random.uniform(*gamma_range)\n",
    "            image_min = image.min()\n",
    "            image_range = image.max() - image_min\n",
    "            if image_range > 0:\n",
    "                image = ((image - image_min) / image_range) ** gamma * image_range + image_min\n",
    "        return image\n",
    "    \n",
    "    @staticmethod\n",
    "    def random_brightness(image, brightness_range=(-0.2, 0.2)):\n",
    "        if random.random() > 0.5:\n",
    "            brightness = random.uniform(*brightness_range)\n",
    "            image = image + brightness * image.std()\n",
    "        return image\n",
    "    \n",
    "    @staticmethod\n",
    "    def random_contrast(image, contrast_range=(0.75, 1.25)):\n",
    "        if random.random() > 0.5:\n",
    "            contrast = random.uniform(*contrast_range)\n",
    "            mean = image.mean()\n",
    "            image = (image - mean) * contrast + mean\n",
    "        return image\n",
    "    \n",
    "    @staticmethod\n",
    "    def random_flip(image, label):\n",
    "        for axis in range(3):\n",
    "            if random.random() > 0.5:\n",
    "                image = np.flip(image, axis=axis).copy()\n",
    "                label = np.flip(label, axis=axis).copy()\n",
    "        return image, label\n",
    "\n",
    "# ==================== æ•¸æ“šé›† ====================\n",
    "\n",
    "class nnUNetDataset(Dataset):\n",
    "    def __init__(self, data_dir, target_size=64, is_train=True, use_augmentation=True):\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.target_size = target_size\n",
    "        self.is_train = is_train\n",
    "        self.use_augmentation = use_augmentation and is_train\n",
    "        \n",
    "        all_image_files = list((self.data_dir / 'imagesTr').glob('*.nii.gz'))\n",
    "        self.image_files = sorted([f for f in all_image_files if not f.name.startswith('._')])\n",
    "        all_label_files = list((self.data_dir / 'labelsTr').glob('*.nii.gz'))\n",
    "        self.label_files = sorted([f for f in all_label_files if not f.name.startswith('._')])\n",
    "        \n",
    "        if len(self.image_files) == 0:\n",
    "            raise ValueError(f\"åœ¨ {self.data_dir / 'imagesTr'} æ‰¾ä¸åˆ°ä»»ä½• .nii.gz æ–‡ä»¶ï¼\")\n",
    "        \n",
    "        self.aug = nnUNetAugmentation()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "    \n",
    "    def preprocess(self, image):\n",
    "        p1, p99 = np.percentile(image[image > 0], [0.5, 99.5]) if (image > 0).any() else (0, 1)\n",
    "        image = np.clip(image, p1, p99)\n",
    "        mean = image[image > 0].mean() if (image > 0).any() else 0\n",
    "        std = image[image > 0].std() if (image > 0).any() else 1\n",
    "        image = (image - mean) / (std + 1e-8)\n",
    "        return image\n",
    "    \n",
    "    def apply_augmentation(self, image, label):\n",
    "        image, label = self.aug.random_rotation(image, label)\n",
    "        image, label = self.aug.random_scaling(image, label)\n",
    "        image, label = self.aug.random_flip(image, label)\n",
    "        image = self.aug.random_gamma(image)\n",
    "        image = self.aug.random_brightness(image)\n",
    "        image = self.aug.random_contrast(image)\n",
    "        return image, label\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = nib.load(self.image_files[idx]).get_fdata(dtype=np.float32)\n",
    "        label = nib.load(self.label_files[idx]).get_fdata(dtype=np.float32)\n",
    "        \n",
    "        image = self.preprocess(image)\n",
    "        \n",
    "        if self.use_augmentation:\n",
    "            image, label = self.apply_augmentation(image, label)\n",
    "        \n",
    "        image = torch.from_numpy(image).unsqueeze(0).unsqueeze(0)\n",
    "        label = torch.from_numpy(label).unsqueeze(0).unsqueeze(0)\n",
    "        \n",
    "        image = F.interpolate(image, size=(self.target_size, self.target_size, self.target_size),\n",
    "                            mode='trilinear', align_corners=False).squeeze(0)\n",
    "        label = F.interpolate(label, size=(self.target_size, self.target_size, self.target_size),\n",
    "                            mode='nearest').squeeze(0)\n",
    "        \n",
    "        label = label.squeeze(0).long()\n",
    "        label = torch.clamp(label, 0, 2)\n",
    "        return image, label\n",
    "\n",
    "# ==================== è©•ä¼°æŒ‡æ¨™ ====================\n",
    "\n",
    "def compute_dice(pred, target, num_classes=3):\n",
    "    dice_scores = []\n",
    "    for c in range(1, num_classes):\n",
    "        pred_c = (pred == c)\n",
    "        target_c = (target == c)\n",
    "        intersection = (pred_c & target_c).sum().float()\n",
    "        union = pred_c.sum().float() + target_c.sum().float()\n",
    "        if union == 0:\n",
    "            dice = 1.0 if intersection == 0 else 0.0\n",
    "        else:\n",
    "            dice = (2.0 * intersection) / union\n",
    "        dice_scores.append(dice.item())\n",
    "    return dice_scores\n",
    "\n",
    "# ==================== å­¸ç¿’ç‡èª¿åº¦å™¨ ====================\n",
    "\n",
    "class PolynomialLRScheduler:\n",
    "    def __init__(self, optimizer, initial_lr, max_epochs, power=0.9):\n",
    "        self.optimizer = optimizer\n",
    "        self.initial_lr = initial_lr\n",
    "        self.max_epochs = max_epochs\n",
    "        self.power = power\n",
    "        self.current_epoch = 0\n",
    "    \n",
    "    def step(self):\n",
    "        self.current_epoch += 1\n",
    "        lr = self.initial_lr * (1 - self.current_epoch / self.max_epochs) ** self.power\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "        return lr\n",
    "\n",
    "# ==================== è¨“ç·´å‡½æ•¸ ====================\n",
    "\n",
    "def train_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    pbar = tqdm(loader, desc='Training')\n",
    "    for images, labels in pbar:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 12)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "def validate(model, loader, device):\n",
    "    model.eval()\n",
    "    all_dice_scores = []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(loader, desc='Validating'):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            if isinstance(outputs, (list, tuple)):\n",
    "                outputs = outputs[-1]\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            for pred, label in zip(preds, labels):\n",
    "                dice_scores = compute_dice(pred.cpu(), label.cpu())\n",
    "                all_dice_scores.append(dice_scores)\n",
    "    all_dice_scores = np.array(all_dice_scores)\n",
    "    mean_dice = all_dice_scores.mean(axis=0)\n",
    "    return mean_dice\n",
    "\n",
    "# ==================== ä¸»è¨“ç·´æµç¨‹ ====================\n",
    "\n",
    "def main(seed=42):\n",
    "    set_seed(seed)\n",
    "    \n",
    "    config = {\n",
    "        'data_dir': './Task04_Hippocampus',\n",
    "        'batch_size': 2,\n",
    "        'num_epochs': 200,  # å¢åŠ åˆ° 200\n",
    "        'initial_lr': 1e-2,\n",
    "        'base_channels': 48,  # å¢åŠ åˆ° 48\n",
    "        'num_pool': 3,\n",
    "        'target_size': 64,\n",
    "        'deep_supervision': True,\n",
    "        'use_augmentation': True,\n",
    "        'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "        'seed': seed\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"å¢å¼·ç‰ˆ nnU-Net è¨“ç·´ (Seed: {seed})\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"é…ç½®:\")\n",
    "    for key, value in config.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    print()\n",
    "    \n",
    "    device = torch.device(config['device'])\n",
    "    \n",
    "    # æ•¸æ“šé›†\n",
    "    full_dataset = nnUNetDataset(\n",
    "        config['data_dir'],\n",
    "        target_size=config['target_size'],\n",
    "        is_train=True,\n",
    "        use_augmentation=config['use_augmentation']\n",
    "    )\n",
    "    \n",
    "    train_size = int(0.8 * len(full_dataset))\n",
    "    val_size = len(full_dataset) - train_size\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "        full_dataset, [train_size, val_size],\n",
    "        generator=torch.Generator().manual_seed(seed)\n",
    "    )\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=config['batch_size'],\n",
    "                            shuffle=True, num_workers=4, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=config['batch_size'],\n",
    "                          shuffle=False, num_workers=4, pin_memory=True)\n",
    "    \n",
    "    print(f\"è¨“ç·´é›†: {len(train_dataset)} æ¨£æœ¬\")\n",
    "    print(f\"é©—è­‰é›†: {len(val_dataset)} æ¨£æœ¬\\n\")\n",
    "    \n",
    "    # æ¨¡å‹\n",
    "    model = nnUNet(\n",
    "        in_channels=1,\n",
    "        num_classes=3,\n",
    "        base_channels=config['base_channels'],\n",
    "        num_pool=config['num_pool'],\n",
    "        deep_supervision=config['deep_supervision']\n",
    "    ).to(device)\n",
    "    \n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"æ¨¡å‹åƒæ•¸é‡: {total_params:,} ({total_params/1e6:.2f}M)\\n\")\n",
    "    \n",
    "    # å„ªåŒ–å™¨\n",
    "    criterion = nnUNetLoss(dice_weight=1.0, ce_weight=1.0)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=config['initial_lr'],\n",
    "                               momentum=0.99, weight_decay=3e-5, nesterov=True)\n",
    "    scheduler = PolynomialLRScheduler(optimizer, config['initial_lr'],\n",
    "                                     config['num_epochs'], power=0.9)\n",
    "    \n",
    "    # è¨“ç·´\n",
    "    best_dice = 0.0\n",
    "    output_dir = Path('./outputs')\n",
    "    output_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    for epoch in range(config['num_epochs']):\n",
    "        print(f\"Epoch {epoch+1}/{config['num_epochs']}\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        train_loss = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        val_dice = validate(model, val_loader, device)\n",
    "        current_lr = scheduler.step()\n",
    "        \n",
    "        print(f\"Train Loss: {train_loss:.4f}\")\n",
    "        print(f\"Val Dice - Class 1: {val_dice[0]:.4f}, Class 2: {val_dice[1]:.4f}, \"\n",
    "              f\"Avg: {val_dice.mean():.4f}\")\n",
    "        print(f\"Learning Rate: {current_lr:.6f}\")\n",
    "        \n",
    "        if val_dice.mean() > best_dice:\n",
    "            best_dice = val_dice.mean()\n",
    "            save_path = output_dir / f'nnunet_enhanced_seed{seed}_best.pth'\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'dice': val_dice,\n",
    "                'config': config\n",
    "            }, save_path)\n",
    "            print(f\"âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹ (Dice: {best_dice:.4f})\")\n",
    "        print()\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(f\"è¨“ç·´å®Œæˆï¼Seed: {seed}\")\n",
    "    print(f\"æœ€ä½³é©—è­‰ Dice: {best_dice:.4f}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    return best_dice\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # æª¢æ¸¬æ˜¯å¦åœ¨ Jupyter ç’°å¢ƒä¸­\n",
    "    try:\n",
    "        get_ipython()\n",
    "        # åœ¨ Jupyter ä¸­ï¼Œç›´æ¥ä½¿ç”¨é»˜èªç¨®å­\n",
    "        print(\"æª¢æ¸¬åˆ° Jupyter ç’°å¢ƒï¼Œä½¿ç”¨é»˜èªç¨®å­ 42\")\n",
    "        main(seed=42)\n",
    "    except NameError:\n",
    "        # åœ¨å‘½ä»¤è¡Œä¸­ï¼Œä½¿ç”¨ argparse\n",
    "        parser = argparse.ArgumentParser()\n",
    "        parser.add_argument('--seed', type=int, default=42, help='éš¨æ©Ÿç¨®å­')\n",
    "        args = parser.parse_args()\n",
    "        main(seed=args.seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1a965f-6956-4aba-a6b0-098c16faba99",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
